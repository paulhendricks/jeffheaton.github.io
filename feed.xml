<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Heaton Research</title>
    <description>Jeff Heaton is a data scientist, phd student and indie publisher.  Heaton Research is the homepage for my projects.
</description>
    <link>http://www.heatonresearch.com/</link>
    <atom:link href="http://www.heatonresearch.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 27 Sep 2015 16:04:18 -0500</pubDate>
    <lastBuildDate>Sun, 27 Sep 2015 16:04:18 -0500</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>My Experience with the Coursera Johns Hopkins Data Science Certification</title>
        <description>&lt;p&gt;This post is a summary of several posts that I had on my old blog about the &lt;a href=&quot;https://www.coursera.org/specializations/jhudatascience&quot;&gt;Johns
Hopkins Data Science certification&lt;/a&gt; offered by &lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;.
I am not sure how typical of a student I was for this program.  I currently work as a 
data scientist, have a decent background in AI, have a number of publications, and am 
currently completing a PhD in computer science.  So, a logical question is what did I 
want from this program?&lt;/p&gt;

&lt;p&gt;At the time I took this certification I had not done a great deal of R programming.  This 
program focused heavily on R.  I view this as both a strength and weakness of the program.&lt;br /&gt;
I am mostly a Java/Python/C#/C++ guy.  I found the R instruction very useful.
I’ve focused mainly in AI/machine learning, I hoped this program would fill in some gaps.
Curiosity.&lt;/p&gt;

&lt;p&gt;I really liked this program.  Courses 1-9 provide a great introduction to the predictive 
modelling side of data science.  Both machine learning and traditional regression models 
were covered.  R can be a slow and painful language, at times, but I was able to get 
through.  It is my opinion that R is primarily useful for ferrying data between models 
and visualization graphs.  It is not good for heavy-lifting and data wrangling.  The 
syntax to R is somewhat appalling.  However, it is a domain specific language (DSL), 
not a general purpose language like Python.  Don’t get me wrong.  I like R for setting up 
models and graphics.  Not for performing tasks better suited to a general purpose language.&lt;/p&gt;

&lt;p&gt;In a nutshell, here are my opinions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; With the exception of the capstone, very practical real-world data sets.  Experience with both black-box (machine learning) and more explainable (regression models) systems.  Introduction to Slidify and Shiny, I’ve already used both in my day-to-day job.  It takes some real work and understanding to make it through this program.  The last three courses rocked!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; Peer review is really hit or miss.  More on this later.  Some lecture material was sub-par (statistical inference) compared to Khan Academy.  Only reinvent the wheel if you are going to make a better wheel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;course-breakdown&quot;&gt;Course Breakdown&lt;/h2&gt;

&lt;p&gt;Here are my quick opinions on some of these courses.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The Data Scientist’s Toolbox&lt;/strong&gt;: Basically, can you install R, RStudio and use GitHub.  I’ve already done all three so I got little from this course.  If you have not dealt with R, RStudio and GitHub this class will be a nice slow intro to the program.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;R Programming&lt;/strong&gt;: I enjoyed this course!  It was hard, considering I was taking classes #1 and #3 at the same time.  If you had no programming experiance, this course will be really hard!  Be ready to supplement the instruction with lots of Google Searching.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Getting and Cleaning Data&lt;/strong&gt;: Data wrangling is an important part of data science.  Getting data into a tidy format is important.  This course used quite a bit of R programming.  For me, not being an R programmer and taking course #2 at the same time meant extra work.  If you are NOT an advanced programmer already DO NOT take #2 and #3 at the same time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploratory Data Analysis&lt;/strong&gt;: This was a valuable class, it taught you all about the R graphing packages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reproducible Research&lt;/strong&gt;:  Valuable course!  Learning about R markdown was very useful, I am already using this in one of my books to make sure that several of my examples are reproducible by providing a RMD script to produce all the charts from my book.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Inference&lt;/strong&gt;:  This was an odd class.  I already knew statistical inference and did quite well despite not watching any lectures (hardly).  I don’t believe this course made anyone happy (hardly).  Either you already knew the topic and were bored, or you were completely lost trying to learn statistics for the first time.  There are several Khan academy videos that cover all the material in this course.  Why dose Hopkins need to reproduce this?  Is this not the point of MOOC?  Why not link to the Khan academy videos and test the students.  Best of both worlds!  Also, 90% of the material was not used in the rest of the course, so I suspect many students might have been left wondering why this course is for.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regression Models&lt;/strong&gt;: Great course, this is the explainable counterpart to machine learning.  You are introduced to linear regression and GLM’s.  This course was setup as the perfect counterpart to #8. My only beef on this course was that I got screwed by peer review.  More on this later.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Practical Machine Learning&lt;/strong&gt;: Great course.  This course showed some of the most current model types in data science: Gradient Boosting Machines (GBMs) and Random Forests.  Also a great description of boosting and an awesome Kaggle like assignment where you submitted results from your model to see if you can match a “hidden data set”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developing Data Products&lt;/strong&gt;: Great course.  I really enjoyed playing with shiny, and even used it for one of the examples in my upcoming book.  You can see my shiny project here. https://jeffheaton.shinyapps.io/shiny/  and writeup here.  http://slidify.org/publish.html  They encouraged you to post these to GitHub and public sites, so I assume I am not violating anything by posting here.  Don’t plagiarize me!!!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Bad ending to an otherwise great program.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;peer-reviewed-grading&quot;&gt;Peer Reviewed Grading&lt;/h2&gt;

&lt;p&gt;If you are not familiar with peer review grading, here is how it works.  For each project 
you are given four counterparts that review and grade your assignment.  This is mostly 
double-blind, as neither the student or reviewer knows the other.  I used my regular 
GitHub account on all assignments.  So it was pretty obvious who I was.  I was even 
emailed by a grader once who recognized me from my open source projects.  Your grade is an 
average of what those four people gave you.  At $49 a course maybe this is the only way 
they can afford grade.  I currently spend nearly 100 times that for each of my 
PhD courses. :(&lt;/p&gt;

&lt;p&gt;Overall, peer review grading worked good for me in all courses but one.  Here are some of 
my concerns on peer grading.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You probably have many graders who are pressed for time and just give high-marks without  much thought.  (just a guess/opinion)&lt;/li&gt;
  &lt;li&gt;You are going to be graded by people who may not have not gotten the question correctly in the first place.&lt;/li&gt;
  &lt;li&gt;You are instructed NOT to run the R program.  So now I am being graded on someone’s ability to mentally compile and execute my program?&lt;/li&gt;
  &lt;li&gt;Each peer is going to apply different standards.  You could get radically different marks depending on who your four peers were.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So here is my story in the one case where peer review did not work for me.  I in the 
upper 98-99% range on most of these courses.  Except for course #8.  I had good scores 
going into the final project.  However, two of my peers knocked me for these reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two of my peers could not download my file from Coursera.  Yet the other two had no problem.  Fine, so I get a zero because someone’s ISP was flaking out.&lt;/li&gt;
  &lt;li&gt;Two of peers did not give me credit because the felt I had not used RMD for my report?? (which I had) Fine, so I lose a fair amount of points because two random peers did not know what RMD output looks like.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This took a toll on my grade, I still passed.  But this is the one course I did not get 
“with distinction” credit.  Yeah big deal.  In the grand scheme of things I don’t really 
care.  Just mildly annoying.  However, if you are hovering near a 70%, and you get one or 
two bad reviewers you are probably toast.&lt;/p&gt;

&lt;h2 id=&quot;capstone-project&quot;&gt;Capstone Project&lt;/h2&gt;

&lt;p&gt;The capstone project was to produce a program similar to &lt;a href=&quot;https://swiftkey.com/en&quot;&gt;Swiftkey&lt;/a&gt;, the company that was 
the partner/sponsor for the capstone.  If you are not familiar with Swiftkey, it attempts 
to speed mobile text input by predicting the next word you are going to type.  For example, 
you might type “to be or not to &lt;em&gt;__&lt;/em&gt;”.  The application should fill in “be”.  The end 
program had to be written in R and deployed to a Shiny Server.&lt;/p&gt;

&lt;p&gt;This project was somewhat flawed in several regards.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Natural Language Processing was not covered in the course.  Neither was unstructured data.  The only material provided on NLP was a handful of links to sites such as Wikipedia.&lt;/li&gt;
  &lt;li&gt;The first 9 courses had a clear direction.  However, less than half of them had anything to do with the capstone.&lt;/li&gt;
  &lt;li&gt;The project is not typical of what you would see in most businesses as a data scientist.  It would have been better to do something similar to Kaggle or one of the KDD cups.&lt;/li&gt;
  &lt;li&gt;In my opinion, R is a bad choice for this sort of project.  During the meetup with Swiftkey, they were asked what tools they used.  R was not among them. R is so cool for many things, why not showcase its abilities?&lt;/li&gt;
  &lt;li&gt;Student peer review is &lt;a href=&quot;https://www.insidehighered.com/blogs/hack-higher-education/problems-peer-grading-coursera&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://gregorulm.com/a-critical-view-on-courseras-peer-review-process/&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://moocnewsandreviews.com/massive-mooc-grading-problem-stanford-hci-group-tackles-peer-assessment/&quot;&gt;bad&lt;/a&gt;… But it might be the &lt;a href=&quot;http://simplystatistics.org/2013/03/26/an-instructors-thoughts-on-peer-review-for-data-analysis-in-coursera/&quot;&gt;only choice&lt;/a&gt;.  The problem with peer review is you have three random reviewers.  They might be easy, they might be hard.  They might penalize you for the fact that they don’t know how to load your program! (this happened to me on a previous coursera course).&lt;/li&gt;
  &lt;li&gt;Perfect scores on the quizzes were really not possible.  We were given several sample sentences to predict.  The sentences were very specialized and no model would predict them correctly.  The Swiftkey surely did not.  Using my own human intuition and several text mining apps I wrote in Java, I did get 100% on the quizzes.  Even though the instructions clearly said to use your final model.  Knowing I might draw a short straw on peer review, I opted to do what I could to get max points.  I don’t care about my grade, but falling below the cutoff for a bad peer review would not be cool!&lt;/li&gt;
  &lt;li&gt;Marketing based rubric for final project.  One of the grading criteria posted the question, “Would you hire this person?”  Seriously?  I do participate in the hiring process for data scientists.  I would never hire someone without meeting them, performing a tech interview, and small coding challenge.  I hope this stat is not used in marketing material.  xx% of our graduates produced programs that might land them a job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After spending several days writing very slow model building code in R, I eventually dropped it and used Java and OpenNLP to write code that would build my model in under 20 minutes.  Others ran into the same issues.  There are somewhat kludge interfaces between R and OpenNLP, Weka and OpenNLP.  But these are native Java apps.  I  just skipped the kludge and built my model in Java and wrote a Shiny app to use the model in R.  This was enough to pass the program.  I was not alone in this approach, based on forum comments.&lt;/p&gt;

&lt;p&gt;Okay, I will just say it.  I thought this was a bad capstone.  This was just my experience 
on the first run of the certification; hopefully, they’ve improved it since.  The rest of 
the program was  really good!  If I could make a suggestion, I would say to let the 
students choose a Kaggle competition to compete.  The Kaggle competitions are closer 
to the sort of data real data scientists will see.  I am proud of the certificate that I earned.&lt;br /&gt;
If I were interviewing someone who had this certificate I would consider it a positive.&lt;br /&gt;
The candidate would still need to go through a standard interview/evaluation process.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Great program.  It won’t make you a star data scientist, but it will give you a great 
foundation to go from.  Kaggle might be a good next step.  Another might be a blog and 
doing some real, and interesting data science to showcase your skills!  This is somewhat 
how I got into data science.&lt;/p&gt;

&lt;p&gt;A question that I am often asked, is what would I think of this certification, if I saw it
on the resume of a new data scientist that I was interviewing.  In isolation, I would not
give a hire recommend based solely on this certification.  However, it would show me that
someone has mastered the basics of data science.  They know what format data needs to be
in for predictive modeling.  They know their way around the R-programming language.  They
also took the initiative to undertake something that took a decent amount of effort.&lt;br /&gt;
So yes, it is important, particularly, if your resume is lacking in the analytics area.&lt;/p&gt;
</description>
        <pubDate>Sat, 26 Sep 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Visiting Spain and the GECCO Confrence</title>
        <description>&lt;p&gt;I spent six weeks in Spain this summer.  My wife, Tracy, is earning a &lt;a href=&quot;http://www.slu.edu/department-of-languages-literatures-and-cultures/programs-of-study/spanish/graduate-studies&quot;&gt;Master’s Degree in 
Spanish&lt;/a&gt; from &lt;a href=&quot;http://www.slu.edu/&quot;&gt;St. Louis University (SLU)&lt;/a&gt;.  Her university has an extension &lt;a href=&quot;http://spain.slu.edu/&quot;&gt;campus in Madrid&lt;/a&gt;.&lt;br /&gt;
I also wanted to be in Spain to visit the &lt;a href=&quot;http://www.sigevo.org/gecco-2015/&quot;&gt;GECCO-2015&lt;/a&gt; conference, that was held in Madrid.&lt;br /&gt;
We were able to line up a nice apartment in &lt;a href=&quot;https://en.wikipedia.org/wiki/Salamanca_(Madrid)&quot;&gt;Salamanca, Madrid&lt;/a&gt; and I was able to take a 
few weeks vacation and work remotely the rest of the time.  It was totally worth it, Spain 
is an amazing country.  I know just enough Spanish to get around and usually be fed at a 
restaurant!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-08-30-madrid-gecco-1.png&quot; alt=&quot;Madrid and GECCO&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GECCO conference was awesome.  I was able to briefly meet &lt;a href=&quot;http://www.cs.ucf.edu/~kstanley/&quot;&gt;Dr. Kenneth Stanley&lt;/a&gt; after 
hearing a tutorial session held by him.  I’ve many of Dr. Stanley’s papers and implemented 
his NEAT/HyperNEAT algorithms.  He also has a &lt;a href=&quot;http://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&quot;&gt;new book&lt;/a&gt; out that I’ve just read and recommend.&lt;br /&gt;
The book points out that it is very difficult to design objective functions that might 
truly create amazing results.  Some of the best results on &lt;a href=&quot;http://picbreeder.org/&quot;&gt;Ken’s PicBreader&lt;/a&gt; site were not 
specifically planned.&lt;/p&gt;

&lt;p&gt;I also saw a fascinating Lisp-based language named &lt;a href=&quot;http://faculty.hampshire.edu/lspector/push3-description.html&quot;&gt;Push3&lt;/a&gt;, created by &lt;a href=&quot;http://faculty.hampshire.edu/lspector/&quot;&gt;Dr. Lee Spector&lt;/a&gt;.&lt;br /&gt;
Genetic programming ha always been fascinating to me because it uses AI to actually 
evolve AI.  Push3 is a programming language designed for computers to evolve.&lt;br /&gt;
Dr. Spector’s group is evolving computer programs, to accomplish basic tasks like word 
count and other basic utilities.  Most machine learning algorithms are simply about 
optimizing coefficients to decrease a supervised training set’s error.  It is amazing to 
see algorithms that can actually evolve themselves into new algorithms.&lt;/p&gt;

&lt;p&gt;I plan to make use of genetic programming for part of my dissertation.  Of course, I am 
still finishing up coursework and this is very much in flux.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Aug 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>My First Kaggle Competition</title>
        <description>&lt;p&gt;I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it, 
&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; is an ongoing forum for competitive data science. Individuals and teams compete to 
create the best model for data sets provided by industry and sometimes academia.&lt;br /&gt;
Individuals who enter are ranked as either Novice, Kaggler and &lt;a href=&quot;https://www.kaggle.com/wiki/UserRankingAndTierSystem&quot;&gt;Kaggle Master&lt;/a&gt;.  To become 
a Kaggle master, one must place in the top 10% of two competitions; and in one of the top 
10 slots of a third competition.&lt;/p&gt;

&lt;p&gt;I’ve talked about Kaggle in many of my presentations.  I’ve also used Kaggle data in 
my books. Until now, I had yet to actually enter a Kaggle competition.  I decided it was 
finally time to try this for myself. I competed in the &lt;a href=&quot;http://otto%20group%20product%20classification%20challenge/&quot;&gt;Otto Group Product Classification&lt;/a&gt; 
Challenge that ended on May 18th, 2015.  My score was sufficient to land in the top 10%, 
so I’ve completed one of the requirements for Kaggle master.  My Kaggle profile can be 
seen here.&lt;/p&gt;

&lt;p&gt;My goals for entering were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;See how hard Kaggle actually is, and move towards a Kaggle master designation.&lt;/li&gt;
  &lt;li&gt;Learn from the other Kagglers and forums.&lt;/li&gt;
  &lt;li&gt;Build a basic toolkit that I will use for future Kaggle competitions.&lt;/li&gt;
  &lt;li&gt;Gain an example (from my entry) for the &lt;a href=&quot;http://www.heatonresearch.com/aifh&quot;&gt;Artificial Intelligence for Humans series&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Maybe get an idea or two for my future dissertation (I am a phd student at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-otto-classification-challenge&quot;&gt;The Otto Classification Challenge&lt;/h2&gt;

&lt;p&gt;First, I will give a brief introduction to the exact nature of the Otto Classification 
Challenge.  For a complete description, refer to the Kaggle description(&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge&quot;&gt;found here&lt;/a&gt;).&lt;br /&gt;
This challenge was introduced by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Otto_GmbH&quot;&gt;Otto Group&lt;/a&gt;, who is the world’s largest mail order 
company and currently one of the biggest &lt;a href=&quot;http://en.wikipedia.org/wiki/E-commerce&quot;&gt;e-commerce&lt;/a&gt; companies, mainly based in Germany 
and France but operating in more than 20 countries.  They have many products sold over 
numerous countries.  They would like to be able to classify these products into 9 
categories, using 93 features (columns).  These 93 columns represent counts, and are 
often zero.&lt;/p&gt;

&lt;p&gt;The data are completely redacted.  You do not know what the 9 categories are, nor do you 
know the meaning behind the 93 features.  You only know that the features are integer 
counts. Most Kaggle competitions provide you with a test and training dataset.  For the 
training dataset you are given the outcomes, or correct answers.  For the test set, you 
are only given the 93 features, and you must provide the outcome.  The test and training 
sets are divided as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Test Data: 144K rows&lt;/li&gt;
  &lt;li&gt;Training Data: 61K rows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You do not actually submit your model to Kaggle.  Rather, you submit your predictions 
based on the test data.  This allows you to use any platform to make these predictions.&lt;br /&gt;
The actual format of a submission for this competition is the probability of each of 
the 9 categories being the outcome.  This is not like a university multiple choice test 
where you must submit your answer as A, B, C, or D.  Rather, you would submit your 
answer as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A: 80% probability&lt;/li&gt;
  &lt;li&gt;B: 16% probability&lt;/li&gt;
  &lt;li&gt;C: 2% probability&lt;/li&gt;
  &lt;li&gt;D: 2% probability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wish college exams were graded like this!  Often I am very confident about two of the 
answers, and can eliminate the other two.  Simply assign a probability to each, and you 
get a partial score.  If A were the correct answer for the above, I would get 80% of the 
points.&lt;/p&gt;

&lt;p&gt;The actual Kaggle score is slightly more complex than that.  Rather, you are graded on a 
logarithm based scale and are very heavily penalized for having a lower probability on 
the correct answer. The following are a few lines from my submission:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1,0.0003,0.2132,0.2340,0.5468,6.2998e-05,0.0001,0.0050,0.0001,4.3826e-05
2,0.0011,0.0029,0.0010,0.0003,0.0001,0.5207,0.0013,0.4711,0.0011
3,3.2977e-06,4.1419e-06,7.4524e-06,2.6550e-06,5.0014e-07,0.9998,5.2621e-06,0.0001,6.6447e-06
4,0.0001,0.6786,0.3162,0.0039,3.3378e-05,4.1196e-05,0.0001,0.0001,0.0006
5,0.1403,0.0002,0.0002,6.734e-05,0.0001,0.0027,0.0009,0.0297,0.8255&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each line starts with a number that specifies the data item that is being answered.&lt;br /&gt;
The sample above shows the answers for items 1-5.  The next 9 values are the probabilities 
for each of the product classes.  These probabilities must add up to 1.0 (100%).&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned-from-kaggle&quot;&gt;What I Learned from Kaggle&lt;/h2&gt;

&lt;p&gt;If you want to do well in Kaggle, the following are very important topics, along with 
the tools I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep Learning - Using &lt;a href=&quot;http://h2o.ai/&quot;&gt;H2O&lt;/a&gt; and &lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;Lasagne&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gradient Boosting Machines (GBM) - &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;Using XGBOOST&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ensemble Learning - &lt;a href=&quot;http://www.numpy.org/&quot;&gt;Using NumPy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Feature Engineering - Using NumPy and &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two areas that I learned the most about, during this challenge, were GBM parameter 
tuning and ensemble learning.  I got pretty good at tuning a GBM.  The individual scores 
for my GBM’s were in line with those used by the top teams.&lt;/p&gt;

&lt;p&gt;Before Kaggle I typically used only one model, if I were using neural networks, I just 
used neural networks.  If I were using an SVM, Random Forest or Gradient Boosting, I stuck 
to just that model.  With Kaggle, it is critical to use multiple models, ensembled to 
produce better results than each of the models could produce independently.&lt;/p&gt;

&lt;p&gt;Some of my main takeaways from the competition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GPU is really important for deep learning.  It is best to use a deep learning package that supports it, such as H2O, Theano or Lasagne.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://lvdmaaten.github.io/tsne/&quot;&gt;t-sne&lt;/a&gt; visualization is awesome for high-dimension visualization and creating features.&lt;/li&gt;
  &lt;li&gt;I need to learn to ensemble better!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This competition was the first time I used &lt;a href=&quot;http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&quot;&gt;T-SNE&lt;/a&gt;.  It works like PCA in that it is capable 
of reducing dimensions, however, the data points separate in such a way that the 
visualization is often clearer than &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt;. This is done using a stochastic nearest 
neighbor process. I plan to learn more about how t-sne actually performs the reduction, 
compared to PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-1.jpeg&quot; alt=&quot;t-SNE Plot of the Otto Group Challenge&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-approach-to-the-otto-challenge&quot;&gt;My Approach to the Otto Challenge&lt;/h2&gt;

&lt;p&gt;So far I’ve only worked with single model systems.  I’ve used models that contain ensembles 
that are “built in”, such as random forests and gradient boosting machines.  However, it 
is possible to create higher-level ensembles of these models.  I used a total of 20 models, 
this included 10 deep neural networks and 10 gradient boosting machines.  My deep neural 
network system provided one prediction and my gradient boosting machines provided the other.&lt;br /&gt;
These two predictions were blended together, using a simple ratio.  The resulting prediction 
vector was then normalized so that the sum equaled 1.0(100%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-2.png&quot; alt=&quot;Jeff Heaton&#39;s Kaggle Model for the Otto Group&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I did not remove or engineer any fields.  For both model types I converted all 93 
attributes into Z-Scores.  For the neural network I normalized all values to be in a 
specific range.&lt;/p&gt;

&lt;p&gt;My 10 deep learning neural networks used a simple bagging method.  I averaged the 
predictions from 20 different neural networks.  Each of these neural networks was created 
by choosing a different 80/20 split between training and validation.  The neural network 
was trained on the training data until the validation score did not improve for 25 epochs.&lt;br /&gt;
Once training stopped I used the weights from the epoch that produced the highest training 
score. This process is a simple form of bagging called &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot;&gt;bootstrap aggregation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My 10 gradient boosting machines (GBM) were each components of a 10-fold cross-validation.&lt;br /&gt;
I essentially broke the Kaggle training data into 10 folds and used each of these folds 
as a validation set, and the others as training.  This produced 10 gradient boosting machines.&lt;br /&gt;
I then used an NxM coefficient matrix to blend each of these together.  Where N is the 
number of models, M is the number of features.  In this case it was a 10x9 grid.  This 
matrix weighted each of the 10 model’s predictive power in each of the 9 categories.&lt;br /&gt;
These coefficients were a straight probability calculation from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt; of 
each of the 10 models.  This allowed each model to potentially specialize in each of the 
9 categories.&lt;/p&gt;

&lt;p&gt;I spent considerable time tuning my GBM.  I used Nelder-Mead searches to optimize my 
hyper-parameter vector.  I ultimately settled on the following parameters:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;min_child_weight&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;subsample&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;colsample_bytree&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;eta&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;threads&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approaches&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GBM&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produced&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;separate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;submission&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blended&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;together&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weighting&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.65&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gave&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;me&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-worked-well-for-top-teams&quot;&gt;What Worked Well for Top Teams&lt;/h2&gt;

&lt;p&gt;The top Kaggle teams made use of more sophisticated ensemble techniques than I did.&lt;br /&gt;
This will be my primary learning area for the next competition.  You can read about 
some of the top models here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov&quot;&gt;The Top Scoring Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score&quot;&gt;Relatively Simple Model for a Top 25 Score&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models&quot;&gt;Share Your Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent&quot;&gt;One of the Top Ten&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging&quot;&gt;TSNE &amp;amp; Meta-Bagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above write-ups are very useful, I’ve already started examining their approaches.&lt;/p&gt;

&lt;p&gt;Some of the top technologies discussed were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Engineering&lt;/li&gt;
  &lt;li&gt;Input Transformation - good write up &lt;a href=&quot;http://fmwww.bc.edu/repec/bocode/t/transint.html&quot;&gt;here&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;log transforms&lt;/li&gt;
      &lt;li&gt;sqrt(x + 3/8) - Not sure what this one is called, but I saw it used a few times&lt;/li&gt;
      &lt;li&gt;z-score transforms&lt;/li&gt;
      &lt;li&gt;ranged transformation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hyperparameter Optimization
    &lt;ul&gt;
      &lt;li&gt;Nelder-Mead&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/JasperSnoek/spearmint&quot;&gt;Spearmint&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will probably not enter another Kaggle until the fall of this year.  This blog post 
will be updated to contain my notes as I investigate other techniques for this 
competition.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 May 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Quick R Tutorial: The Big-O Chart</title>
        <description>&lt;p&gt;I needed an original &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;Big-O&lt;/a&gt; chart for a publication.  This tutorial does not cover what 
Big-O actually is, just how to chart it.  If you want more information on Big-O I 
recommend reading &lt;a href=&quot;http://bigocheatsheet.com/&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;this&lt;/a&gt;.  I know, go figure, for a computer science student. I 
really like using R for all things chart and visualization.  So, I turned to R for this 
task.  While this is a very simple chart, it does demonstrate several very common 
tasks in R:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overlaying several plots&lt;/li&gt;
  &lt;li&gt;Adding a legend to a plot&lt;/li&gt;
  &lt;li&gt;Using “math symbols” inside of the text on a chart&lt;/li&gt;
  &lt;li&gt;Stripping off all the extra whitespace that R likes to generate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the end-result here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-21-r_big_o-1.png&quot; alt=&quot;Computer Science Big-O Chart in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This was produced with the following R code:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Remove white space on chart&lt;/span&gt;
par&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mar&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;+0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The x &amp;amp; y limits for the plot&lt;/span&gt;
xl &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
yl &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Just use R&amp;#39;s standard list of colors for the lines (I am too tired to be creative this morning)&lt;/span&gt;
pal &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; palette&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot each of the equations that we are interested in. Note the add=TRUE&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# it causes them to overlay each other.&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;xlab&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Elements(N)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylab&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Operations&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generate the legend, note the use of expression to handle n to the power of 2.&lt;/span&gt;
legend&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;topright&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(1)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(log(n))&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(n)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(n log(n))&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;kp&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;O&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;O&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(!n)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;lty&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; bty&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;n&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; cex&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 21 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o.html</guid>
        
        
        <category>compsci</category>
        
        <category>r</category>
        
      </item>
    
      <item>
        <title>Using Encog to Replicate Research</title>
        <description>&lt;p&gt;For the one of my PhD courses at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University (NSU)&lt;/a&gt; it was necessary to 
reproduce the research of the following paper:&lt;/p&gt;

&lt;p&gt;I. Ahmad, A. Abdullah, and A. Alghamdi, “Application of artificial neural network in 
detection of probing attacks,” in IEEE Symposium on Industrial Electronics Applications, 
2009. ISIEA 2009., vol. 2, Oct 2009, pp. 557–562.&lt;/p&gt;

&lt;p&gt;This paper demonstrated how to use a neural network to build a &lt;a href=&quot;http://en.wikipedia.org/wiki/Intrusion_detection_system&quot;&gt;basic intrusion detection 
system (IDS)&lt;/a&gt; for the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;KDD99 dataset&lt;/a&gt;.  It is important to reproduce research, in an academic 
setting.  This means that you were able to obtain the same results as the original 
researchers, using the same techniques.  I do this often when I write books or implement 
parts of Encog. This allows me convince myself that I have implemented an algorithm 
correctly, and as the researchers intended.  I don’t always agree with what the original 
researcher did.  If I change it, when I implement Encog, I am now in the area of “original 
research,” and my changes must be labeled as such.&lt;/p&gt;

&lt;p&gt;Some researchers are more helpful than others for replication of research.  Additionally, 
neural networks are stochastic (they use random numbers).  Basing recommendations off of 
a small number of runs is usually a bad idea, when dealing with a stochastic system.&lt;br /&gt;
Their small number of runs caused the above researchers to conclude that two hidden layers 
was optimal for their dataset.  Unless you are dealing with deep learning, this is almost 
always not the case.  The universal approximation theorem rules out more than a single 
layer for the old-school sort of perceptron neural network used in this paper.&lt;br /&gt;
Additionally, the vanishing gradient problem prevents the RPROP training that the 
researchers from fitting well with larger numbers of hidden layers.  The researchers 
tried up to 4 hidden layers.&lt;/p&gt;

&lt;p&gt;For my own research replication I used the same dataset, with many training runs to make 
sure that their results were within my high-low range.  To prove that a single layer does 
better I used &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;ANOVA&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot;&gt;Tukey’s HSD&lt;/a&gt; to show that differences among the different neural 
network architectures were indeed statistically significant and my box and whiskers plot 
shows that training runs with a single layer more consistently converged to a better 
mean &lt;a href=&quot;http://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;&gt;RMSE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am attaching both my paper and code in case it is useful.  This is a decent tutorial 
on using the latest Encog code to normalize and fit to a data set.&lt;/p&gt;

&lt;p&gt;The class also required us to write up the results in &lt;a href=&quot;http://www.ieee.org/conferences_events/conferences/publishing/templates.html&quot;&gt;IEEE conference format&lt;/a&gt;.  I am a 
fan of &lt;a href=&quot;http://www.latex-project.org/&quot;&gt;LaTex&lt;/a&gt;, so that is what I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source code Includes: &lt;a href=&quot;https://github.com/jeffheaton/phd/tree/master/ids-replicate-neural&quot;&gt;Source Code Link&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Python data prep script&lt;/li&gt;
      &lt;li&gt;R code used to produce graphics and stat analysis&lt;/li&gt;
      &lt;li&gt;Java code to run the training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;My report for download (PDF): &lt;a href=&quot;https://github.com/jeffheaton/phd/blob/master/ids-replicate-neural/jheaton-ids-replicate.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;My report on ResearchGate: &lt;a href=&quot;https://www.researchgate.net/publication/273441572_Replicating_the_Research_of_the_Paper_Application_of_Artificial_Neural_Network_in_Detection_of_Probing_Attacks&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code is under LGPL, so feel free to reuse.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research.html</guid>
        
        
        <category>phd</category>
        
        <category>compsci</category>
        
        <category>encog</category>
        
      </item>
    
      <item>
        <title>How I Got Into Data Science from IT Programming</title>
        <description>&lt;p&gt;Drew Conway describes data scientist as the combination of domain expertise, statistics 
and hacker skills.  If you are an IT programmer, you likely already have the last 
requirement.  If you are a good IT programmer, you probably already understand something 
about the business data, and have the domain expertise requirement.  In this post I 
describe how I gained knowledge of statistics/machine learning through a path of open 
source involvement and publication.&lt;/p&gt;

&lt;p&gt;There are quite a few articles that discuss how to become a data scientist.  Some of them 
are &lt;a href=&quot;http://www.quora.com/How-do-I-become-a-data-scientist&quot;&gt;even&lt;/a&gt; quite &lt;a href=&quot;http://i1.wp.com/blog.datacamp.com/wp-content/uploads/2014/08/How-to-become-a-data-scientist.jpg&quot;&gt;good&lt;/a&gt;! Most speak in very general terms.  I wrote such an summary awhile 
back that provides a &lt;a href=&quot;/datascience/2014/02/26/be-a-data-scientist.html&quot;&gt;very general description&lt;/a&gt; of what a data scientist is. In this post, 
I will describe my own path to becoming a data scientist.  I started out as a Java 
programmer in a typical IT job.&lt;/p&gt;

&lt;h2 id=&quot;publications&quot;&gt;Publications&lt;/h2&gt;

&lt;p&gt;My publications were some of my earliest credentials.  I started publishing before I had 
my bachelor’s degree.  My publications and side-programming jobswere the major factors 
that helped me obtain my first “real” programming job, working for a Fortune 500 
manufacturing company, back in 1995.  I did not have my first degree at that point.  In 
1995 I was working on a bachelor’s degree part-time.&lt;/p&gt;

&lt;p&gt;Back in the day, I wrote for publications such as C/C++ Users Journal, Java Developers 
Journal, and Windows/DOS Developer’s journal.  These were all paper-based magazines.&lt;br /&gt;
Often on the racks at book stores.  The world has really changed since then!  These days 
I publish code on sites like &lt;a href=&quot;http://www.github.com/&quot;&gt;GitHub&lt;/a&gt; and &lt;a href=&quot;http://www.codeproject.com/&quot;&gt;CodeProject&lt;/a&gt;.  A great way to gain experience is 
to find interesting projects to work on, using open source tools.  Then post your projects 
to GitHub, CodeProject and others.&lt;/p&gt;

&lt;p&gt;I’ve always enjoyed programming and have applied it to many individual projects.  Back in 
the 80’s I was writing BBS software so that I could run a board on a C64 despite 
insufficient funds from high school jobs to purchase a RAM expander.  In the 90’s I was 
hooking up web web cams and writing &lt;a href=&quot;http://en.wikipedia.org/wiki/Common_Gateway_Interface&quot;&gt;CGI&lt;/a&gt; and then later &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_Server_Pages&quot;&gt;ASP&lt;/a&gt;/&lt;a href=&quot;http://en.wikipedia.org/wiki/JavaServer_Pages&quot;&gt;JSP&lt;/a&gt; code to build websites.  I 
wrote web servers and spiders from the socket up in C++.  Around that time I wrote my 
first neural network.  Always publish! A hard drive full of cool project code sitting 
in your desk is not telling the world what you’ve done.  Support open source, a nice set 
of independent projects on GitHub looks really good.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-ai&quot;&gt;Starting with AI&lt;/h2&gt;

&lt;p&gt;Artificial intelligence is closely related to data science.  In many ways data science 
is the application of certain AI techniques to potentially large amounts of data.  AI is 
also closely linked with statistics, an integral part of data science.  I started with AI 
because it was fun.  I never envisioned using it in my “day job”.  As soon as I got my 
first neural network done I wrote an article for Java Users Journal.  I quickly discovered 
that AI had a coolness factor that could help me convince editors to publish my software.&lt;br /&gt;
I also published my first book on AI.&lt;/p&gt;

&lt;p&gt;Writing code for a book is very different than writing code for a corporate project/open 
source project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Book code&lt;/strong&gt;: Readability and understandably are paramount.  Second to none.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Corporate/Open Source Code&lt;/strong&gt;: Readability and understandably are important.  However, real-world necessity often forces scalability &amp;amp; performance to take the front seat.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if my book’s main goal is to show how to use JSP to build a simple blog, 
do I really care if the blog can scale to the traffic seen by a top-100 website?&lt;br /&gt;
Likewise, if my goal is to show how a backpropagation neural network trains, do I really 
want to muddy the water with concurrency?&lt;/p&gt;

&lt;p&gt;The neural network code in my books is meant to be example code.  A clear starting point 
for something.  But this code is not meant to be “industrial strength”.  However, when 
people start asking you questions that indicate that they are using your example code 
for “real projects”, it is now time to start (or join) an open source project!  This is 
why I started the &lt;a href=&quot;/encog/&quot;&gt;Encog project&lt;/a&gt;.  This might be a path to an open source project for you!&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-understanding-of-ai&quot;&gt;Deepening my Understanding of AI&lt;/h2&gt;

&lt;p&gt;I’ve often heard that neural networks are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gateway_drug_theory&quot;&gt;gateway drug&lt;/a&gt; to greater artificial 
intelligence.  Neural networks are an interesting creature.  They have risen and fallen 
from grace several times.  Currently they are back, and with a vengeance.   Most 
implementations of deep learning are based on neural networks.  If you would like to 
learn more about deep learning, I ran a successful Kickstarter campaign on that 
&lt;a href=&quot;https://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-3-deep-lear&quot;&gt;very topic&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I took several really good classes from &lt;a href=&quot;https://www.udacity.com/&quot;&gt;UDacity&lt;/a&gt;, just as they were introduced.  These 
classes have been somewhat re-branded.  However, UDacity still several great AI and Machine 
Learning courses.  I also recommend (and have taken) the &lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1&quot;&gt;Johns Hopkins Coursera Data Science&lt;/a&gt; 
specialization.  Its not perfect, but it will expose you to many concepts in AI.  You can 
read my &lt;a href=&quot;/phd/2015/09/26/coursera-johns-hopkins-data-science.html&quot;&gt;summary of it here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, learn statistics.  At least the basics of classical statistics.  You should 
understand concepts like mean, mode, median, linear regression, anova, manova, Tukey HSD, 
p-values, etc.  A simple undergraduate course in statistics will give you the foundation.&lt;br /&gt;
You can build on more complex topics such as Bayesian networks, belief networks, and 
others later.  Udacity has a nice intro to statistics course.&lt;/p&gt;

&lt;h2 id=&quot;kickstarter-projects&quot;&gt;Kickstarter Projects&lt;/h2&gt;

&lt;p&gt;Public projects are always a good thing.  My projects have brought me speaking 
opportunities and book opportunities (though I mostly self publish now).  Kickstarter 
has been great for this.  I launched my Artificial Intelligence for Humans series of books 
through Kickstarter.&lt;/p&gt;

&lt;h2 id=&quot;from-ai-to-data-science&quot;&gt;From AI to Data Science&lt;/h2&gt;

&lt;p&gt;When data science first started to enter the public scene I was working as a Java 
programmer writing AI books as a hobby.  A job opportunity at my current company later 
opened up in data science.  I did not even realize that the opportunity was available, 
I really was not looking.  However, during the recruiting process they discovered that 
someone with knowledge of the needed areas lived right here in town.  They had found my 
project pages.  This led to some good opportunities right in my current company.&lt;/p&gt;

&lt;p&gt;The point is, get your projects out there!  If you don’t have an idea for a project, then 
enter &lt;a href=&quot;http://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;.  You probably won’t win.  Try to become a Kaggle master.  That will be hard.&lt;br /&gt;
But you will learn quite a bit trying.  Write about your efforts.  Post code to GitHub.&lt;br /&gt;
If you use open source tools, write to their creators and send links to your efforts.&lt;br /&gt;
Open source creators love to post links to people who are actually using their code.&lt;br /&gt;
For bigger projects (with many or institutional creators), post to their communities.&lt;br /&gt;
Kaggle gives you a problem to solve.  You don’t have to win.  It will give you something 
to talk about during an interview.&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-knowledge-as-a-data-scientist&quot;&gt;Deepening my Knowledge as a Data Scientist&lt;/h2&gt;

&lt;p&gt;I try to always be learning.  You will always hear terminology that you feel like you 
should know, but do not.  This happens to me every day.  Keep a list of what you don’t 
know, and keep prioritizing and tacking the list (dare I say &lt;a href=&quot;http://guide.agilealliance.org/guide/backlog-grooming.html&quot;&gt;backlog grooming&lt;/a&gt;).&lt;br /&gt;
Keep learning!  Get involved in projects like Kaggle and read the discussion board.&lt;br /&gt;
This will show you what you do not know really quick. Write tutorials on your efforts.&lt;br /&gt;
If something was hard for you, it was hard for others who will appreciate a tutorial.&lt;/p&gt;

&lt;p&gt;I’ve seen a number of articles that question “Do you need a PhD to work as a data 
scientist?” The answer is that it will help, but is not necessary.  I know numerous 
data scientists with varying levels of academic credentials.  A PhD demonstrates that 
someone can follow the rigors of formal academic research and extend human knowledge.&lt;br /&gt;
When I became a data scientist I was not a PhD student.&lt;/p&gt;

&lt;p&gt;At this point, I am a PhD student in computer science, you can read more about that &lt;a href=&quot;/phd/compsci/2014/05/02/starting-computer-science-phd.html&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
I want to learn the process of academic research because I am starting to look at 
algorithms and techniques that would qualify as original research.  Additionally, I’ve 
given advice to a several other PhD students, who were using my projects open source 
projects in their dissertations.  It was time for me to take the leap.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Data science is described, by Drew Conway, as the intersection of hacker skills, 
statistics and domain knowledge.  To be an “IT programmer” you most likely already have 
two of these skills.  Hacker skills is ability to write programs that can wrangle data 
into many different formats and automate processes.  Domain knowledge is knowing something 
about the business that you are programming for.  Is your business data just a bunch of 
columns to you?  An effective IT programmer learns about the business and it’s data.&lt;br /&gt;
So does an effective data scientist.&lt;/p&gt;

&lt;p&gt;This leaves, only really statistics (and machine learning/AI).  You can learn that from 
books, MOOCS, and other sources.  Some were mentioned earlier in this article.  I have 
a list of some of my favorites here.  I also have a &lt;a href=&quot;/book/&quot;&gt;few books&lt;/a&gt; to teach you about AI.&lt;/p&gt;

&lt;p&gt;Most importantly, tinker and learn.  Build/publish projects, blog and contribute to open 
source.  When you talk to someone interested in hiring you as a data scientist, you will 
have experience to talk about.  Also have a &lt;a href=&quot;https://github.com/jeffheaton&quot;&gt;GitHub profile&lt;/a&gt;, linked to LinkedIn that shows 
you do in-fact have something to talk about.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>PhD Update: Second Cluster Visit for Winter 2015 Semester at NSU</title>
        <description>&lt;p&gt;I just got back to St. Louis from my second cluster visit to NSU for the phd in computer 
science.  I was feeling pretty good about choosing a distance learning program at a 
university in Ft. Lauderdale, Florida after the really cold weather we’ve been seeing in 
St. Louis lately.  There was a new blanket of snow on my drive way the morning that I 
left for the airport.  I just drove over it and headed out, the snow was melted by the 
time I returned.  The regular (non-distance learning) students were all on spring break, 
so the campus was unusually empty.  I never did take a spring break trip as an undergrad, 
so it works out that I have to go to Florida 4 times a year for my doctoral program.&lt;/p&gt;

&lt;p&gt;I am taking two classes: CISD 792: Computer Graphics, taught by Dr. Laszlo, and 
ISEC 730: Computer Security and Cryptography, taught by Dr. Cannady. The computer 
graphics class focuses on Three.JS and OpenGL, and consists of numerous programming 
assignments.  I am learning about 3D programming. I think this will be very useful for 
some visualizations that I might want to do for my books.  The security class is more 
focused on writing.  I did a decent amount of programming to replicate the research of a 
paper that applied neural networks to intrusion detection.  I will post my Java code for 
this later, as it is a decent Encog tutorial.  One of several reasons that I entered a 
PhD program was to learn academic writing, so the security class is working out well.&lt;br /&gt;
I am finding both classes every beneficial and interesting.&lt;/p&gt;

&lt;p&gt;This is my fourth trip to Ft. Lauderdale for the program.  My wife has come along with me 
each time so far.  We usually try to do at least one “tourist activity” each time.  This 
time we went to see a spring training baseball game that featured our home-team, the St. 
Louis Cardinals against the Miami Marlins, (notes to everyone, especially lawyers: both 
of those names are trademarks of the MLB, MLB is also a trademark of the MLB)  The 
St. Louis team won, so this made for a particularly enjoyable game!&lt;/p&gt;

&lt;p&gt;I also kept tabs on my Kickstarter campaign while traveling.  So far the deep learning and 
neural network is going well!  If you would like to back, and obtain my latest book, 
click here.&lt;/p&gt;

&lt;p&gt;I took this picture at the student center at NSU.  They have a really cool Dr. Who police 
box.  You can also see the palm trees out the window.  They have a beautiful campus!&lt;br /&gt;
And now they have a police box!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-09-phd-2nd-cluster-visit-1.png&quot; alt=&quot;Dr. Who Police Box at NSU&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Quick and Very Dirty Data Wrangling Example</title>
        <description>&lt;p&gt;Data science is often described as the intersection of statistics, domain knowledge and 
hacking skills.  One important part of hacking skills is data wrangling.  Data are rarely 
in the exact form that you need them.  I am currently working on an example for 
AIFH Vol 3 that will use a SOM and compare nations based on several statistics.  I could 
not find a dataset that fit exactly what I was looking for.  So I decided to create my 
own dataset.&lt;/p&gt;

&lt;p&gt;I wanted a list of countries with three different data points that somehow indicate that 
nation’s prosperity.  I chose GDP, lifespan and literacy rate.  Remember, this is a 
computer science experiment, not a sociology experiment.  I am sure others could come up 
with a much better set of data points to compare countries.  However, for my example 
program these will work just fine.&lt;/p&gt;

&lt;p&gt;I could not find a data set that was already completed.  However, all of this data is 
contained in Wikipedia.  To wrangle the data I created a simple Python script to 
accomplish this.  I am really starting to like Python for quick scripting projects.&lt;br /&gt;
I could have also used R, Groovy, Perl or a host of others.  The end result looks 
something like this:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;code,country,gdp,lifespan,literacy
AFG,Afghanistan,20650,60,0.431
ALB,Albania,12800,74,0.98
DZA,Algeria,215700,73.12,0.918
AND,Andorra,4800,84.2,1.0
AGO,Angola,124000,52,0.826
ATG,Antigua and Barbuda,1220,75.8,0.984
[Full File]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can download the entire contents of Wikipedia into a data file.  This is usually how 
you should deal with Wikipedia data.  Do not use HTTP to pull large volumes of data from 
Wikipedia.  This is a good way to get blocked from Wikipedia.  Also, the datafile for 
Wikipedia is not HTML encoded and much easier to parse.  I simply pulled the &lt;a href=&quot;http://en.wikipedia.org/wiki/ISO_3166-1&quot;&gt;nation codes&lt;/a&gt;
page, &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29&quot;&gt;GDP&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_literacy_rate&quot;&gt;literacy&lt;/a&gt;, and &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy&quot;&gt;lifespan&lt;/a&gt; pages into text files that my Python script could parse.&lt;/p&gt;

&lt;p&gt;I linked the files together (joined) using the nation name as a key.  If a nation’s name 
did not appear in all lists I discarded that nation.&lt;/p&gt;

&lt;p&gt;You can see my Python code &lt;a href=&quot;https://github.com/jeffheaton/aifh/blob/master/vol3/misc/nations/build_nations.py&quot;&gt;here&lt;/a&gt;.  This code could be more readable.  But it gets the job 
done.  It is a quick data wrangling hack.  If I needed to re-pull the data on a frequent 
basis, particularly if it were high-velocity data, I would do something more formal.&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Dec 2014 07:26:00 -0600</pubDate>
        <link>http://www.heatonresearch.com/datascience/2014/12/27/quick-dirty-data-wrangling.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/datascience/2014/12/27/quick-dirty-data-wrangling.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>First time at the NSU Campus - Fall 2014</title>
        <description>&lt;p&gt;I just returned from my first cluster meeting for PhD program in computer science at Nova 
Southeastern University.  Because it was Labor Day weekend my wife went with me, and we 
made it a mini Florida vacation.  We flew into Ft. Lauderdale on Wed, Sept 27, 2014.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wednesday:&lt;/strong&gt; Travel day, and check into hotel.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thursday:&lt;/strong&gt; program orientation, library tour and kickoff reception.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Friday:&lt;/strong&gt; Two class sessions (4-hours each), with a 1.5 hr break for lunch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Saturday:&lt;/strong&gt; Two class sessions (4-hours each), with a 1.5 hr break for lunch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sunday:&lt;/strong&gt; Vacation day with my wife.  We checked out the beach at Ft. Lauderdale and had a a canal tour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monday:&lt;/strong&gt; (Labor day, USA holiday): Flew back to St. Louis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I stayed at the closest hotel to the &lt;a href=&quot;http://www.ihg.com/holidayinnexpress/hotels/us/en/fort-lauderdale/fllsr/hoteldetail&quot;&gt;NSU campus&lt;/a&gt;, the &lt;a href=&quot;http://www.ihg.com/holidayinnexpress/hotels/us/en/fort-lauderdale/fllsr/hoteldetail&quot;&gt;Holiday Inn Airport&lt;/a&gt;.  The hotel is a good value.  They offer a free breakfast and shuttle to/from the airport.  It is nearly a 1.2 mile walk to the campus.  The roads are walk-able and have crosswalks.  In October I might walk it and shower on campus before class.  This would save the rental car expense.  I did walk to the university a few times this trip, however, more for exercise.  After a 1.2 mile walk, in the hot Florida summer sun, I would not be very popular!&lt;/p&gt;

&lt;h2 id=&quot;program-orientation&quot;&gt;Program Orientation&lt;/h2&gt;

&lt;p&gt;The program orientation was lead by &lt;a href=&quot;http://cec.nova.edu/faculty/seagull.html&quot;&gt;Dr. Seagull&lt;/a&gt;, 
the Associate Dean of Academic Affairs at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;GSCIS&lt;/a&gt;. He presented an overview of the program 
and the school.  NSU was founded in 1964, and is currently celebrating their 
50th anniversary.  There were many different 50th anniversary banners and sings throughout 
the campus.  For my program I must complete the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;32 credit hours of course work.&lt;/strong&gt;  This will amount to 8 individual four-credit hour courses.  I am currently enrolled in my first two. I will need to fly to Ft. Lauderdale twice a semester for these courses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;8 credit hours of directed research.&lt;/strong&gt;  It will be at least a year before I start this part. However, my understanding is that I will work on a research problem with one of the professors.  This should help prepare for my own dissertation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;24(or more) dissertation hours.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The faculty is composed of all full-time professors for the doctoral programs.  For my 
first two classes, both instructors are full-time and live in the Florida area and had 
been with NSU for over a decade.&lt;/p&gt;

&lt;h2 id=&quot;artificial-intelligence-class&quot;&gt;Artificial Intelligence Class&lt;/h2&gt;

&lt;p&gt;The artificial intelligence class (CISD 760) is taught by &lt;a href=&quot;http://www.cec.nova.edu/faculty/mukherjee.html&quot;&gt;Sumitra Mukherjee&lt;/a&gt; 
and uses the textbook &lt;a href=&quot;http://aima.cs.berkeley.edu/&quot;&gt;Artificial Intelligence a Modern Approach&lt;/a&gt;.  For the first class sessions, the professor lectured on path finding, modeling, optimization, and Bayesian inference. We saw neural networks, genetic algorithms, decision trees, Bayesian belief networks and other algorithms.&lt;/p&gt;

&lt;p&gt;The professor also covered the assignments for the semester.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 1:&lt;/strong&gt; Select a peer reviewed paper in your research area of interest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 2:&lt;/strong&gt; Complete programs for four AI problems.  Path finding, vector optimization, data science/predictive modeling (neural net vs decision tree) and Bayesian inference.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 3:&lt;/strong&gt; Critique the paper from assignment 1, and write an “idea paper” describing further research you might like to pursue.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;In-class mid-term&lt;/strong&gt; at the next cluster meeting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Final examination&lt;/strong&gt; assignment completed over the last several weeks of the semester.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most PhD programs have &lt;a href=&quot;http://en.wikipedia.org/wiki/Prelims&quot;&gt;qualifying exams&lt;/a&gt;, and NSU’s CS PhD program breaks this requirement 
over 8 in-class mid-semester examinations.  For the AI class, this is accomplished with 
the mid-term assignment.&lt;/p&gt;

&lt;p&gt;I’ve already started the programming assignment and am making use of Python with DEAP, 
&lt;a href=&quot;http://www.numpy.org/&quot;&gt;Numpy&lt;/a&gt; and &lt;a href=&quot;http://www.numpy.org/&quot;&gt;scikit-learn&lt;/a&gt;.  I think this 
will be a great class.  The assignment gives a good 
chance to try out some of the AI algorithms.  The assignments also allow us to start 
thinking about dissertation topics in AI.  I plan to conduct my dissertation research in 
the field of AI.&lt;/p&gt;

&lt;h2 id=&quot;data-base-management-systems-class&quot;&gt;Data Base Management Systems Class&lt;/h2&gt;

&lt;p&gt;The data base management systems class (CISD 750) class is taught by &lt;a href=&quot;http://cec.nova.edu/~jps/&quot;&gt;Junping Sun&lt;/a&gt; using 
the textbook &lt;a href=&quot;http://www.amazon.com/gp/product/0131873253&quot;&gt;Database Systems: The Complete Book (2nd Edition)&lt;/a&gt;.  For the first class 
sessions, the professor lectured on a variety of topics in database theory.  This class is different than your typical “IT SQL” class.  This course is really more on the design and implementation of an actual database system.  I was familiar with the topics discussed, but I will have quite a bit of studying to do for this class.  Both professors seemed very knowledgeable of their respective areas, and very current on the latest research.&lt;/p&gt;

&lt;p&gt;The assignments for this course are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Research Proposal.&lt;/li&gt;
  &lt;li&gt;Research Report.&lt;/li&gt;
  &lt;li&gt;In-class mid-term at the next cluster meeting.&lt;/li&gt;
  &lt;li&gt;Final examination assignment completed over the last several weeks of the semester.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are supposed to select a research topic for this course.  KDD (Knowledge Discovery 
in Databases) is one of the topics.  This is the area I plan to research.  KDD is what 
computer science groups, such as the ACM, call Data Science.&lt;/p&gt;

&lt;p&gt;So far it looks like a good program.  I wanted to enter the world of academic publishing, 
but could not fit a traditional PhD program into my life.  This program will be quite a 
bit of work.  But, so far, the program looks like it will be a good fit for me.&lt;/p&gt;

&lt;h2 id=&quot;around-the-nsu-campus&quot;&gt;Around the NSU Campus&lt;/h2&gt;

&lt;p&gt;Here are some pictures that my wife and I took around the NSU campus.&lt;/p&gt;

&lt;p&gt;The campus is large, and it took some walking to learn my way around!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-1.jpg&quot; alt=&quot;Walking to class at NSU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the main entrance to the campus below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-2.jpg&quot; alt=&quot;Jeff in front of NSU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the Carl DeSantis building at NSU.  The Graduate School of Information and Computer Science is located here.  I spent most of my time in the building below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-3.jpg&quot; alt=&quot;Carl DeSantis Building&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, this is Ft. Lauderdale, Florida, I had to stop at the beach!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-4.jpg&quot; alt=&quot;Jeff at Ft. Lauderdale Beach&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ft. Lauderdale is made up of many interesting canals.  My wife and I also took a canal tour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-5.jpg&quot; alt=&quot;Ft. Lauderdale Canal Tour&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Sep 2014 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2014/09/02/first-visit-nsu.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2014/09/02/first-visit-nsu.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Washington University Mini Medical School</title>
        <description>&lt;p&gt;My wife and I just finished the &lt;a href=&quot;https://minimed.wustl.edu/&quot;&gt;Mini-Med series&lt;/a&gt; at &lt;a href=&quot;http://www.wustl.edu/&quot;&gt;Washington University in St. Louis&lt;/a&gt;.&lt;br /&gt;
Mini-Med is a program offered by WashU that allows laypeople to learn about medicine.&lt;br /&gt;
Each night is taught by a world class expert in their field.  The biographies of these 
lecturers is absolutely amazing!  These are some of the doctors who are pushing the 
boundaries of human understanding in medicine!&lt;/p&gt;

&lt;p&gt;There are three different courses offered in Mini-Med.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 1&lt;/strong&gt;: Lectures and some hands-on labs.  I learned to suture and used a laparoscopy simulator. I also got to tour the Washington University Genome Institute.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 2&lt;/strong&gt;: Lectures and more hands-on labs.  I saw specimens of human organs from cadavers, became CPR certified, learned to examine patients, attended a posture lab, and took a walking tour of the medical school.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 3&lt;/strong&gt;: Revolves more around patient stories and MD’s providing information on their cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Mini-Med courses are taught around the typical semester schedule.  There are no 
classes during the summer semester.  At the end of each class you are given a certificate, 
if you attended the required number of class sessions.  This is me at the Mini-Med&lt;br /&gt;
graduation ceremony!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-1.jpg&quot; alt=&quot;Jeff Graduates Mini-Med&quot; /&gt;
Jeff graduating Mini-Med 2&lt;/p&gt;

&lt;p&gt;I really enjoyed the hand-on labs.  I particularly enjoyed trying my hand at microsurgery.&lt;br /&gt;
My wife and I also earned our CPR certification.  Most of the labs were led by medical 
students, residents and postdocs. They were very knowledgeable, and it was interesting 
talking with some of them about their medical school journey. We really enjoyed learning 
from them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-2.jpg&quot; alt=&quot;Jeff trying his hand at microsurgery&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One night we saw human organ specimens from cadavers.  It was fascinating to be able 
to hold and examine vital organs.  We saw specimens of hearts, kidneys, bladders, 
the GI track, and even the brain!  Seeing two human brains was fascinating.&lt;br /&gt;
Being an AI/Machine Learning programmer, it was fascinating to see the “real thing”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-3.jpg&quot; alt=&quot;Jeff holding a human brain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My wife, Tracy, and I are both life-long learners.  We are both involved in graduate 
programs. So this is right up our alley.  Tracy is earning a masters degree in Spanish.&lt;br /&gt;
So she found it fascinating to see the actual “voice box” that she has seen many times 
in her linguistics texts.  It was fascinating for us both to learn valuable knowledge 
from outside out fields of study.&lt;/p&gt;

&lt;p&gt;I work as a data scientist for a life insurance company and also working on a doctorate 
of Computer Science.  I use predictive modeling for insurance underwriting.  Life 
underwriting has much affinity with medicine.  The two Mini-Med classes have given me 
valuable information for my job.  As a data scientist it is very useful to learn about 
the knowledge domain that you are attempting to analyze. In addition to Mini-Med I also 
worked on the &lt;a href=&quot;http://www.jeffheaton.com/2014/05/review-of-the-first-three-johns-hopkins-coursera-data-science-courses/&quot;&gt;Johns Hopkins Coursera Data Science specialization&lt;/a&gt; this semester.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 May 2014 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/learning/2014/05/20/washington-university-mini-medical-school.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/learning/2014/05/20/washington-university-mini-medical-school.html</guid>
        
        
        <category>learning</category>
        
      </item>
    
  </channel>
</rss>
