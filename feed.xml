<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Heaton Research</title>
    <description>Jeff Heaton is a data scientist, phd student and indie publisher.  Heaton Research is the homepage for his projects.
</description>
    <link>http://www.heatonresearch.com/</link>
    <atom:link href="http://www.heatonresearch.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 02 Jun 2016 06:08:59 -0500</pubDate>
    <lastBuildDate>Thu, 02 Jun 2016 06:08:59 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Presented Two Papers at IEEE SoutheastCON 2016 in Norfolk, VA</title>
        <description>&lt;p&gt;I presented two papers at the IEEE SoutheastCON 2016 conference in NorfolkVA on 
April 1, 2016:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;April, 2016: &lt;a href=&quot;https://www.xcdsystem.com/SoutheastCon/program/index.cfm?pgid=457&amp;amp;day=01&amp;amp;sid=18224&quot;&gt;An Empirical Analysis of Feature Engineering for Predictive Modeling&lt;/a&gt; &lt;a href=&quot;http://www.xcdsystem.com/SoutheastCon/abstract/finalpapers/FinalPaper_11.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;a href=&quot;https://github.com/jeffheaton/jh_public/raw/master/jheaton-southeastcon-2016-features-presentation.pdf&quot;&gt;[Slides]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;April, 2016: &lt;a href=&quot;https://www.xcdsystem.com/SoutheastCon/program/index.cfm?pgid=457&amp;amp;day=01&amp;amp;sid=18224&quot;&gt;Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms&lt;/a&gt; &lt;a href=&quot;http://www.xcdsystem.com/SoutheastCon/abstract/finalpapers/FinalPaper_12.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;a href=&quot;https://github.com/jeffheaton/jh_public/raw/master/jheaton_secon2016_freqitemset-presentation.pdf&quot;&gt;[Slides]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first article is very much related to my phd dissertation topic.  For this paper, 
I generated datasets for neural networks, support vector machines, random forests, and
gradient boosting machines trying to see what types of equations they could learn; and
more importantly, what types of equations they cannot learn.  My dissertation topic
is in the area of feature engineering, so I am very interested in what types of 
equation representations of features you can augment a model’s feature vector with to
enhance its predictive power.  This conference paper is based on research I did for my
phd, while I was exploring dissertation topics.&lt;/p&gt;

&lt;p&gt;The second article is on frequent itemset mining.  For this paper, I examined several
common frequent set mining items to see what effects the underlying dataset had on the
algorithm runtime.  Frequent itemsets are outside my research area.  This paper was
based on a paper that I wrote near the beginning of my phd program.&lt;/p&gt;

&lt;p&gt;Both of these papers relied heavily on experimentation, and my code is available at
my &lt;a href=&quot;https://github.com/jeffheaton/papers&quot;&gt;github site&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Mar 2016 07:00:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/2016/03/19/ieee-southeast-con-2016-papers.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/2016/03/19/ieee-southeast-con-2016-papers.html</guid>
        
        
      </item>
    
      <item>
        <title>Teaching a Deep Learning Graduate Course at Washington University</title>
        <description>&lt;p&gt;I will be teaching a graduate course at &lt;a href=&quot;https://engineering.wustl.edu/Pages/home.aspx&quot;&gt;Washington University in St. Louis&lt;/a&gt; for the 
fall 2016 semester in the &lt;a href=&quot;https://sever.wustl.edu/degreeprograms/information-systems/Pages/MSIS-Curriculum.aspx&quot;&gt;Master of Science in Information Systems (MSIS)&lt;/a&gt; program.  You can see the course description here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;T81 INFO 558	Applications of Deep Neural Networks&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. By using a 
combination of advanced training techniques neural network architectural components, it 
is now possible to train neural networks of much greater complexity. This course will 
introduce the student to deep belief neural networks, regularization units (ReLU), 
convolution neural networks and recurrent neural networks. High performance 
computing (HPC) aspects will demonstrate how deep learning can be leveraged both on 
graphical processing units (GPUs), as well as grids. Deep learning allows a model to 
learn hierarchies of information in a way that is similar to the function of the human 
brain. Focus will be primarily upon the application of deep learning, with some 
introduction to the mathematical foundations of deep learning. Students will use the 
Python programming language to architect a deep learning model for several of real-world 
data sets and interpret the results of these networks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I will make use of my &lt;a href=&quot;http://www.heatonresearch.com/book/aifh-vol3-deep-neural.html&quot;&gt;book on deep learning&lt;/a&gt;, as well as some supplementary information.
This will be a hands-on technical course, so we will use the Python programming language
to make use of deep neural networks.  At this point I believe that I will make use of
both the &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Google TensorFlow&lt;/a&gt; and &lt;a href=&quot;http://blogs.microsoft.com/next/2016/01/25/microsoft-releases-cntk-its-open-source-deep-learning-toolkit-on-github/&quot;&gt;Microsoft CNTK&lt;/a&gt;
I feel that both of these will give the students an easy install path, depending on their system.
Currently, CNTK has an easy install path for Linux and Windows (but not Mac); TensorFlow has 
an easy install path for Mac and Linux (but not Windows).  I am fond of the &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe package&lt;/a&gt; as well, but it does have an easy install path for Windows.
For my own use I make use of Theano, but it does not have a particularly easy install path for anything!  What every path I end up taking, I will update my books examples to include it.&lt;/p&gt;

&lt;p&gt;My schedule, subject to change, is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;08/29/2016: Class 01: Python for Machine Learning&lt;/li&gt;
  &lt;li&gt;09/05/2016: Labor Day, No class&lt;/li&gt;
  &lt;li&gt;09/12/2016: Class 02: Neural Network Basics, Ch1&lt;/li&gt;
  &lt;li&gt;09/19/2016: Class 03: Training a Neural Network, Ch 4, Ch 5&lt;/li&gt;
  &lt;li&gt;09/26/2016: Class 04: Introduction to TensorFlow&lt;/li&gt;
  &lt;li&gt;10/03/2016: Class 05: Modeling and Kaggle&lt;/li&gt;
  &lt;li&gt;10/10/2016: Class 06: Backpropagation, Ch 6&lt;/li&gt;
  &lt;li&gt;10/17/2016: Class 07: Neural Networks for Classification&lt;/li&gt;
  &lt;li&gt;10/24/2016: Class 08: Neural Networks for Regression&lt;/li&gt;
  &lt;li&gt;10/31/2016: Class 09: Preprocessing&lt;/li&gt;
  &lt;li&gt;11/07/2016: Class 10: Regularization and Dropout, Ch 12&lt;/li&gt;
  &lt;li&gt;11/14/2016: Class 11: Timeseries and Recurrent, Ch 13&lt;/li&gt;
  &lt;li&gt;11/21/2016: Class 12: Convolutional Neural Networks, Ch 10&lt;/li&gt;
  &lt;li&gt;11/28/2016: Class 13: Architecting Neural Networks, Ch 14&lt;/li&gt;
  &lt;li&gt;12/05/2016: Class 14: Special Applications of Neural Networks&lt;/li&gt;
  &lt;li&gt;12/12/2016: Class 15: GPU, HPC and Cloud&lt;/li&gt;
  &lt;li&gt;12/19/2016: Final Exam&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point I am planning on two of the assignments using &lt;a href=&quot;https://inclass.kaggle.com/&quot;&gt;Kaggle for Class&lt;/a&gt;.  I think it 
will be a great way to introduce the students to Kaggle.&lt;/p&gt;

&lt;p&gt;This will be my first time teaching a graduate course.  I’ve taught undergrad at &lt;a href=&quot;https://www.maryville.edu/&quot;&gt;Maryville University&lt;/a&gt; and &lt;a href=&quot;http://www.stlcc.edu/&quot;&gt;St. Louis Community College&lt;/a&gt;, 
but its been a few years.  I’ve always taught technical courses, such as various levels of Java, C++, C# and SQL. WUSTL is the 
university that I got my masters degree from, so I am somewhat familiar with them.&lt;/p&gt;

&lt;p&gt;I will have a website that will contain all of my course information, I will post more information on that once the course starts.&lt;/p&gt;
</description>
        <pubDate>Wed, 24 Feb 2016 06:00:00 -0600</pubDate>
        <link>http://www.heatonresearch.com/2016/02/24/teaching-deep-learning-washington-university.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/2016/02/24/teaching-deep-learning-washington-university.html</guid>
        
        
      </item>
    
      <item>
        <title>New Look for All My Websites: Jekyll and Github</title>
        <description>&lt;p&gt;I’ve consolidated my websites to a single domain, everything now points to &lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt;.  Previously I had:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jeffheaton.com&quot;&gt;http://www.jeffheaton.com&lt;/a&gt; - My blog, previously running &lt;a href=&quot;https://wordpress.com/&quot;&gt;Wordpress&lt;/a&gt;. Now lives at &lt;a href=&quot;http://www.heatonresearch.com/jeff&quot;&gt;http://www.heatonresearch.com/jeff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt; - My books and the Encog project, previously running a combination of &lt;a href=&quot;http://www.drupal.org&quot;&gt;Drupal&lt;/a&gt; and &lt;a href=&quot;https://www.mediawiki.org/wiki/MediaWiki&quot;&gt;MediaWiki&lt;/a&gt;.  Still lives at &lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aifh.org&quot;&gt;http://www.aifh.org&lt;/a&gt; - The examples for the current generation of my books: Artificial Intelligence for Humans.  Running on a site statically generated by &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The three sites above were hosted on two Amazon EC2 servers that cost about $165/month.&lt;br /&gt;
Additionally, keeping two UNIX hosts, Drupal and MediaWiki all patched was becoming a real 
chore. Additionally, weeding out spammer/hacker rootkits when I got behind on patches was 
a frequent nightmare. I replaced the whole mess with a static site generated by &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;.
These static files are hosted on &lt;a href=&quot;https://github.com/&quot;&gt;GitHub&lt;/a&gt;, with zero transaction costs, no 
matter how high my traffic goes (at least in theory).  However, even if traffic went over-the-top
and GitHub were to “boot me,” I could easily use Amazon S3 and just host my own static 
content.  The beauty of the generated static content is that endless headache of patches
and staying ahead of the spammer hackers is gone.&lt;/p&gt;

&lt;p&gt;I will miss some of the nice features of MediaWiki and Wordpress.  I won’t miss Drupal, 
and their witch’s brew of themes and modules, in the least.  Right now I am using a simple
theme for Jekyll, I will likely upgrade this to something &lt;a href=&quot;http://getbootstrap.com/&quot;&gt;bootstrap&lt;/a&gt; based soon.&lt;br /&gt;
Ideally, this will give me more time to do what I enjoy, writing AI software and tutorials,
and not play UNIX admin and SPAM cop all the time.&lt;/p&gt;

&lt;p&gt;I am trying to keep all dynamic features “off world”.  For now this includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Blog Comments - Handled through DISQUES.&lt;/li&gt;
  &lt;li&gt;Forum Features - My old Drupal forum was getting about 300 auto-deleted spam posts a day and around 30-40 manually deleted (by me) spam posts a day.  This become overwhelming and I shut down the forum.  I will probably try a Google Groups replacement of the forums at some point.&lt;/li&gt;
  &lt;li&gt;Support - I still do support through email and links to stack overflow questions.  Stack Overflow tends to be a good place for this, I often do not have time to answer every technical question sent to me.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am still organizing the flow and structure of the new website.  So you will probably see some changes as I continue to tweak it.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Sep 2015 10:43:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/09/27/using-jekyll-and-github.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/09/27/using-jekyll-and-github.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>My Experience with the Coursera Johns Hopkins Data Science Certification</title>
        <description>&lt;p&gt;This post is a summary of several posts that I had on my old blog about the &lt;a href=&quot;https://www.coursera.org/specializations/jhudatascience&quot;&gt;Johns
Hopkins Data Science certification&lt;/a&gt; offered by &lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;.
I am not sure how typical of a student I was for this program.  I currently work as a 
data scientist, have a decent background in AI, have a number of publications, and am 
currently completing a PhD in computer science.  So, a logical question is what did I 
want from this program?&lt;/p&gt;

&lt;p&gt;At the time I took this certification I had not done a great deal of R programming.  This 
program focused heavily on R.  I view this as both a strength and weakness of the program.&lt;br /&gt;
I am mostly a Java/Python/C#/C++ guy.  I found the R instruction very useful.
I’ve focused mainly in AI/machine learning, I hoped this program would fill in some gaps.
Curiosity.&lt;/p&gt;

&lt;p&gt;I really liked this program.  Courses 1-9 provide a great introduction to the predictive 
modelling side of data science.  Both machine learning and traditional regression models 
were covered.  R can be a slow and painful language, at times, but I was able to get 
through.  It is my opinion that R is primarily useful for ferrying data between models 
and visualization graphs.  It is not good for heavy-lifting and data wrangling.  The 
syntax to R is somewhat appalling.  However, it is a domain specific language (DSL), 
not a general purpose language like Python.  Don’t get me wrong.  I like R for setting up 
models and graphics.  Not for performing tasks better suited to a general purpose language.&lt;/p&gt;

&lt;p&gt;In a nutshell, here are my opinions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; With the exception of the capstone, very practical real-world data sets.  Experience with both black-box (machine learning) and more explainable (regression models) systems.  Introduction to Slidify and Shiny, I’ve already used both in my day-to-day job.  It takes some real work and understanding to make it through this program.  The last three courses rocked!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; Peer review is really hit or miss.  More on this later.  Some lecture material was sub-par (statistical inference) compared to Khan Academy.  Only reinvent the wheel if you are going to make a better wheel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;course-breakdown&quot;&gt;Course Breakdown&lt;/h2&gt;

&lt;p&gt;Here are my quick opinions on some of these courses.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The Data Scientist’s Toolbox&lt;/strong&gt;: Basically, can you install R, RStudio and use GitHub.  I’ve already done all three so I got little from this course.  If you have not dealt with R, RStudio and GitHub this class will be a nice slow intro to the program.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;R Programming&lt;/strong&gt;: I enjoyed this course!  It was hard, considering I was taking classes #1 and #3 at the same time.  If you had no programming experiance, this course will be really hard!  Be ready to supplement the instruction with lots of Google Searching.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Getting and Cleaning Data&lt;/strong&gt;: Data wrangling is an important part of data science.  Getting data into a tidy format is important.  This course used quite a bit of R programming.  For me, not being an R programmer and taking course #2 at the same time meant extra work.  If you are NOT an advanced programmer already DO NOT take #2 and #3 at the same time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploratory Data Analysis&lt;/strong&gt;: This was a valuable class, it taught you all about the R graphing packages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reproducible Research&lt;/strong&gt;:  Valuable course!  Learning about R markdown was very useful, I am already using this in one of my books to make sure that several of my examples are reproducible by providing a RMD script to produce all the charts from my book.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Inference&lt;/strong&gt;:  This was an odd class.  I already knew statistical inference and did quite well despite not watching any lectures (hardly).  I don’t believe this course made anyone happy (hardly).  Either you already knew the topic and were bored, or you were completely lost trying to learn statistics for the first time.  There are several Khan academy videos that cover all the material in this course.  Why dose Hopkins need to reproduce this?  Is this not the point of MOOC?  Why not link to the Khan academy videos and test the students.  Best of both worlds!  Also, 90% of the material was not used in the rest of the course, so I suspect many students might have been left wondering why this course is for.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regression Models&lt;/strong&gt;: Great course, this is the explainable counterpart to machine learning.  You are introduced to linear regression and GLM’s.  This course was setup as the perfect counterpart to #8. My only beef on this course was that I got screwed by peer review.  More on this later.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Practical Machine Learning&lt;/strong&gt;: Great course.  This course showed some of the most current model types in data science: Gradient Boosting Machines (GBMs) and Random Forests.  Also a great description of boosting and an awesome Kaggle like assignment where you submitted results from your model to see if you can match a “hidden data set”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developing Data Products&lt;/strong&gt;: Great course.  I really enjoyed playing with shiny, and even used it for one of the examples in my upcoming book.  You can see my shiny project here. https://jeffheaton.shinyapps.io/shiny/  and writeup here.  http://slidify.org/publish.html  They encouraged you to post these to GitHub and public sites, so I assume I am not violating anything by posting here.  Don’t plagiarize me!!!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Bad ending to an otherwise great program.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;peer-reviewed-grading&quot;&gt;Peer Reviewed Grading&lt;/h2&gt;

&lt;p&gt;If you are not familiar with peer review grading, here is how it works.  For each project 
you are given four counterparts that review and grade your assignment.  This is mostly 
double-blind, as neither the student or reviewer knows the other.  I used my regular 
GitHub account on all assignments.  So it was pretty obvious who I was.  I was even 
emailed by a grader once who recognized me from my open source projects.  Your grade is an 
average of what those four people gave you.  At $49 a course maybe this is the only way 
they can afford grade.  I currently spend nearly 100 times that for each of my 
PhD courses. :(&lt;/p&gt;

&lt;p&gt;Overall, peer review grading worked good for me in all courses but one.  Here are some of 
my concerns on peer grading.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You probably have many graders who are pressed for time and just give high-marks without  much thought.  (just a guess/opinion)&lt;/li&gt;
  &lt;li&gt;You are going to be graded by people who may not have not gotten the question correctly in the first place.&lt;/li&gt;
  &lt;li&gt;You are instructed NOT to run the R program.  So now I am being graded on someone’s ability to mentally compile and execute my program?&lt;/li&gt;
  &lt;li&gt;Each peer is going to apply different standards.  You could get radically different marks depending on who your four peers were.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So here is my story in the one case where peer review did not work for me.  I in the 
upper 98-99% range on most of these courses.  Except for course #8.  I had good scores 
going into the final project.  However, two of my peers knocked me for these reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two of my peers could not download my file from Coursera.  Yet the other two had no problem.  Fine, so I get a zero because someone’s ISP was flaking out.&lt;/li&gt;
  &lt;li&gt;Two of peers did not give me credit because the felt I had not used RMD for my report?? (which I had) Fine, so I lose a fair amount of points because two random peers did not know what RMD output looks like.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This took a toll on my grade, I still passed.  But this is the one course I did not get 
“with distinction” credit.  Yeah big deal.  In the grand scheme of things I don’t really 
care.  Just mildly annoying.  However, if you are hovering near a 70%, and you get one or 
two bad reviewers you are probably toast.&lt;/p&gt;

&lt;h2 id=&quot;capstone-project&quot;&gt;Capstone Project&lt;/h2&gt;

&lt;p&gt;The capstone project was to produce a program similar to &lt;a href=&quot;https://swiftkey.com/en&quot;&gt;Swiftkey&lt;/a&gt;, the company that was 
the partner/sponsor for the capstone.  If you are not familiar with Swiftkey, it attempts 
to speed mobile text input by predicting the next word you are going to type.  For example, 
you might type “to be or not to &lt;em&gt;__&lt;/em&gt;”.  The application should fill in “be”.  The end 
program had to be written in R and deployed to a Shiny Server.&lt;/p&gt;

&lt;p&gt;This project was somewhat flawed in several regards.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Natural Language Processing was not covered in the course.  Neither was unstructured data.  The only material provided on NLP was a handful of links to sites such as Wikipedia.&lt;/li&gt;
  &lt;li&gt;The first 9 courses had a clear direction.  However, less than half of them had anything to do with the capstone.&lt;/li&gt;
  &lt;li&gt;The project is not typical of what you would see in most businesses as a data scientist.  It would have been better to do something similar to Kaggle or one of the KDD cups.&lt;/li&gt;
  &lt;li&gt;In my opinion, R is a bad choice for this sort of project.  During the meetup with Swiftkey, they were asked what tools they used.  R was not among them. R is so cool for many things, why not showcase its abilities?&lt;/li&gt;
  &lt;li&gt;Student peer review is &lt;a href=&quot;https://www.insidehighered.com/blogs/hack-higher-education/problems-peer-grading-coursera&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://gregorulm.com/a-critical-view-on-courseras-peer-review-process/&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://moocnewsandreviews.com/massive-mooc-grading-problem-stanford-hci-group-tackles-peer-assessment/&quot;&gt;bad&lt;/a&gt;… But it might be the &lt;a href=&quot;http://simplystatistics.org/2013/03/26/an-instructors-thoughts-on-peer-review-for-data-analysis-in-coursera/&quot;&gt;only choice&lt;/a&gt;.  The problem with peer review is you have three random reviewers.  They might be easy, they might be hard.  They might penalize you for the fact that they don’t know how to load your program! (this happened to me on a previous coursera course).&lt;/li&gt;
  &lt;li&gt;Perfect scores on the quizzes were really not possible.  We were given several sample sentences to predict.  The sentences were very specialized and no model would predict them correctly.  The Swiftkey surely did not.  Using my own human intuition and several text mining apps I wrote in Java, I did get 100% on the quizzes.  Even though the instructions clearly said to use your final model.  Knowing I might draw a short straw on peer review, I opted to do what I could to get max points.  I don’t care about my grade, but falling below the cutoff for a bad peer review would not be cool!&lt;/li&gt;
  &lt;li&gt;Marketing based rubric for final project.  One of the grading criteria posted the question, “Would you hire this person?”  Seriously?  I do participate in the hiring process for data scientists.  I would never hire someone without meeting them, performing a tech interview, and small coding challenge.  I hope this stat is not used in marketing material.  xx% of our graduates produced programs that might land them a job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After spending several days writing very slow model building code in R, I eventually dropped it and used Java and OpenNLP to write code that would build my model in under 20 minutes.  Others ran into the same issues.  There are somewhat kludge interfaces between R and OpenNLP, Weka and OpenNLP.  But these are native Java apps.  I  just skipped the kludge and built my model in Java and wrote a Shiny app to use the model in R.  This was enough to pass the program.  I was not alone in this approach, based on forum comments.&lt;/p&gt;

&lt;p&gt;Okay, I will just say it.  I thought this was a bad capstone.  This was just my experience 
on the first run of the certification; hopefully, they’ve improved it since.  The rest of 
the program was  really good!  If I could make a suggestion, I would say to let the 
students choose a Kaggle competition to compete.  The Kaggle competitions are closer 
to the sort of data real data scientists will see.  I am proud of the certificate that I earned.&lt;br /&gt;
If I were interviewing someone who had this certificate I would consider it a positive.&lt;br /&gt;
The candidate would still need to go through a standard interview/evaluation process.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Great program.  It won’t make you a star data scientist, but it will give you a great 
foundation to go from.  Kaggle might be a good next step.  Another might be a blog and 
doing some real, and interesting data science to showcase your skills!  This is somewhat 
how I got into data science.&lt;/p&gt;

&lt;p&gt;A question that I am often asked, is what would I think of this certification, if I saw it
on the resume of a new data scientist that I was interviewing.  In isolation, I would not
give a hire recommend based solely on this certification.  However, it would show me that
someone has mastered the basics of data science.  They know what format data needs to be
in for predictive modeling.  They know their way around the R-programming language.  They
also took the initiative to undertake something that took a decent amount of effort.&lt;br /&gt;
So yes, it is important, particularly, if your resume is lacking in the analytics area.&lt;/p&gt;
</description>
        <pubDate>Sat, 26 Sep 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Visiting Spain and the GECCO Confrence</title>
        <description>&lt;p&gt;I spent six weeks in Spain this summer.  My wife, Tracy, is earning a &lt;a href=&quot;http://www.slu.edu/department-of-languages-literatures-and-cultures/programs-of-study/spanish/graduate-studies&quot;&gt;Master’s Degree in 
Spanish&lt;/a&gt; from &lt;a href=&quot;http://www.slu.edu/&quot;&gt;St. Louis University (SLU)&lt;/a&gt;.  Her university has an extension &lt;a href=&quot;http://spain.slu.edu/&quot;&gt;campus in Madrid&lt;/a&gt;.&lt;br /&gt;
I also wanted to be in Spain to visit the &lt;a href=&quot;http://www.sigevo.org/gecco-2015/&quot;&gt;GECCO-2015&lt;/a&gt; conference, that was held in Madrid.&lt;br /&gt;
We were able to line up a nice apartment in &lt;a href=&quot;https://en.wikipedia.org/wiki/Salamanca_(Madrid)&quot;&gt;Salamanca, Madrid&lt;/a&gt; and I was able to take a 
few weeks vacation and work remotely the rest of the time.  It was totally worth it, Spain 
is an amazing country.  I know just enough Spanish to get around and usually be fed at a 
restaurant!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-08-30-madrid-gecco-1.png&quot; alt=&quot;Madrid and GECCO&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GECCO conference was awesome.  I was able to briefly meet &lt;a href=&quot;http://www.cs.ucf.edu/~kstanley/&quot;&gt;Dr. Kenneth Stanley&lt;/a&gt; after 
hearing a tutorial session held by him.  I’ve many of Dr. Stanley’s papers and implemented 
his NEAT/HyperNEAT algorithms.  He also has a &lt;a href=&quot;http://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&quot;&gt;new book&lt;/a&gt; out that I’ve just read and recommend.&lt;br /&gt;
The book points out that it is very difficult to design objective functions that might 
truly create amazing results.  Some of the best results on &lt;a href=&quot;http://picbreeder.org/&quot;&gt;Ken’s PicBreader&lt;/a&gt; site were not 
specifically planned.&lt;/p&gt;

&lt;p&gt;I also saw a fascinating Lisp-based language named &lt;a href=&quot;http://faculty.hampshire.edu/lspector/push3-description.html&quot;&gt;Push3&lt;/a&gt;, created by &lt;a href=&quot;http://faculty.hampshire.edu/lspector/&quot;&gt;Dr. Lee Spector&lt;/a&gt;.&lt;br /&gt;
Genetic programming ha always been fascinating to me because it uses AI to actually 
evolve AI.  Push3 is a programming language designed for computers to evolve.&lt;br /&gt;
Dr. Spector’s group is evolving computer programs, to accomplish basic tasks like word 
count and other basic utilities.  Most machine learning algorithms are simply about 
optimizing coefficients to decrease a supervised training set’s error.  It is amazing to 
see algorithms that can actually evolve themselves into new algorithms.&lt;/p&gt;

&lt;p&gt;I plan to make use of genetic programming for part of my dissertation.  Of course, I am 
still finishing up coursework and this is very much in flux.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Aug 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>My First Kaggle Competition</title>
        <description>&lt;p&gt;I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it, 
&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; is an ongoing forum for competitive data science. Individuals and teams compete to 
create the best model for data sets provided by industry and sometimes academia.&lt;br /&gt;
Individuals who enter are ranked as either Novice, Kaggler and &lt;a href=&quot;https://www.kaggle.com/wiki/UserRankingAndTierSystem&quot;&gt;Kaggle Master&lt;/a&gt;.  To become 
a Kaggle master, one must place in the top 10% of two competitions; and in one of the top 
10 slots of a third competition.&lt;/p&gt;

&lt;p&gt;I’ve talked about Kaggle in many of my presentations.  I’ve also used Kaggle data in 
my books. Until now, I had yet to actually enter a Kaggle competition.  I decided it was 
finally time to try this for myself. I competed in the &lt;a href=&quot;http://otto%20group%20product%20classification%20challenge/&quot;&gt;Otto Group Product Classification&lt;/a&gt; 
Challenge that ended on May 18th, 2015.  My score was sufficient to land in the top 10%, 
so I’ve completed one of the requirements for Kaggle master.  My Kaggle profile can be 
seen here.&lt;/p&gt;

&lt;p&gt;My goals for entering were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;See how hard Kaggle actually is, and move towards a Kaggle master designation.&lt;/li&gt;
  &lt;li&gt;Learn from the other Kagglers and forums.&lt;/li&gt;
  &lt;li&gt;Build a basic toolkit that I will use for future Kaggle competitions.&lt;/li&gt;
  &lt;li&gt;Gain an example (from my entry) for the &lt;a href=&quot;http://www.heatonresearch.com/aifh&quot;&gt;Artificial Intelligence for Humans series&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Maybe get an idea or two for my future dissertation (I am a phd student at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-otto-classification-challenge&quot;&gt;The Otto Classification Challenge&lt;/h2&gt;

&lt;p&gt;First, I will give a brief introduction to the exact nature of the Otto Classification 
Challenge.  For a complete description, refer to the Kaggle description(&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge&quot;&gt;found here&lt;/a&gt;).&lt;br /&gt;
This challenge was introduced by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Otto_GmbH&quot;&gt;Otto Group&lt;/a&gt;, who is the world’s largest mail order 
company and currently one of the biggest &lt;a href=&quot;http://en.wikipedia.org/wiki/E-commerce&quot;&gt;e-commerce&lt;/a&gt; companies, mainly based in Germany 
and France but operating in more than 20 countries.  They have many products sold over 
numerous countries.  They would like to be able to classify these products into 9 
categories, using 93 features (columns).  These 93 columns represent counts, and are 
often zero.&lt;/p&gt;

&lt;p&gt;The data are completely redacted.  You do not know what the 9 categories are, nor do you 
know the meaning behind the 93 features.  You only know that the features are integer 
counts. Most Kaggle competitions provide you with a test and training dataset.  For the 
training dataset you are given the outcomes, or correct answers.  For the test set, you 
are only given the 93 features, and you must provide the outcome.  The test and training 
sets are divided as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Test Data: 144K rows&lt;/li&gt;
  &lt;li&gt;Training Data: 61K rows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You do not actually submit your model to Kaggle.  Rather, you submit your predictions 
based on the test data.  This allows you to use any platform to make these predictions.&lt;br /&gt;
The actual format of a submission for this competition is the probability of each of 
the 9 categories being the outcome.  This is not like a university multiple choice test 
where you must submit your answer as A, B, C, or D.  Rather, you would submit your 
answer as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A: 80% probability&lt;/li&gt;
  &lt;li&gt;B: 16% probability&lt;/li&gt;
  &lt;li&gt;C: 2% probability&lt;/li&gt;
  &lt;li&gt;D: 2% probability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wish college exams were graded like this!  Often I am very confident about two of the 
answers, and can eliminate the other two.  Simply assign a probability to each, and you 
get a partial score.  If A were the correct answer for the above, I would get 80% of the 
points.&lt;/p&gt;

&lt;p&gt;The actual Kaggle score is slightly more complex than that.  Rather, you are graded on a 
logarithm based scale and are very heavily penalized for having a lower probability on 
the correct answer. The following are a few lines from my submission:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1,0.0003,0.2132,0.2340,0.5468,6.2998e-05,0.0001,0.0050,0.0001,4.3826e-05
2,0.0011,0.0029,0.0010,0.0003,0.0001,0.5207,0.0013,0.4711,0.0011
3,3.2977e-06,4.1419e-06,7.4524e-06,2.6550e-06,5.0014e-07,0.9998,5.2621e-06,0.0001,6.6447e-06
4,0.0001,0.6786,0.3162,0.0039,3.3378e-05,4.1196e-05,0.0001,0.0001,0.0006
5,0.1403,0.0002,0.0002,6.734e-05,0.0001,0.0027,0.0009,0.0297,0.8255&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Each line starts with a number that specifies the data item that is being answered.&lt;br /&gt;
The sample above shows the answers for items 1-5.  The next 9 values are the probabilities 
for each of the product classes.  These probabilities must add up to 1.0 (100%).&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned-from-kaggle&quot;&gt;What I Learned from Kaggle&lt;/h2&gt;

&lt;p&gt;If you want to do well in Kaggle, the following are very important topics, along with 
the tools I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep Learning - Using &lt;a href=&quot;http://h2o.ai/&quot;&gt;H2O&lt;/a&gt; and &lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;Lasagne&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gradient Boosting Machines (GBM) - &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;Using XGBOOST&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ensemble Learning - &lt;a href=&quot;http://www.numpy.org/&quot;&gt;Using NumPy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Feature Engineering - Using NumPy and &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two areas that I learned the most about, during this challenge, were GBM parameter 
tuning and ensemble learning.  I got pretty good at tuning a GBM.  The individual scores 
for my GBM’s were in line with those used by the top teams.&lt;/p&gt;

&lt;p&gt;Before Kaggle I typically used only one model, if I were using neural networks, I just 
used neural networks.  If I were using an SVM, Random Forest or Gradient Boosting, I stuck 
to just that model.  With Kaggle, it is critical to use multiple models, ensembled to 
produce better results than each of the models could produce independently.&lt;/p&gt;

&lt;p&gt;Some of my main takeaways from the competition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GPU is really important for deep learning.  It is best to use a deep learning package that supports it, such as H2O, Theano or Lasagne.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://lvdmaaten.github.io/tsne/&quot;&gt;t-sne&lt;/a&gt; visualization is awesome for high-dimension visualization and creating features.&lt;/li&gt;
  &lt;li&gt;I need to learn to ensemble better!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This competition was the first time I used &lt;a href=&quot;http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&quot;&gt;T-SNE&lt;/a&gt;.  It works like PCA in that it is capable 
of reducing dimensions, however, the data points separate in such a way that the 
visualization is often clearer than &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt;. This is done using a stochastic nearest 
neighbor process. I plan to learn more about how t-sne actually performs the reduction, 
compared to PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-1.jpeg&quot; alt=&quot;t-SNE Plot of the Otto Group Challenge&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-approach-to-the-otto-challenge&quot;&gt;My Approach to the Otto Challenge&lt;/h2&gt;

&lt;p&gt;So far I’ve only worked with single model systems.  I’ve used models that contain ensembles 
that are “built in”, such as random forests and gradient boosting machines.  However, it 
is possible to create higher-level ensembles of these models.  I used a total of 20 models, 
this included 10 deep neural networks and 10 gradient boosting machines.  My deep neural 
network system provided one prediction and my gradient boosting machines provided the other.&lt;br /&gt;
These two predictions were blended together, using a simple ratio.  The resulting prediction 
vector was then normalized so that the sum equaled 1.0(100%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-2.png&quot; alt=&quot;Jeff Heaton&#39;s Kaggle Model for the Otto Group&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I did not remove or engineer any fields.  For both model types I converted all 93 
attributes into Z-Scores.  For the neural network I normalized all values to be in a 
specific range.&lt;/p&gt;

&lt;p&gt;My 10 deep learning neural networks used a simple bagging method.  I averaged the 
predictions from 20 different neural networks.  Each of these neural networks was created 
by choosing a different 80/20 split between training and validation.  The neural network 
was trained on the training data until the validation score did not improve for 25 epochs.&lt;br /&gt;
Once training stopped I used the weights from the epoch that produced the highest training 
score. This process is a simple form of bagging called &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot;&gt;bootstrap aggregation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My 10 gradient boosting machines (GBM) were each components of a 10-fold cross-validation.&lt;br /&gt;
I essentially broke the Kaggle training data into 10 folds and used each of these folds 
as a validation set, and the others as training.  This produced 10 gradient boosting machines.&lt;br /&gt;
I then used an NxM coefficient matrix to blend each of these together.  Where N is the 
number of models, M is the number of features.  In this case it was a 10x9 grid.  This 
matrix weighted each of the 10 model’s predictive power in each of the 9 categories.&lt;br /&gt;
These coefficients were a straight probability calculation from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt; of 
each of the 10 models.  This allowed each model to potentially specialize in each of the 
9 categories.&lt;/p&gt;

&lt;p&gt;I spent considerable time tuning my GBM.  I used Nelder-Mead searches to optimize my 
hyper-parameter vector.  I ultimately settled on the following parameters:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;max_depth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;min_child_weight&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;subsample&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;gamma&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;colsample_bytree&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;eta&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;threads&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approaches&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GBM&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produced&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;separate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;submission&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blended&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;together&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weighting&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.65&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gave&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;me&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;what-worked-well-for-top-teams&quot;&gt;What Worked Well for Top Teams&lt;/h2&gt;

&lt;p&gt;The top Kaggle teams made use of more sophisticated ensemble techniques than I did.&lt;br /&gt;
This will be my primary learning area for the next competition.  You can read about 
some of the top models here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov&quot;&gt;The Top Scoring Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score&quot;&gt;Relatively Simple Model for a Top 25 Score&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models&quot;&gt;Share Your Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent&quot;&gt;One of the Top Ten&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging&quot;&gt;TSNE &amp;amp; Meta-Bagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above write-ups are very useful, I’ve already started examining their approaches.&lt;/p&gt;

&lt;p&gt;Some of the top technologies discussed were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Engineering&lt;/li&gt;
  &lt;li&gt;Input Transformation - good write up &lt;a href=&quot;http://fmwww.bc.edu/repec/bocode/t/transint.html&quot;&gt;here&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;log transforms&lt;/li&gt;
      &lt;li&gt;sqrt(x + 3/8) - Not sure what this one is called, but I saw it used a few times&lt;/li&gt;
      &lt;li&gt;z-score transforms&lt;/li&gt;
      &lt;li&gt;ranged transformation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hyperparameter Optimization
    &lt;ul&gt;
      &lt;li&gt;Nelder-Mead&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/JasperSnoek/spearmint&quot;&gt;Spearmint&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will probably not enter another Kaggle until the fall of this year.  This blog post 
will be updated to contain my notes as I investigate other techniques for this 
competition.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 May 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
      <item>
        <title>Quick R Tutorial: The Big-O Chart</title>
        <description>&lt;p&gt;I needed an original &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;Big-O&lt;/a&gt; chart for a publication.  This tutorial does not cover what 
Big-O actually is, just how to chart it.  If you want more information on Big-O I 
recommend reading &lt;a href=&quot;http://bigocheatsheet.com/&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;this&lt;/a&gt;.  I know, go figure, for a computer science student. I 
really like using R for all things chart and visualization.  So, I turned to R for this 
task.  While this is a very simple chart, it does demonstrate several very common 
tasks in R:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overlaying several plots&lt;/li&gt;
  &lt;li&gt;Adding a legend to a plot&lt;/li&gt;
  &lt;li&gt;Using “math symbols” inside of the text on a chart&lt;/li&gt;
  &lt;li&gt;Stripping off all the extra whitespace that R likes to generate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the end-result here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-21-r_big_o-1.png&quot; alt=&quot;Computer Science Big-O Chart in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This was produced with the following R code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Remove white space on chart
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;par&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;+0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The x &amp;amp; y limits for the plot
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Just use R&#39;s standard list of colors for the lines (I am too tired to be creative this morning)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot each of the equations that we are interested in. Note the add=TRUE
# it causes them to overlay each other.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Elements(N)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Operations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generate the legend, note the use of expression to handle n to the power of 2.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;topright&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;O(1)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;O(log(n))&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;O(n)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;O(n log(n))&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;O(!n)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;n&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sat, 21 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o.html</guid>
        
        
        <category>compsci</category>
        
        <category>r</category>
        
      </item>
    
      <item>
        <title>Using Encog to Replicate Research</title>
        <description>&lt;p&gt;For the one of my PhD courses at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University (NSU)&lt;/a&gt; it was necessary to 
reproduce the research of the following paper:&lt;/p&gt;

&lt;p&gt;I. Ahmad, A. Abdullah, and A. Alghamdi, “Application of artificial neural network in 
detection of probing attacks,” in IEEE Symposium on Industrial Electronics Applications, 
2009. ISIEA 2009., vol. 2, Oct 2009, pp. 557–562.&lt;/p&gt;

&lt;p&gt;This paper demonstrated how to use a neural network to build a &lt;a href=&quot;http://en.wikipedia.org/wiki/Intrusion_detection_system&quot;&gt;basic intrusion detection 
system (IDS)&lt;/a&gt; for the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;KDD99 dataset&lt;/a&gt;.  It is important to reproduce research, in an academic 
setting.  This means that you were able to obtain the same results as the original 
researchers, using the same techniques.  I do this often when I write books or implement 
parts of Encog. This allows me convince myself that I have implemented an algorithm 
correctly, and as the researchers intended.  I don’t always agree with what the original 
researcher did.  If I change it, when I implement Encog, I am now in the area of “original 
research,” and my changes must be labeled as such.&lt;/p&gt;

&lt;p&gt;Some researchers are more helpful than others for replication of research.  Additionally, 
neural networks are stochastic (they use random numbers).  Basing recommendations off of 
a small number of runs is usually a bad idea, when dealing with a stochastic system.&lt;br /&gt;
Their small number of runs caused the above researchers to conclude that two hidden layers 
was optimal for their dataset.  Unless you are dealing with deep learning, this is almost 
always not the case.  The universal approximation theorem rules out more than a single 
layer for the old-school sort of perceptron neural network used in this paper.&lt;br /&gt;
Additionally, the vanishing gradient problem prevents the RPROP training that the 
researchers from fitting well with larger numbers of hidden layers.  The researchers 
tried up to 4 hidden layers.&lt;/p&gt;

&lt;p&gt;For my own research replication I used the same dataset, with many training runs to make 
sure that their results were within my high-low range.  To prove that a single layer does 
better I used &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;ANOVA&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot;&gt;Tukey’s HSD&lt;/a&gt; to show that differences among the different neural 
network architectures were indeed statistically significant and my box and whiskers plot 
shows that training runs with a single layer more consistently converged to a better 
mean &lt;a href=&quot;http://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;&gt;RMSE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am attaching both my paper and code in case it is useful.  This is a decent tutorial 
on using the latest Encog code to normalize and fit to a data set.&lt;/p&gt;

&lt;p&gt;The class also required us to write up the results in &lt;a href=&quot;http://www.ieee.org/conferences_events/conferences/publishing/templates.html&quot;&gt;IEEE conference format&lt;/a&gt;.  I am a 
fan of &lt;a href=&quot;http://www.latex-project.org/&quot;&gt;LaTex&lt;/a&gt;, so that is what I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source code Includes: &lt;a href=&quot;https://github.com/jeffheaton/phd/tree/master/ids-replicate-neural&quot;&gt;Source Code Link&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Python data prep script&lt;/li&gt;
      &lt;li&gt;R code used to produce graphics and stat analysis&lt;/li&gt;
      &lt;li&gt;Java code to run the training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;My report for download (PDF): &lt;a href=&quot;https://github.com/jeffheaton/phd/blob/master/ids-replicate-neural/jheaton-ids-replicate.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;My report on ResearchGate: &lt;a href=&quot;https://www.researchgate.net/publication/273441572_Replicating_the_Research_of_the_Paper_Application_of_Artificial_Neural_Network_in_Detection_of_Probing_Attacks&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code is under LGPL, so feel free to reuse.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research.html</guid>
        
        
        <category>phd</category>
        
        <category>compsci</category>
        
        <category>encog</category>
        
      </item>
    
      <item>
        <title>How I Got Into Data Science from IT Programming</title>
        <description>&lt;p&gt;Drew Conway describes data scientist as the combination of domain expertise, statistics 
and hacker skills.  If you are an IT programmer, you likely already have the last 
requirement.  If you are a good IT programmer, you probably already understand something 
about the business data, and have the domain expertise requirement.  In this post I 
describe how I gained knowledge of statistics/machine learning through a path of open 
source involvement and publication.&lt;/p&gt;

&lt;p&gt;There are quite a few articles that discuss how to become a data scientist.  Some of them 
are &lt;a href=&quot;http://www.quora.com/How-do-I-become-a-data-scientist&quot;&gt;even&lt;/a&gt; quite &lt;a href=&quot;http://i1.wp.com/blog.datacamp.com/wp-content/uploads/2014/08/How-to-become-a-data-scientist.jpg&quot;&gt;good&lt;/a&gt;! Most speak in very general terms.  I wrote such an summary awhile 
back that provides a &lt;a href=&quot;/datascience/2014/02/26/be-a-data-scientist.html&quot;&gt;very general description&lt;/a&gt; of what a data scientist is. In this post, 
I will describe my own path to becoming a data scientist.  I started out as a Java 
programmer in a typical IT job.&lt;/p&gt;

&lt;h2 id=&quot;publications&quot;&gt;Publications&lt;/h2&gt;

&lt;p&gt;My publications were some of my earliest credentials.  I started publishing before I had 
my bachelor’s degree.  My publications and side-programming jobswere the major factors 
that helped me obtain my first “real” programming job, working for a Fortune 500 
manufacturing company, back in 1995.  I did not have my first degree at that point.  In 
1995 I was working on a bachelor’s degree part-time.&lt;/p&gt;

&lt;p&gt;Back in the day, I wrote for publications such as C/C++ Users Journal, Java Developers 
Journal, and Windows/DOS Developer’s journal.  These were all paper-based magazines.&lt;br /&gt;
Often on the racks at book stores.  The world has really changed since then!  These days 
I publish code on sites like &lt;a href=&quot;http://www.github.com/&quot;&gt;GitHub&lt;/a&gt; and &lt;a href=&quot;http://www.codeproject.com/&quot;&gt;CodeProject&lt;/a&gt;.  A great way to gain experience is 
to find interesting projects to work on, using open source tools.  Then post your projects 
to GitHub, CodeProject and others.&lt;/p&gt;

&lt;p&gt;I’ve always enjoyed programming and have applied it to many individual projects.  Back in 
the 80’s I was writing BBS software so that I could run a board on a C64 despite 
insufficient funds from high school jobs to purchase a RAM expander.  In the 90’s I was 
hooking up web web cams and writing &lt;a href=&quot;http://en.wikipedia.org/wiki/Common_Gateway_Interface&quot;&gt;CGI&lt;/a&gt; and then later &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_Server_Pages&quot;&gt;ASP&lt;/a&gt;/&lt;a href=&quot;http://en.wikipedia.org/wiki/JavaServer_Pages&quot;&gt;JSP&lt;/a&gt; code to build websites.  I 
wrote web servers and spiders from the socket up in C++.  Around that time I wrote my 
first neural network.  Always publish! A hard drive full of cool project code sitting 
in your desk is not telling the world what you’ve done.  Support open source, a nice set 
of independent projects on GitHub looks really good.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-ai&quot;&gt;Starting with AI&lt;/h2&gt;

&lt;p&gt;Artificial intelligence is closely related to data science.  In many ways data science 
is the application of certain AI techniques to potentially large amounts of data.  AI is 
also closely linked with statistics, an integral part of data science.  I started with AI 
because it was fun.  I never envisioned using it in my “day job”.  As soon as I got my 
first neural network done I wrote an article for Java Users Journal.  I quickly discovered 
that AI had a coolness factor that could help me convince editors to publish my software.&lt;br /&gt;
I also published my first book on AI.&lt;/p&gt;

&lt;p&gt;Writing code for a book is very different than writing code for a corporate project/open 
source project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Book code&lt;/strong&gt;: Readability and understandably are paramount.  Second to none.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Corporate/Open Source Code&lt;/strong&gt;: Readability and understandably are important.  However, real-world necessity often forces scalability &amp;amp; performance to take the front seat.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if my book’s main goal is to show how to use JSP to build a simple blog, 
do I really care if the blog can scale to the traffic seen by a top-100 website?&lt;br /&gt;
Likewise, if my goal is to show how a backpropagation neural network trains, do I really 
want to muddy the water with concurrency?&lt;/p&gt;

&lt;p&gt;The neural network code in my books is meant to be example code.  A clear starting point 
for something.  But this code is not meant to be “industrial strength”.  However, when 
people start asking you questions that indicate that they are using your example code 
for “real projects”, it is now time to start (or join) an open source project!  This is 
why I started the &lt;a href=&quot;/encog/&quot;&gt;Encog project&lt;/a&gt;.  This might be a path to an open source project for you!&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-understanding-of-ai&quot;&gt;Deepening my Understanding of AI&lt;/h2&gt;

&lt;p&gt;I’ve often heard that neural networks are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gateway_drug_theory&quot;&gt;gateway drug&lt;/a&gt; to greater artificial 
intelligence.  Neural networks are an interesting creature.  They have risen and fallen 
from grace several times.  Currently they are back, and with a vengeance.   Most 
implementations of deep learning are based on neural networks.  If you would like to 
learn more about deep learning, I ran a successful Kickstarter campaign on that 
&lt;a href=&quot;https://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-3-deep-lear&quot;&gt;very topic&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I took several really good classes from &lt;a href=&quot;https://www.udacity.com/&quot;&gt;UDacity&lt;/a&gt;, just as they were introduced.  These 
classes have been somewhat re-branded.  However, UDacity still several great AI and Machine 
Learning courses.  I also recommend (and have taken) the &lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1&quot;&gt;Johns Hopkins Coursera Data Science&lt;/a&gt; 
specialization.  Its not perfect, but it will expose you to many concepts in AI.  You can 
read my &lt;a href=&quot;/phd/2015/09/26/coursera-johns-hopkins-data-science.html&quot;&gt;summary of it here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, learn statistics.  At least the basics of classical statistics.  You should 
understand concepts like mean, mode, median, linear regression, anova, manova, Tukey HSD, 
p-values, etc.  A simple undergraduate course in statistics will give you the foundation.&lt;br /&gt;
You can build on more complex topics such as Bayesian networks, belief networks, and 
others later.  Udacity has a nice intro to statistics course.&lt;/p&gt;

&lt;h2 id=&quot;kickstarter-projects&quot;&gt;Kickstarter Projects&lt;/h2&gt;

&lt;p&gt;Public projects are always a good thing.  My projects have brought me speaking 
opportunities and book opportunities (though I mostly self publish now).  Kickstarter 
has been great for this.  I launched my Artificial Intelligence for Humans series of books 
through Kickstarter.&lt;/p&gt;

&lt;h2 id=&quot;from-ai-to-data-science&quot;&gt;From AI to Data Science&lt;/h2&gt;

&lt;p&gt;When data science first started to enter the public scene I was working as a Java 
programmer writing AI books as a hobby.  A job opportunity at my current company later 
opened up in data science.  I did not even realize that the opportunity was available, 
I really was not looking.  However, during the recruiting process they discovered that 
someone with knowledge of the needed areas lived right here in town.  They had found my 
project pages.  This led to some good opportunities right in my current company.&lt;/p&gt;

&lt;p&gt;The point is, get your projects out there!  If you don’t have an idea for a project, then 
enter &lt;a href=&quot;http://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;.  You probably won’t win.  Try to become a Kaggle master.  That will be hard.&lt;br /&gt;
But you will learn quite a bit trying.  Write about your efforts.  Post code to GitHub.&lt;br /&gt;
If you use open source tools, write to their creators and send links to your efforts.&lt;br /&gt;
Open source creators love to post links to people who are actually using their code.&lt;br /&gt;
For bigger projects (with many or institutional creators), post to their communities.&lt;br /&gt;
Kaggle gives you a problem to solve.  You don’t have to win.  It will give you something 
to talk about during an interview.&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-knowledge-as-a-data-scientist&quot;&gt;Deepening my Knowledge as a Data Scientist&lt;/h2&gt;

&lt;p&gt;I try to always be learning.  You will always hear terminology that you feel like you 
should know, but do not.  This happens to me every day.  Keep a list of what you don’t 
know, and keep prioritizing and tacking the list (dare I say &lt;a href=&quot;http://guide.agilealliance.org/guide/backlog-grooming.html&quot;&gt;backlog grooming&lt;/a&gt;).&lt;br /&gt;
Keep learning!  Get involved in projects like Kaggle and read the discussion board.&lt;br /&gt;
This will show you what you do not know really quick. Write tutorials on your efforts.&lt;br /&gt;
If something was hard for you, it was hard for others who will appreciate a tutorial.&lt;/p&gt;

&lt;p&gt;I’ve seen a number of articles that question “Do you need a PhD to work as a data 
scientist?” The answer is that it will help, but is not necessary.  I know numerous 
data scientists with varying levels of academic credentials.  A PhD demonstrates that 
someone can follow the rigors of formal academic research and extend human knowledge.&lt;br /&gt;
When I became a data scientist I was not a PhD student.&lt;/p&gt;

&lt;p&gt;At this point, I am a PhD student in computer science, you can read more about that &lt;a href=&quot;/phd/compsci/2014/05/02/starting-computer-science-phd.html&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
I want to learn the process of academic research because I am starting to look at 
algorithms and techniques that would qualify as original research.  Additionally, I’ve 
given advice to a several other PhD students, who were using my projects open source 
projects in their dissertations.  It was time for me to take the leap.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Data science is described, by Drew Conway, as the intersection of hacker skills, 
statistics and domain knowledge.  To be an “IT programmer” you most likely already have 
two of these skills.  Hacker skills is ability to write programs that can wrangle data 
into many different formats and automate processes.  Domain knowledge is knowing something 
about the business that you are programming for.  Is your business data just a bunch of 
columns to you?  An effective IT programmer learns about the business and it’s data.&lt;br /&gt;
So does an effective data scientist.&lt;/p&gt;

&lt;p&gt;This leaves, only really statistics (and machine learning/AI).  You can learn that from 
books, MOOCS, and other sources.  Some were mentioned earlier in this article.  I have 
a list of some of my favorites here.  I also have a &lt;a href=&quot;/book/&quot;&gt;few books&lt;/a&gt; to teach you about AI.&lt;/p&gt;

&lt;p&gt;Most importantly, tinker and learn.  Build/publish projects, blog and contribute to open 
source.  When you talk to someone interested in hiring you as a data scientist, you will 
have experience to talk about.  Also have a &lt;a href=&quot;https://github.com/jeffheaton&quot;&gt;GitHub profile&lt;/a&gt;, linked to LinkedIn that shows 
you do in-fact have something to talk about.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>PhD Update: Second Cluster Visit for Winter 2015 Semester at NSU</title>
        <description>&lt;p&gt;I just got back to St. Louis from my second cluster visit to NSU for the phd in computer 
science.  I was feeling pretty good about choosing a distance learning program at a 
university in Ft. Lauderdale, Florida after the really cold weather we’ve been seeing in 
St. Louis lately.  There was a new blanket of snow on my drive way the morning that I 
left for the airport.  I just drove over it and headed out, the snow was melted by the 
time I returned.  The regular (non-distance learning) students were all on spring break, 
so the campus was unusually empty.  I never did take a spring break trip as an undergrad, 
so it works out that I have to go to Florida 4 times a year for my doctoral program.&lt;/p&gt;

&lt;p&gt;I am taking two classes: CISD 792: Computer Graphics, taught by Dr. Laszlo, and 
ISEC 730: Computer Security and Cryptography, taught by Dr. Cannady. The computer 
graphics class focuses on Three.JS and OpenGL, and consists of numerous programming 
assignments.  I am learning about 3D programming. I think this will be very useful for 
some visualizations that I might want to do for my books.  The security class is more 
focused on writing.  I did a decent amount of programming to replicate the research of a 
paper that applied neural networks to intrusion detection.  I will post my Java code for 
this later, as it is a decent Encog tutorial.  One of several reasons that I entered a 
PhD program was to learn academic writing, so the security class is working out well.&lt;br /&gt;
I am finding both classes every beneficial and interesting.&lt;/p&gt;

&lt;p&gt;This is my fourth trip to Ft. Lauderdale for the program.  My wife has come along with me 
each time so far.  We usually try to do at least one “tourist activity” each time.  This 
time we went to see a spring training baseball game that featured our home-team, the St. 
Louis Cardinals against the Miami Marlins, (notes to everyone, especially lawyers: both 
of those names are trademarks of the MLB, MLB is also a trademark of the MLB)  The 
St. Louis team won, so this made for a particularly enjoyable game!&lt;/p&gt;

&lt;p&gt;I also kept tabs on my Kickstarter campaign while traveling.  So far the deep learning and 
neural network is going well!  If you would like to back, and obtain my latest book, 
click here.&lt;/p&gt;

&lt;p&gt;I took this picture at the student center at NSU.  They have a really cool Dr. Who police 
box.  You can also see the palm trees out the window.  They have a beautiful campus!&lt;br /&gt;
And now they have a police box!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-09-phd-2nd-cluster-visit-1.png&quot; alt=&quot;Dr. Who Police Box at NSU&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Mar 2015 08:26:00 -0500</pubDate>
        <link>http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit.html</link>
        <guid isPermaLink="true">http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit.html</guid>
        
        
        <category>phd</category>
        
      </item>
    
  </channel>
</rss>
