<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Heaton Research</title>
 <link href="http://www.heatonresearch.com/atom.xml" rel="self"/>
 <link href="http://www.heatonresearch.com/"/>
 <updated>2015-10-09T08:02:24-05:00</updated>
 <id>http://www.heatonresearch.com</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>New Look for All My Websites: Jekyll and Github</title>
   <link href="http://www.heatonresearch.com/phd/2015/09/27/using-jekyll-and-github.html"/>
   <updated>2015-09-27T10:43:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2015/09/27/using-jekyll-and-github</id>
   <content type="html">&lt;p&gt;I’ve consolidated my websites to a single domain, everything now points to &lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt;.  Previously I had:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jeffheaton.com&quot;&gt;http://www.jeffheaton.com&lt;/a&gt; - My blog, previously running &lt;a href=&quot;https://wordpress.com/&quot;&gt;Wordpress&lt;/a&gt;. Now lives at &lt;a href=&quot;http://www.heatonresearch.com/jeff&quot;&gt;http://www.heatonresearch.com/jeff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt; - My books and the Encog project, previously running a combination of &lt;a href=&quot;http://www.drupal.org&quot;&gt;Drupal&lt;/a&gt; and &lt;a href=&quot;https://www.mediawiki.org/wiki/MediaWiki&quot;&gt;MediaWiki&lt;/a&gt;.  Still lives at &lt;a href=&quot;http://www.heatonresearch.com&quot;&gt;http://www.heatonresearch.com&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.aifh.org&quot;&gt;http://www.aifh.org&lt;/a&gt; - The examples for the current generation of my books: Artificial Intelligence for Humans.  Running on a site statically generated by &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The three sites above were hosted on two Amazon EC2 servers that cost about $165/month.&lt;br /&gt;
Additionally, keeping two UNIX hosts, Drupal and MediaWiki all patched was becoming a real 
chore. Additionally, weeding out spammer/hacker rootkits when I got behind on patches was 
a frequent nightmare. I replaced the whole mess with a static site generated by &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;.
These static files are hosted on &lt;a href=&quot;https://github.com/&quot;&gt;GitHub&lt;/a&gt;, with zero transaction costs, no 
matter how high my traffic goes (at least in theory).  However, even if traffic went over-the-top
and GitHub were to “boot me,” I could easily use Amazon S3 and just host my own static 
content.  The beauty of the generated static content is that endless headache of patches
and staying ahead of the spammer hackers is gone.&lt;/p&gt;

&lt;p&gt;I will miss some of the nice features of MediaWiki and Wordpress.  I won’t miss Drupal, 
and their witch’s brew of themes and modules, in the least.  Right now I am using a simple
theme for Jekyll, I will likely upgrade this to something &lt;a href=&quot;http://getbootstrap.com/&quot;&gt;bootstrap&lt;/a&gt; based soon.&lt;br /&gt;
Ideally, this will give me more time to do what I enjoy, writing AI software and tutorials,
and not play UNIX admin and SPAM cop all the time.&lt;/p&gt;

&lt;p&gt;I am trying to keep all dynamic features “off world”.  For now this includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Blog Comments - Handled through DISQUES.&lt;/li&gt;
  &lt;li&gt;Forum Features - My old Drupal forum was getting about 300 auto-deleted spam posts a day and around 30-40 manually deleted (by me) spam posts a day.  This become overwhelming and I shut down the forum.  I will probably try a Google Groups replacement of the forums at some point.&lt;/li&gt;
  &lt;li&gt;Support - I still do support through email and links to stack overflow questions.  Stack Overflow tends to be a good place for this, I often do not have time to answer every technical question sent to me.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am still organizing the flow and structure of the new website.  So you will probably see some changes as I continue to tweak it.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>My Experience with the Coursera Johns Hopkins Data Science Certification</title>
   <link href="http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science.html"/>
   <updated>2015-09-26T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2015/09/26/coursera-johns-hopkins-data-science</id>
   <content type="html">&lt;p&gt;This post is a summary of several posts that I had on my old blog about the &lt;a href=&quot;https://www.coursera.org/specializations/jhudatascience&quot;&gt;Johns
Hopkins Data Science certification&lt;/a&gt; offered by &lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;.
I am not sure how typical of a student I was for this program.  I currently work as a 
data scientist, have a decent background in AI, have a number of publications, and am 
currently completing a PhD in computer science.  So, a logical question is what did I 
want from this program?&lt;/p&gt;

&lt;p&gt;At the time I took this certification I had not done a great deal of R programming.  This 
program focused heavily on R.  I view this as both a strength and weakness of the program.&lt;br /&gt;
I am mostly a Java/Python/C#/C++ guy.  I found the R instruction very useful.
I’ve focused mainly in AI/machine learning, I hoped this program would fill in some gaps.
Curiosity.&lt;/p&gt;

&lt;p&gt;I really liked this program.  Courses 1-9 provide a great introduction to the predictive 
modelling side of data science.  Both machine learning and traditional regression models 
were covered.  R can be a slow and painful language, at times, but I was able to get 
through.  It is my opinion that R is primarily useful for ferrying data between models 
and visualization graphs.  It is not good for heavy-lifting and data wrangling.  The 
syntax to R is somewhat appalling.  However, it is a domain specific language (DSL), 
not a general purpose language like Python.  Don’t get me wrong.  I like R for setting up 
models and graphics.  Not for performing tasks better suited to a general purpose language.&lt;/p&gt;

&lt;p&gt;In a nutshell, here are my opinions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; With the exception of the capstone, very practical real-world data sets.  Experience with both black-box (machine learning) and more explainable (regression models) systems.  Introduction to Slidify and Shiny, I’ve already used both in my day-to-day job.  It takes some real work and understanding to make it through this program.  The last three courses rocked!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; Peer review is really hit or miss.  More on this later.  Some lecture material was sub-par (statistical inference) compared to Khan Academy.  Only reinvent the wheel if you are going to make a better wheel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;course-breakdown&quot;&gt;Course Breakdown&lt;/h2&gt;

&lt;p&gt;Here are my quick opinions on some of these courses.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The Data Scientist’s Toolbox&lt;/strong&gt;: Basically, can you install R, RStudio and use GitHub.  I’ve already done all three so I got little from this course.  If you have not dealt with R, RStudio and GitHub this class will be a nice slow intro to the program.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;R Programming&lt;/strong&gt;: I enjoyed this course!  It was hard, considering I was taking classes #1 and #3 at the same time.  If you had no programming experiance, this course will be really hard!  Be ready to supplement the instruction with lots of Google Searching.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Getting and Cleaning Data&lt;/strong&gt;: Data wrangling is an important part of data science.  Getting data into a tidy format is important.  This course used quite a bit of R programming.  For me, not being an R programmer and taking course #2 at the same time meant extra work.  If you are NOT an advanced programmer already DO NOT take #2 and #3 at the same time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploratory Data Analysis&lt;/strong&gt;: This was a valuable class, it taught you all about the R graphing packages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reproducible Research&lt;/strong&gt;:  Valuable course!  Learning about R markdown was very useful, I am already using this in one of my books to make sure that several of my examples are reproducible by providing a RMD script to produce all the charts from my book.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Inference&lt;/strong&gt;:  This was an odd class.  I already knew statistical inference and did quite well despite not watching any lectures (hardly).  I don’t believe this course made anyone happy (hardly).  Either you already knew the topic and were bored, or you were completely lost trying to learn statistics for the first time.  There are several Khan academy videos that cover all the material in this course.  Why dose Hopkins need to reproduce this?  Is this not the point of MOOC?  Why not link to the Khan academy videos and test the students.  Best of both worlds!  Also, 90% of the material was not used in the rest of the course, so I suspect many students might have been left wondering why this course is for.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regression Models&lt;/strong&gt;: Great course, this is the explainable counterpart to machine learning.  You are introduced to linear regression and GLM’s.  This course was setup as the perfect counterpart to #8. My only beef on this course was that I got screwed by peer review.  More on this later.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Practical Machine Learning&lt;/strong&gt;: Great course.  This course showed some of the most current model types in data science: Gradient Boosting Machines (GBMs) and Random Forests.  Also a great description of boosting and an awesome Kaggle like assignment where you submitted results from your model to see if you can match a “hidden data set”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developing Data Products&lt;/strong&gt;: Great course.  I really enjoyed playing with shiny, and even used it for one of the examples in my upcoming book.  You can see my shiny project here. https://jeffheaton.shinyapps.io/shiny/  and writeup here.  http://slidify.org/publish.html  They encouraged you to post these to GitHub and public sites, so I assume I am not violating anything by posting here.  Don’t plagiarize me!!!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Bad ending to an otherwise great program.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;peer-reviewed-grading&quot;&gt;Peer Reviewed Grading&lt;/h2&gt;

&lt;p&gt;If you are not familiar with peer review grading, here is how it works.  For each project 
you are given four counterparts that review and grade your assignment.  This is mostly 
double-blind, as neither the student or reviewer knows the other.  I used my regular 
GitHub account on all assignments.  So it was pretty obvious who I was.  I was even 
emailed by a grader once who recognized me from my open source projects.  Your grade is an 
average of what those four people gave you.  At $49 a course maybe this is the only way 
they can afford grade.  I currently spend nearly 100 times that for each of my 
PhD courses. :(&lt;/p&gt;

&lt;p&gt;Overall, peer review grading worked good for me in all courses but one.  Here are some of 
my concerns on peer grading.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You probably have many graders who are pressed for time and just give high-marks without  much thought.  (just a guess/opinion)&lt;/li&gt;
  &lt;li&gt;You are going to be graded by people who may not have not gotten the question correctly in the first place.&lt;/li&gt;
  &lt;li&gt;You are instructed NOT to run the R program.  So now I am being graded on someone’s ability to mentally compile and execute my program?&lt;/li&gt;
  &lt;li&gt;Each peer is going to apply different standards.  You could get radically different marks depending on who your four peers were.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So here is my story in the one case where peer review did not work for me.  I in the 
upper 98-99% range on most of these courses.  Except for course #8.  I had good scores 
going into the final project.  However, two of my peers knocked me for these reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two of my peers could not download my file from Coursera.  Yet the other two had no problem.  Fine, so I get a zero because someone’s ISP was flaking out.&lt;/li&gt;
  &lt;li&gt;Two of peers did not give me credit because the felt I had not used RMD for my report?? (which I had) Fine, so I lose a fair amount of points because two random peers did not know what RMD output looks like.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This took a toll on my grade, I still passed.  But this is the one course I did not get 
“with distinction” credit.  Yeah big deal.  In the grand scheme of things I don’t really 
care.  Just mildly annoying.  However, if you are hovering near a 70%, and you get one or 
two bad reviewers you are probably toast.&lt;/p&gt;

&lt;h2 id=&quot;capstone-project&quot;&gt;Capstone Project&lt;/h2&gt;

&lt;p&gt;The capstone project was to produce a program similar to &lt;a href=&quot;https://swiftkey.com/en&quot;&gt;Swiftkey&lt;/a&gt;, the company that was 
the partner/sponsor for the capstone.  If you are not familiar with Swiftkey, it attempts 
to speed mobile text input by predicting the next word you are going to type.  For example, 
you might type “to be or not to &lt;em&gt;__&lt;/em&gt;”.  The application should fill in “be”.  The end 
program had to be written in R and deployed to a Shiny Server.&lt;/p&gt;

&lt;p&gt;This project was somewhat flawed in several regards.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Natural Language Processing was not covered in the course.  Neither was unstructured data.  The only material provided on NLP was a handful of links to sites such as Wikipedia.&lt;/li&gt;
  &lt;li&gt;The first 9 courses had a clear direction.  However, less than half of them had anything to do with the capstone.&lt;/li&gt;
  &lt;li&gt;The project is not typical of what you would see in most businesses as a data scientist.  It would have been better to do something similar to Kaggle or one of the KDD cups.&lt;/li&gt;
  &lt;li&gt;In my opinion, R is a bad choice for this sort of project.  During the meetup with Swiftkey, they were asked what tools they used.  R was not among them. R is so cool for many things, why not showcase its abilities?&lt;/li&gt;
  &lt;li&gt;Student peer review is &lt;a href=&quot;https://www.insidehighered.com/blogs/hack-higher-education/problems-peer-grading-coursera&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://gregorulm.com/a-critical-view-on-courseras-peer-review-process/&quot;&gt;bad&lt;/a&gt;… &lt;a href=&quot;http://moocnewsandreviews.com/massive-mooc-grading-problem-stanford-hci-group-tackles-peer-assessment/&quot;&gt;bad&lt;/a&gt;… But it might be the &lt;a href=&quot;http://simplystatistics.org/2013/03/26/an-instructors-thoughts-on-peer-review-for-data-analysis-in-coursera/&quot;&gt;only choice&lt;/a&gt;.  The problem with peer review is you have three random reviewers.  They might be easy, they might be hard.  They might penalize you for the fact that they don’t know how to load your program! (this happened to me on a previous coursera course).&lt;/li&gt;
  &lt;li&gt;Perfect scores on the quizzes were really not possible.  We were given several sample sentences to predict.  The sentences were very specialized and no model would predict them correctly.  The Swiftkey surely did not.  Using my own human intuition and several text mining apps I wrote in Java, I did get 100% on the quizzes.  Even though the instructions clearly said to use your final model.  Knowing I might draw a short straw on peer review, I opted to do what I could to get max points.  I don’t care about my grade, but falling below the cutoff for a bad peer review would not be cool!&lt;/li&gt;
  &lt;li&gt;Marketing based rubric for final project.  One of the grading criteria posted the question, “Would you hire this person?”  Seriously?  I do participate in the hiring process for data scientists.  I would never hire someone without meeting them, performing a tech interview, and small coding challenge.  I hope this stat is not used in marketing material.  xx% of our graduates produced programs that might land them a job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After spending several days writing very slow model building code in R, I eventually dropped it and used Java and OpenNLP to write code that would build my model in under 20 minutes.  Others ran into the same issues.  There are somewhat kludge interfaces between R and OpenNLP, Weka and OpenNLP.  But these are native Java apps.  I  just skipped the kludge and built my model in Java and wrote a Shiny app to use the model in R.  This was enough to pass the program.  I was not alone in this approach, based on forum comments.&lt;/p&gt;

&lt;p&gt;Okay, I will just say it.  I thought this was a bad capstone.  This was just my experience 
on the first run of the certification; hopefully, they’ve improved it since.  The rest of 
the program was  really good!  If I could make a suggestion, I would say to let the 
students choose a Kaggle competition to compete.  The Kaggle competitions are closer 
to the sort of data real data scientists will see.  I am proud of the certificate that I earned.&lt;br /&gt;
If I were interviewing someone who had this certificate I would consider it a positive.&lt;br /&gt;
The candidate would still need to go through a standard interview/evaluation process.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Great program.  It won’t make you a star data scientist, but it will give you a great 
foundation to go from.  Kaggle might be a good next step.  Another might be a blog and 
doing some real, and interesting data science to showcase your skills!  This is somewhat 
how I got into data science.&lt;/p&gt;

&lt;p&gt;A question that I am often asked, is what would I think of this certification, if I saw it
on the resume of a new data scientist that I was interviewing.  In isolation, I would not
give a hire recommend based solely on this certification.  However, it would show me that
someone has mastered the basics of data science.  They know what format data needs to be
in for predictive modeling.  They know their way around the R-programming language.  They
also took the initiative to undertake something that took a decent amount of effort.&lt;br /&gt;
So yes, it is important, particularly, if your resume is lacking in the analytics area.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visiting Spain and the GECCO Confrence</title>
   <link href="http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco.html"/>
   <updated>2015-08-30T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2015/08/30/madrid-gecco</id>
   <content type="html">&lt;p&gt;I spent six weeks in Spain this summer.  My wife, Tracy, is earning a &lt;a href=&quot;http://www.slu.edu/department-of-languages-literatures-and-cultures/programs-of-study/spanish/graduate-studies&quot;&gt;Master’s Degree in 
Spanish&lt;/a&gt; from &lt;a href=&quot;http://www.slu.edu/&quot;&gt;St. Louis University (SLU)&lt;/a&gt;.  Her university has an extension &lt;a href=&quot;http://spain.slu.edu/&quot;&gt;campus in Madrid&lt;/a&gt;.&lt;br /&gt;
I also wanted to be in Spain to visit the &lt;a href=&quot;http://www.sigevo.org/gecco-2015/&quot;&gt;GECCO-2015&lt;/a&gt; conference, that was held in Madrid.&lt;br /&gt;
We were able to line up a nice apartment in &lt;a href=&quot;https://en.wikipedia.org/wiki/Salamanca_(Madrid)&quot;&gt;Salamanca, Madrid&lt;/a&gt; and I was able to take a 
few weeks vacation and work remotely the rest of the time.  It was totally worth it, Spain 
is an amazing country.  I know just enough Spanish to get around and usually be fed at a 
restaurant!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-08-30-madrid-gecco-1.png&quot; alt=&quot;Madrid and GECCO&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GECCO conference was awesome.  I was able to briefly meet &lt;a href=&quot;http://www.cs.ucf.edu/~kstanley/&quot;&gt;Dr. Kenneth Stanley&lt;/a&gt; after 
hearing a tutorial session held by him.  I’ve many of Dr. Stanley’s papers and implemented 
his NEAT/HyperNEAT algorithms.  He also has a &lt;a href=&quot;http://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&quot;&gt;new book&lt;/a&gt; out that I’ve just read and recommend.&lt;br /&gt;
The book points out that it is very difficult to design objective functions that might 
truly create amazing results.  Some of the best results on &lt;a href=&quot;http://picbreeder.org/&quot;&gt;Ken’s PicBreader&lt;/a&gt; site were not 
specifically planned.&lt;/p&gt;

&lt;p&gt;I also saw a fascinating Lisp-based language named &lt;a href=&quot;http://faculty.hampshire.edu/lspector/push3-description.html&quot;&gt;Push3&lt;/a&gt;, created by &lt;a href=&quot;http://faculty.hampshire.edu/lspector/&quot;&gt;Dr. Lee Spector&lt;/a&gt;.&lt;br /&gt;
Genetic programming ha always been fascinating to me because it uses AI to actually 
evolve AI.  Push3 is a programming language designed for computers to evolve.&lt;br /&gt;
Dr. Spector’s group is evolving computer programs, to accomplish basic tasks like word 
count and other basic utilities.  Most machine learning algorithms are simply about 
optimizing coefficients to decrease a supervised training set’s error.  It is amazing to 
see algorithms that can actually evolve themselves into new algorithms.&lt;/p&gt;

&lt;p&gt;I plan to make use of genetic programming for part of my dissertation.  Of course, I am 
still finishing up coursework and this is very much in flux.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>My First Kaggle Competition</title>
   <link href="http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html"/>
   <updated>2015-05-25T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2015/05/25/first-kaggle</id>
   <content type="html">&lt;p&gt;I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it, 
&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; is an ongoing forum for competitive data science. Individuals and teams compete to 
create the best model for data sets provided by industry and sometimes academia.&lt;br /&gt;
Individuals who enter are ranked as either Novice, Kaggler and &lt;a href=&quot;https://www.kaggle.com/wiki/UserRankingAndTierSystem&quot;&gt;Kaggle Master&lt;/a&gt;.  To become 
a Kaggle master, one must place in the top 10% of two competitions; and in one of the top 
10 slots of a third competition.&lt;/p&gt;

&lt;p&gt;I’ve talked about Kaggle in many of my presentations.  I’ve also used Kaggle data in 
my books. Until now, I had yet to actually enter a Kaggle competition.  I decided it was 
finally time to try this for myself. I competed in the &lt;a href=&quot;http://otto%20group%20product%20classification%20challenge/&quot;&gt;Otto Group Product Classification&lt;/a&gt; 
Challenge that ended on May 18th, 2015.  My score was sufficient to land in the top 10%, 
so I’ve completed one of the requirements for Kaggle master.  My Kaggle profile can be 
seen here.&lt;/p&gt;

&lt;p&gt;My goals for entering were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;See how hard Kaggle actually is, and move towards a Kaggle master designation.&lt;/li&gt;
  &lt;li&gt;Learn from the other Kagglers and forums.&lt;/li&gt;
  &lt;li&gt;Build a basic toolkit that I will use for future Kaggle competitions.&lt;/li&gt;
  &lt;li&gt;Gain an example (from my entry) for the &lt;a href=&quot;http://www.heatonresearch.com/aifh&quot;&gt;Artificial Intelligence for Humans series&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Maybe get an idea or two for my future dissertation (I am a phd student at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-otto-classification-challenge&quot;&gt;The Otto Classification Challenge&lt;/h2&gt;

&lt;p&gt;First, I will give a brief introduction to the exact nature of the Otto Classification 
Challenge.  For a complete description, refer to the Kaggle description(&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge&quot;&gt;found here&lt;/a&gt;).&lt;br /&gt;
This challenge was introduced by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Otto_GmbH&quot;&gt;Otto Group&lt;/a&gt;, who is the world’s largest mail order 
company and currently one of the biggest &lt;a href=&quot;http://en.wikipedia.org/wiki/E-commerce&quot;&gt;e-commerce&lt;/a&gt; companies, mainly based in Germany 
and France but operating in more than 20 countries.  They have many products sold over 
numerous countries.  They would like to be able to classify these products into 9 
categories, using 93 features (columns).  These 93 columns represent counts, and are 
often zero.&lt;/p&gt;

&lt;p&gt;The data are completely redacted.  You do not know what the 9 categories are, nor do you 
know the meaning behind the 93 features.  You only know that the features are integer 
counts. Most Kaggle competitions provide you with a test and training dataset.  For the 
training dataset you are given the outcomes, or correct answers.  For the test set, you 
are only given the 93 features, and you must provide the outcome.  The test and training 
sets are divided as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Test Data: 144K rows&lt;/li&gt;
  &lt;li&gt;Training Data: 61K rows&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You do not actually submit your model to Kaggle.  Rather, you submit your predictions 
based on the test data.  This allows you to use any platform to make these predictions.&lt;br /&gt;
The actual format of a submission for this competition is the probability of each of 
the 9 categories being the outcome.  This is not like a university multiple choice test 
where you must submit your answer as A, B, C, or D.  Rather, you would submit your 
answer as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A: 80% probability&lt;/li&gt;
  &lt;li&gt;B: 16% probability&lt;/li&gt;
  &lt;li&gt;C: 2% probability&lt;/li&gt;
  &lt;li&gt;D: 2% probability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wish college exams were graded like this!  Often I am very confident about two of the 
answers, and can eliminate the other two.  Simply assign a probability to each, and you 
get a partial score.  If A were the correct answer for the above, I would get 80% of the 
points.&lt;/p&gt;

&lt;p&gt;The actual Kaggle score is slightly more complex than that.  Rather, you are graded on a 
logarithm based scale and are very heavily penalized for having a lower probability on 
the correct answer. The following are a few lines from my submission:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1,0.0003,0.2132,0.2340,0.5468,6.2998e-05,0.0001,0.0050,0.0001,4.3826e-05
2,0.0011,0.0029,0.0010,0.0003,0.0001,0.5207,0.0013,0.4711,0.0011
3,3.2977e-06,4.1419e-06,7.4524e-06,2.6550e-06,5.0014e-07,0.9998,5.2621e-06,0.0001,6.6447e-06
4,0.0001,0.6786,0.3162,0.0039,3.3378e-05,4.1196e-05,0.0001,0.0001,0.0006
5,0.1403,0.0002,0.0002,6.734e-05,0.0001,0.0027,0.0009,0.0297,0.8255&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each line starts with a number that specifies the data item that is being answered.&lt;br /&gt;
The sample above shows the answers for items 1-5.  The next 9 values are the probabilities 
for each of the product classes.  These probabilities must add up to 1.0 (100%).&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned-from-kaggle&quot;&gt;What I Learned from Kaggle&lt;/h2&gt;

&lt;p&gt;If you want to do well in Kaggle, the following are very important topics, along with 
the tools I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep Learning - Using &lt;a href=&quot;http://h2o.ai/&quot;&gt;H2O&lt;/a&gt; and &lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;Lasagne&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gradient Boosting Machines (GBM) - &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;Using XGBOOST&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ensemble Learning - &lt;a href=&quot;http://www.numpy.org/&quot;&gt;Using NumPy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Feature Engineering - Using NumPy and &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two areas that I learned the most about, during this challenge, were GBM parameter 
tuning and ensemble learning.  I got pretty good at tuning a GBM.  The individual scores 
for my GBM’s were in line with those used by the top teams.&lt;/p&gt;

&lt;p&gt;Before Kaggle I typically used only one model, if I were using neural networks, I just 
used neural networks.  If I were using an SVM, Random Forest or Gradient Boosting, I stuck 
to just that model.  With Kaggle, it is critical to use multiple models, ensembled to 
produce better results than each of the models could produce independently.&lt;/p&gt;

&lt;p&gt;Some of my main takeaways from the competition:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GPU is really important for deep learning.  It is best to use a deep learning package that supports it, such as H2O, Theano or Lasagne.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://lvdmaaten.github.io/tsne/&quot;&gt;t-sne&lt;/a&gt; visualization is awesome for high-dimension visualization and creating features.&lt;/li&gt;
  &lt;li&gt;I need to learn to ensemble better!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This competition was the first time I used &lt;a href=&quot;http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&quot;&gt;T-SNE&lt;/a&gt;.  It works like PCA in that it is capable 
of reducing dimensions, however, the data points separate in such a way that the 
visualization is often clearer than &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt;. This is done using a stochastic nearest 
neighbor process. I plan to learn more about how t-sne actually performs the reduction, 
compared to PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-1.jpeg&quot; alt=&quot;t-SNE Plot of the Otto Group Challenge&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-approach-to-the-otto-challenge&quot;&gt;My Approach to the Otto Challenge&lt;/h2&gt;

&lt;p&gt;So far I’ve only worked with single model systems.  I’ve used models that contain ensembles 
that are “built in”, such as random forests and gradient boosting machines.  However, it 
is possible to create higher-level ensembles of these models.  I used a total of 20 models, 
this included 10 deep neural networks and 10 gradient boosting machines.  My deep neural 
network system provided one prediction and my gradient boosting machines provided the other.&lt;br /&gt;
These two predictions were blended together, using a simple ratio.  The resulting prediction 
vector was then normalized so that the sum equaled 1.0(100%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-05-25-first-kaggle-2.png&quot; alt=&quot;Jeff Heaton&#39;s Kaggle Model for the Otto Group&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I did not remove or engineer any fields.  For both model types I converted all 93 
attributes into Z-Scores.  For the neural network I normalized all values to be in a 
specific range.&lt;/p&gt;

&lt;p&gt;My 10 deep learning neural networks used a simple bagging method.  I averaged the 
predictions from 20 different neural networks.  Each of these neural networks was created 
by choosing a different 80/20 split between training and validation.  The neural network 
was trained on the training data until the validation score did not improve for 25 epochs.&lt;br /&gt;
Once training stopped I used the weights from the epoch that produced the highest training 
score. This process is a simple form of bagging called &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot;&gt;bootstrap aggregation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My 10 gradient boosting machines (GBM) were each components of a 10-fold cross-validation.&lt;br /&gt;
I essentially broke the Kaggle training data into 10 folds and used each of these folds 
as a validation set, and the others as training.  This produced 10 gradient boosting machines.&lt;br /&gt;
I then used an NxM coefficient matrix to blend each of these together.  Where N is the 
number of models, M is the number of features.  In this case it was a 10x9 grid.  This 
matrix weighted each of the 10 model’s predictive power in each of the 9 categories.&lt;br /&gt;
These coefficients were a straight probability calculation from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt; of 
each of the 10 models.  This allowed each model to potentially specialize in each of the 
9 categories.&lt;/p&gt;

&lt;p&gt;I spent considerable time tuning my GBM.  I used Nelder-Mead searches to optimize my 
hyper-parameter vector.  I ultimately settled on the following parameters:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;min_child_weight&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;subsample&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;colsample_bytree&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;eta&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mo&quot;&gt;005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;threads&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approaches&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GBM&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;produced&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;separate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;submission&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blended&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;these&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;together&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weighting&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.65&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gave&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;me&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-worked-well-for-top-teams&quot;&gt;What Worked Well for Top Teams&lt;/h2&gt;

&lt;p&gt;The top Kaggle teams made use of more sophisticated ensemble techniques than I did.&lt;br /&gt;
This will be my primary learning area for the next competition.  You can read about 
some of the top models here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov&quot;&gt;The Top Scoring Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score&quot;&gt;Relatively Simple Model for a Top 25 Score&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models&quot;&gt;Share Your Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent&quot;&gt;One of the Top Ten&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging&quot;&gt;TSNE &amp;amp; Meta-Bagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above write-ups are very useful, I’ve already started examining their approaches.&lt;/p&gt;

&lt;p&gt;Some of the top technologies discussed were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Engineering&lt;/li&gt;
  &lt;li&gt;Input Transformation - good write up &lt;a href=&quot;http://fmwww.bc.edu/repec/bocode/t/transint.html&quot;&gt;here&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;log transforms&lt;/li&gt;
      &lt;li&gt;sqrt(x + 3/8) - Not sure what this one is called, but I saw it used a few times&lt;/li&gt;
      &lt;li&gt;z-score transforms&lt;/li&gt;
      &lt;li&gt;ranged transformation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hyperparameter Optimization
    &lt;ul&gt;
      &lt;li&gt;Nelder-Mead&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/JasperSnoek/spearmint&quot;&gt;Spearmint&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will probably not enter another Kaggle until the fall of this year.  This blog post 
will be updated to contain my notes as I investigate other techniques for this 
competition.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Quick R Tutorial: The Big-O Chart</title>
   <link href="http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o.html"/>
   <updated>2015-03-21T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/compsci/r/2015/03/21/r_big_o</id>
   <content type="html">&lt;p&gt;I needed an original &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;Big-O&lt;/a&gt; chart for a publication.  This tutorial does not cover what 
Big-O actually is, just how to chart it.  If you want more information on Big-O I 
recommend reading &lt;a href=&quot;http://bigocheatsheet.com/&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;this&lt;/a&gt;.  I know, go figure, for a computer science student. I 
really like using R for all things chart and visualization.  So, I turned to R for this 
task.  While this is a very simple chart, it does demonstrate several very common 
tasks in R:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overlaying several plots&lt;/li&gt;
  &lt;li&gt;Adding a legend to a plot&lt;/li&gt;
  &lt;li&gt;Using “math symbols” inside of the text on a chart&lt;/li&gt;
  &lt;li&gt;Stripping off all the extra whitespace that R likes to generate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the end-result here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-21-r_big_o-1.png&quot; alt=&quot;Computer Science Big-O Chart in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This was produced with the following R code:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Remove white space on chart&lt;/span&gt;
par&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;mar&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;+0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The x &amp;amp; y limits for the plot&lt;/span&gt;
xl &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
yl &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Just use R&amp;#39;s standard list of colors for the lines (I am too tired to be creative this morning)&lt;/span&gt;
pal &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; palette&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot each of the equations that we are interested in. Note the add=TRUE&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# it causes them to overlay each other.&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;xlab&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Elements(N)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylab&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Operations&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yl&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;add&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generate the legend, note the use of expression to handle n to the power of 2.&lt;/span&gt;
legend&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;topright&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(1)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(log(n))&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(n)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(n log(n))&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;kp&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;O&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;n&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;O&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;O(!n)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;lty&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; col&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pal&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; bty&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;n&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; cex&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Using Encog to Replicate Research</title>
   <link href="http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research.html"/>
   <updated>2015-03-12T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/compsci/encog/2015/03/12/encog-replicate-research</id>
   <content type="html">&lt;p&gt;For the one of my PhD courses at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;Nova Southeastern University (NSU)&lt;/a&gt; it was necessary to 
reproduce the research of the following paper:&lt;/p&gt;

&lt;p&gt;I. Ahmad, A. Abdullah, and A. Alghamdi, “Application of artificial neural network in 
detection of probing attacks,” in IEEE Symposium on Industrial Electronics Applications, 
2009. ISIEA 2009., vol. 2, Oct 2009, pp. 557–562.&lt;/p&gt;

&lt;p&gt;This paper demonstrated how to use a neural network to build a &lt;a href=&quot;http://en.wikipedia.org/wiki/Intrusion_detection_system&quot;&gt;basic intrusion detection 
system (IDS)&lt;/a&gt; for the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;KDD99 dataset&lt;/a&gt;.  It is important to reproduce research, in an academic 
setting.  This means that you were able to obtain the same results as the original 
researchers, using the same techniques.  I do this often when I write books or implement 
parts of Encog. This allows me convince myself that I have implemented an algorithm 
correctly, and as the researchers intended.  I don’t always agree with what the original 
researcher did.  If I change it, when I implement Encog, I am now in the area of “original 
research,” and my changes must be labeled as such.&lt;/p&gt;

&lt;p&gt;Some researchers are more helpful than others for replication of research.  Additionally, 
neural networks are stochastic (they use random numbers).  Basing recommendations off of 
a small number of runs is usually a bad idea, when dealing with a stochastic system.&lt;br /&gt;
Their small number of runs caused the above researchers to conclude that two hidden layers 
was optimal for their dataset.  Unless you are dealing with deep learning, this is almost 
always not the case.  The universal approximation theorem rules out more than a single 
layer for the old-school sort of perceptron neural network used in this paper.&lt;br /&gt;
Additionally, the vanishing gradient problem prevents the RPROP training that the 
researchers from fitting well with larger numbers of hidden layers.  The researchers 
tried up to 4 hidden layers.&lt;/p&gt;

&lt;p&gt;For my own research replication I used the same dataset, with many training runs to make 
sure that their results were within my high-low range.  To prove that a single layer does 
better I used &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;ANOVA&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot;&gt;Tukey’s HSD&lt;/a&gt; to show that differences among the different neural 
network architectures were indeed statistically significant and my box and whiskers plot 
shows that training runs with a single layer more consistently converged to a better 
mean &lt;a href=&quot;http://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;&gt;RMSE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am attaching both my paper and code in case it is useful.  This is a decent tutorial 
on using the latest Encog code to normalize and fit to a data set.&lt;/p&gt;

&lt;p&gt;The class also required us to write up the results in &lt;a href=&quot;http://www.ieee.org/conferences_events/conferences/publishing/templates.html&quot;&gt;IEEE conference format&lt;/a&gt;.  I am a 
fan of &lt;a href=&quot;http://www.latex-project.org/&quot;&gt;LaTex&lt;/a&gt;, so that is what I used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source code Includes: &lt;a href=&quot;https://github.com/jeffheaton/phd/tree/master/ids-replicate-neural&quot;&gt;Source Code Link&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Python data prep script&lt;/li&gt;
      &lt;li&gt;R code used to produce graphics and stat analysis&lt;/li&gt;
      &lt;li&gt;Java code to run the training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;My report for download (PDF): &lt;a href=&quot;https://github.com/jeffheaton/phd/blob/master/ids-replicate-neural/jheaton-ids-replicate.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;My report on ResearchGate: &lt;a href=&quot;https://www.researchgate.net/publication/273441572_Replicating_the_Research_of_the_Paper_Application_of_Artificial_Neural_Network_in_Detection_of_Probing_Attacks&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code is under LGPL, so feel free to reuse.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How I Got Into Data Science from IT Programming</title>
   <link href="http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it.html"/>
   <updated>2015-03-11T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/datascience/2015/03/11/data-science-from-it</id>
   <content type="html">&lt;p&gt;Drew Conway describes data scientist as the combination of domain expertise, statistics 
and hacker skills.  If you are an IT programmer, you likely already have the last 
requirement.  If you are a good IT programmer, you probably already understand something 
about the business data, and have the domain expertise requirement.  In this post I 
describe how I gained knowledge of statistics/machine learning through a path of open 
source involvement and publication.&lt;/p&gt;

&lt;p&gt;There are quite a few articles that discuss how to become a data scientist.  Some of them 
are &lt;a href=&quot;http://www.quora.com/How-do-I-become-a-data-scientist&quot;&gt;even&lt;/a&gt; quite &lt;a href=&quot;http://i1.wp.com/blog.datacamp.com/wp-content/uploads/2014/08/How-to-become-a-data-scientist.jpg&quot;&gt;good&lt;/a&gt;! Most speak in very general terms.  I wrote such an summary awhile 
back that provides a &lt;a href=&quot;/datascience/2014/02/26/be-a-data-scientist.html&quot;&gt;very general description&lt;/a&gt; of what a data scientist is. In this post, 
I will describe my own path to becoming a data scientist.  I started out as a Java 
programmer in a typical IT job.&lt;/p&gt;

&lt;h2 id=&quot;publications&quot;&gt;Publications&lt;/h2&gt;

&lt;p&gt;My publications were some of my earliest credentials.  I started publishing before I had 
my bachelor’s degree.  My publications and side-programming jobswere the major factors 
that helped me obtain my first “real” programming job, working for a Fortune 500 
manufacturing company, back in 1995.  I did not have my first degree at that point.  In 
1995 I was working on a bachelor’s degree part-time.&lt;/p&gt;

&lt;p&gt;Back in the day, I wrote for publications such as C/C++ Users Journal, Java Developers 
Journal, and Windows/DOS Developer’s journal.  These were all paper-based magazines.&lt;br /&gt;
Often on the racks at book stores.  The world has really changed since then!  These days 
I publish code on sites like &lt;a href=&quot;http://www.github.com/&quot;&gt;GitHub&lt;/a&gt; and &lt;a href=&quot;http://www.codeproject.com/&quot;&gt;CodeProject&lt;/a&gt;.  A great way to gain experience is 
to find interesting projects to work on, using open source tools.  Then post your projects 
to GitHub, CodeProject and others.&lt;/p&gt;

&lt;p&gt;I’ve always enjoyed programming and have applied it to many individual projects.  Back in 
the 80’s I was writing BBS software so that I could run a board on a C64 despite 
insufficient funds from high school jobs to purchase a RAM expander.  In the 90’s I was 
hooking up web web cams and writing &lt;a href=&quot;http://en.wikipedia.org/wiki/Common_Gateway_Interface&quot;&gt;CGI&lt;/a&gt; and then later &lt;a href=&quot;http://en.wikipedia.org/wiki/Active_Server_Pages&quot;&gt;ASP&lt;/a&gt;/&lt;a href=&quot;http://en.wikipedia.org/wiki/JavaServer_Pages&quot;&gt;JSP&lt;/a&gt; code to build websites.  I 
wrote web servers and spiders from the socket up in C++.  Around that time I wrote my 
first neural network.  Always publish! A hard drive full of cool project code sitting 
in your desk is not telling the world what you’ve done.  Support open source, a nice set 
of independent projects on GitHub looks really good.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-ai&quot;&gt;Starting with AI&lt;/h2&gt;

&lt;p&gt;Artificial intelligence is closely related to data science.  In many ways data science 
is the application of certain AI techniques to potentially large amounts of data.  AI is 
also closely linked with statistics, an integral part of data science.  I started with AI 
because it was fun.  I never envisioned using it in my “day job”.  As soon as I got my 
first neural network done I wrote an article for Java Users Journal.  I quickly discovered 
that AI had a coolness factor that could help me convince editors to publish my software.&lt;br /&gt;
I also published my first book on AI.&lt;/p&gt;

&lt;p&gt;Writing code for a book is very different than writing code for a corporate project/open 
source project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Book code&lt;/strong&gt;: Readability and understandably are paramount.  Second to none.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Corporate/Open Source Code&lt;/strong&gt;: Readability and understandably are important.  However, real-world necessity often forces scalability &amp;amp; performance to take the front seat.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if my book’s main goal is to show how to use JSP to build a simple blog, 
do I really care if the blog can scale to the traffic seen by a top-100 website?&lt;br /&gt;
Likewise, if my goal is to show how a backpropagation neural network trains, do I really 
want to muddy the water with concurrency?&lt;/p&gt;

&lt;p&gt;The neural network code in my books is meant to be example code.  A clear starting point 
for something.  But this code is not meant to be “industrial strength”.  However, when 
people start asking you questions that indicate that they are using your example code 
for “real projects”, it is now time to start (or join) an open source project!  This is 
why I started the &lt;a href=&quot;/encog/&quot;&gt;Encog project&lt;/a&gt;.  This might be a path to an open source project for you!&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-understanding-of-ai&quot;&gt;Deepening my Understanding of AI&lt;/h2&gt;

&lt;p&gt;I’ve often heard that neural networks are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gateway_drug_theory&quot;&gt;gateway drug&lt;/a&gt; to greater artificial 
intelligence.  Neural networks are an interesting creature.  They have risen and fallen 
from grace several times.  Currently they are back, and with a vengeance.   Most 
implementations of deep learning are based on neural networks.  If you would like to 
learn more about deep learning, I ran a successful Kickstarter campaign on that 
&lt;a href=&quot;https://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-3-deep-lear&quot;&gt;very topic&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I took several really good classes from &lt;a href=&quot;https://www.udacity.com/&quot;&gt;UDacity&lt;/a&gt;, just as they were introduced.  These 
classes have been somewhat re-branded.  However, UDacity still several great AI and Machine 
Learning courses.  I also recommend (and have taken) the &lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1&quot;&gt;Johns Hopkins Coursera Data Science&lt;/a&gt; 
specialization.  Its not perfect, but it will expose you to many concepts in AI.  You can 
read my &lt;a href=&quot;/phd/2015/09/26/coursera-johns-hopkins-data-science.html&quot;&gt;summary of it here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, learn statistics.  At least the basics of classical statistics.  You should 
understand concepts like mean, mode, median, linear regression, anova, manova, Tukey HSD, 
p-values, etc.  A simple undergraduate course in statistics will give you the foundation.&lt;br /&gt;
You can build on more complex topics such as Bayesian networks, belief networks, and 
others later.  Udacity has a nice intro to statistics course.&lt;/p&gt;

&lt;h2 id=&quot;kickstarter-projects&quot;&gt;Kickstarter Projects&lt;/h2&gt;

&lt;p&gt;Public projects are always a good thing.  My projects have brought me speaking 
opportunities and book opportunities (though I mostly self publish now).  Kickstarter 
has been great for this.  I launched my Artificial Intelligence for Humans series of books 
through Kickstarter.&lt;/p&gt;

&lt;h2 id=&quot;from-ai-to-data-science&quot;&gt;From AI to Data Science&lt;/h2&gt;

&lt;p&gt;When data science first started to enter the public scene I was working as a Java 
programmer writing AI books as a hobby.  A job opportunity at my current company later 
opened up in data science.  I did not even realize that the opportunity was available, 
I really was not looking.  However, during the recruiting process they discovered that 
someone with knowledge of the needed areas lived right here in town.  They had found my 
project pages.  This led to some good opportunities right in my current company.&lt;/p&gt;

&lt;p&gt;The point is, get your projects out there!  If you don’t have an idea for a project, then 
enter &lt;a href=&quot;http://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;.  You probably won’t win.  Try to become a Kaggle master.  That will be hard.&lt;br /&gt;
But you will learn quite a bit trying.  Write about your efforts.  Post code to GitHub.&lt;br /&gt;
If you use open source tools, write to their creators and send links to your efforts.&lt;br /&gt;
Open source creators love to post links to people who are actually using their code.&lt;br /&gt;
For bigger projects (with many or institutional creators), post to their communities.&lt;br /&gt;
Kaggle gives you a problem to solve.  You don’t have to win.  It will give you something 
to talk about during an interview.&lt;/p&gt;

&lt;h2 id=&quot;deepening-my-knowledge-as-a-data-scientist&quot;&gt;Deepening my Knowledge as a Data Scientist&lt;/h2&gt;

&lt;p&gt;I try to always be learning.  You will always hear terminology that you feel like you 
should know, but do not.  This happens to me every day.  Keep a list of what you don’t 
know, and keep prioritizing and tacking the list (dare I say &lt;a href=&quot;http://guide.agilealliance.org/guide/backlog-grooming.html&quot;&gt;backlog grooming&lt;/a&gt;).&lt;br /&gt;
Keep learning!  Get involved in projects like Kaggle and read the discussion board.&lt;br /&gt;
This will show you what you do not know really quick. Write tutorials on your efforts.&lt;br /&gt;
If something was hard for you, it was hard for others who will appreciate a tutorial.&lt;/p&gt;

&lt;p&gt;I’ve seen a number of articles that question “Do you need a PhD to work as a data 
scientist?” The answer is that it will help, but is not necessary.  I know numerous 
data scientists with varying levels of academic credentials.  A PhD demonstrates that 
someone can follow the rigors of formal academic research and extend human knowledge.&lt;br /&gt;
When I became a data scientist I was not a PhD student.&lt;/p&gt;

&lt;p&gt;At this point, I am a PhD student in computer science, you can read more about that &lt;a href=&quot;/phd/compsci/2014/05/02/starting-computer-science-phd.html&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
I want to learn the process of academic research because I am starting to look at 
algorithms and techniques that would qualify as original research.  Additionally, I’ve 
given advice to a several other PhD students, who were using my projects open source 
projects in their dissertations.  It was time for me to take the leap.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Data science is described, by Drew Conway, as the intersection of hacker skills, 
statistics and domain knowledge.  To be an “IT programmer” you most likely already have 
two of these skills.  Hacker skills is ability to write programs that can wrangle data 
into many different formats and automate processes.  Domain knowledge is knowing something 
about the business that you are programming for.  Is your business data just a bunch of 
columns to you?  An effective IT programmer learns about the business and it’s data.&lt;br /&gt;
So does an effective data scientist.&lt;/p&gt;

&lt;p&gt;This leaves, only really statistics (and machine learning/AI).  You can learn that from 
books, MOOCS, and other sources.  Some were mentioned earlier in this article.  I have 
a list of some of my favorites here.  I also have a &lt;a href=&quot;/book/&quot;&gt;few books&lt;/a&gt; to teach you about AI.&lt;/p&gt;

&lt;p&gt;Most importantly, tinker and learn.  Build/publish projects, blog and contribute to open 
source.  When you talk to someone interested in hiring you as a data scientist, you will 
have experience to talk about.  Also have a &lt;a href=&quot;https://github.com/jeffheaton&quot;&gt;GitHub profile&lt;/a&gt;, linked to LinkedIn that shows 
you do in-fact have something to talk about.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>PhD Update: Second Cluster Visit for Winter 2015 Semester at NSU</title>
   <link href="http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit.html"/>
   <updated>2015-03-09T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2015/03/09/phd-2nd-cluster-visit</id>
   <content type="html">&lt;p&gt;I just got back to St. Louis from my second cluster visit to NSU for the phd in computer 
science.  I was feeling pretty good about choosing a distance learning program at a 
university in Ft. Lauderdale, Florida after the really cold weather we’ve been seeing in 
St. Louis lately.  There was a new blanket of snow on my drive way the morning that I 
left for the airport.  I just drove over it and headed out, the snow was melted by the 
time I returned.  The regular (non-distance learning) students were all on spring break, 
so the campus was unusually empty.  I never did take a spring break trip as an undergrad, 
so it works out that I have to go to Florida 4 times a year for my doctoral program.&lt;/p&gt;

&lt;p&gt;I am taking two classes: CISD 792: Computer Graphics, taught by Dr. Laszlo, and 
ISEC 730: Computer Security and Cryptography, taught by Dr. Cannady. The computer 
graphics class focuses on Three.JS and OpenGL, and consists of numerous programming 
assignments.  I am learning about 3D programming. I think this will be very useful for 
some visualizations that I might want to do for my books.  The security class is more 
focused on writing.  I did a decent amount of programming to replicate the research of a 
paper that applied neural networks to intrusion detection.  I will post my Java code for 
this later, as it is a decent Encog tutorial.  One of several reasons that I entered a 
PhD program was to learn academic writing, so the security class is working out well.&lt;br /&gt;
I am finding both classes every beneficial and interesting.&lt;/p&gt;

&lt;p&gt;This is my fourth trip to Ft. Lauderdale for the program.  My wife has come along with me 
each time so far.  We usually try to do at least one “tourist activity” each time.  This 
time we went to see a spring training baseball game that featured our home-team, the St. 
Louis Cardinals against the Miami Marlins, (notes to everyone, especially lawyers: both 
of those names are trademarks of the MLB, MLB is also a trademark of the MLB)  The 
St. Louis team won, so this made for a particularly enjoyable game!&lt;/p&gt;

&lt;p&gt;I also kept tabs on my Kickstarter campaign while traveling.  So far the deep learning and 
neural network is going well!  If you would like to back, and obtain my latest book, 
click here.&lt;/p&gt;

&lt;p&gt;I took this picture at the student center at NSU.  They have a really cool Dr. Who police 
box.  You can also see the palm trees out the window.  They have a beautiful campus!&lt;br /&gt;
And now they have a police box!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2015-03-09-phd-2nd-cluster-visit-1.png&quot; alt=&quot;Dr. Who Police Box at NSU&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Quick and Very Dirty Data Wrangling Example</title>
   <link href="http://www.heatonresearch.com/datascience/2014/12/27/quick-dirty-data-wrangling.html"/>
   <updated>2014-12-27T07:26:00-06:00</updated>
   <id>http://www.heatonresearch.com/datascience/2014/12/27/quick-dirty-data-wrangling</id>
   <content type="html">&lt;p&gt;Data science is often described as the intersection of statistics, domain knowledge and 
hacking skills.  One important part of hacking skills is data wrangling.  Data are rarely 
in the exact form that you need them.  I am currently working on an example for 
AIFH Vol 3 that will use a SOM and compare nations based on several statistics.  I could 
not find a dataset that fit exactly what I was looking for.  So I decided to create my 
own dataset.&lt;/p&gt;

&lt;p&gt;I wanted a list of countries with three different data points that somehow indicate that 
nation’s prosperity.  I chose GDP, lifespan and literacy rate.  Remember, this is a 
computer science experiment, not a sociology experiment.  I am sure others could come up 
with a much better set of data points to compare countries.  However, for my example 
program these will work just fine.&lt;/p&gt;

&lt;p&gt;I could not find a data set that was already completed.  However, all of this data is 
contained in Wikipedia.  To wrangle the data I created a simple Python script to 
accomplish this.  I am really starting to like Python for quick scripting projects.&lt;br /&gt;
I could have also used R, Groovy, Perl or a host of others.  The end result looks 
something like this:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;code,country,gdp,lifespan,literacy
AFG,Afghanistan,20650,60,0.431
ALB,Albania,12800,74,0.98
DZA,Algeria,215700,73.12,0.918
AND,Andorra,4800,84.2,1.0
AGO,Angola,124000,52,0.826
ATG,Antigua and Barbuda,1220,75.8,0.984
[Full File]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can download the entire contents of Wikipedia into a data file.  This is usually how 
you should deal with Wikipedia data.  Do not use HTTP to pull large volumes of data from 
Wikipedia.  This is a good way to get blocked from Wikipedia.  Also, the datafile for 
Wikipedia is not HTML encoded and much easier to parse.  I simply pulled the &lt;a href=&quot;http://en.wikipedia.org/wiki/ISO_3166-1&quot;&gt;nation codes&lt;/a&gt;
page, &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29&quot;&gt;GDP&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_literacy_rate&quot;&gt;literacy&lt;/a&gt;, and &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy&quot;&gt;lifespan&lt;/a&gt; pages into text files that my Python script could parse.&lt;/p&gt;

&lt;p&gt;I linked the files together (joined) using the nation name as a key.  If a nation’s name 
did not appear in all lists I discarded that nation.&lt;/p&gt;

&lt;p&gt;You can see my Python code &lt;a href=&quot;https://github.com/jeffheaton/aifh/blob/master/vol3/misc/nations/build_nations.py&quot;&gt;here&lt;/a&gt;.  This code could be more readable.  But it gets the job 
done.  It is a quick data wrangling hack.  If I needed to re-pull the data on a frequent 
basis, particularly if it were high-velocity data, I would do something more formal.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>First time at the NSU Campus - Fall 2014</title>
   <link href="http://www.heatonresearch.com/phd/2014/09/02/first-visit-nsu.html"/>
   <updated>2014-09-02T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/2014/09/02/first-visit-nsu</id>
   <content type="html">&lt;p&gt;I just returned from my first cluster meeting for PhD program in computer science at Nova 
Southeastern University.  Because it was Labor Day weekend my wife went with me, and we 
made it a mini Florida vacation.  We flew into Ft. Lauderdale on Wed, Sept 27, 2014.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wednesday:&lt;/strong&gt; Travel day, and check into hotel.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thursday:&lt;/strong&gt; program orientation, library tour and kickoff reception.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Friday:&lt;/strong&gt; Two class sessions (4-hours each), with a 1.5 hr break for lunch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Saturday:&lt;/strong&gt; Two class sessions (4-hours each), with a 1.5 hr break for lunch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sunday:&lt;/strong&gt; Vacation day with my wife.  We checked out the beach at Ft. Lauderdale and had a a canal tour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monday:&lt;/strong&gt; (Labor day, USA holiday): Flew back to St. Louis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I stayed at the closest hotel to the &lt;a href=&quot;http://www.ihg.com/holidayinnexpress/hotels/us/en/fort-lauderdale/fllsr/hoteldetail&quot;&gt;NSU campus&lt;/a&gt;, the &lt;a href=&quot;http://www.ihg.com/holidayinnexpress/hotels/us/en/fort-lauderdale/fllsr/hoteldetail&quot;&gt;Holiday Inn Airport&lt;/a&gt;.  The hotel is a good value.  They offer a free breakfast and shuttle to/from the airport.  It is nearly a 1.2 mile walk to the campus.  The roads are walk-able and have crosswalks.  In October I might walk it and shower on campus before class.  This would save the rental car expense.  I did walk to the university a few times this trip, however, more for exercise.  After a 1.2 mile walk, in the hot Florida summer sun, I would not be very popular!&lt;/p&gt;

&lt;h2 id=&quot;program-orientation&quot;&gt;Program Orientation&lt;/h2&gt;

&lt;p&gt;The program orientation was lead by &lt;a href=&quot;http://cec.nova.edu/faculty/seagull.html&quot;&gt;Dr. Seagull&lt;/a&gt;, 
the Associate Dean of Academic Affairs at &lt;a href=&quot;http://cec.nova.edu/&quot;&gt;GSCIS&lt;/a&gt;. He presented an overview of the program 
and the school.  NSU was founded in 1964, and is currently celebrating their 
50th anniversary.  There were many different 50th anniversary banners and sings throughout 
the campus.  For my program I must complete the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;32 credit hours of course work.&lt;/strong&gt;  This will amount to 8 individual four-credit hour courses.  I am currently enrolled in my first two. I will need to fly to Ft. Lauderdale twice a semester for these courses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;8 credit hours of directed research.&lt;/strong&gt;  It will be at least a year before I start this part. However, my understanding is that I will work on a research problem with one of the professors.  This should help prepare for my own dissertation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;24(or more) dissertation hours.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The faculty is composed of all full-time professors for the doctoral programs.  For my 
first two classes, both instructors are full-time and live in the Florida area and had 
been with NSU for over a decade.&lt;/p&gt;

&lt;h2 id=&quot;artificial-intelligence-class&quot;&gt;Artificial Intelligence Class&lt;/h2&gt;

&lt;p&gt;The artificial intelligence class (CISD 760) is taught by &lt;a href=&quot;http://www.cec.nova.edu/faculty/mukherjee.html&quot;&gt;Sumitra Mukherjee&lt;/a&gt; 
and uses the textbook &lt;a href=&quot;http://aima.cs.berkeley.edu/&quot;&gt;Artificial Intelligence a Modern Approach&lt;/a&gt;.  For the first class sessions, the professor lectured on path finding, modeling, optimization, and Bayesian inference. We saw neural networks, genetic algorithms, decision trees, Bayesian belief networks and other algorithms.&lt;/p&gt;

&lt;p&gt;The professor also covered the assignments for the semester.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 1:&lt;/strong&gt; Select a peer reviewed paper in your research area of interest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 2:&lt;/strong&gt; Complete programs for four AI problems.  Path finding, vector optimization, data science/predictive modeling (neural net vs decision tree) and Bayesian inference.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assignment 3:&lt;/strong&gt; Critique the paper from assignment 1, and write an “idea paper” describing further research you might like to pursue.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;In-class mid-term&lt;/strong&gt; at the next cluster meeting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Final examination&lt;/strong&gt; assignment completed over the last several weeks of the semester.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most PhD programs have &lt;a href=&quot;http://en.wikipedia.org/wiki/Prelims&quot;&gt;qualifying exams&lt;/a&gt;, and NSU’s CS PhD program breaks this requirement 
over 8 in-class mid-semester examinations.  For the AI class, this is accomplished with 
the mid-term assignment.&lt;/p&gt;

&lt;p&gt;I’ve already started the programming assignment and am making use of Python with DEAP, 
&lt;a href=&quot;http://www.numpy.org/&quot;&gt;Numpy&lt;/a&gt; and &lt;a href=&quot;http://www.numpy.org/&quot;&gt;scikit-learn&lt;/a&gt;.  I think this 
will be a great class.  The assignment gives a good 
chance to try out some of the AI algorithms.  The assignments also allow us to start 
thinking about dissertation topics in AI.  I plan to conduct my dissertation research in 
the field of AI.&lt;/p&gt;

&lt;h2 id=&quot;data-base-management-systems-class&quot;&gt;Data Base Management Systems Class&lt;/h2&gt;

&lt;p&gt;The data base management systems class (CISD 750) class is taught by &lt;a href=&quot;http://cec.nova.edu/~jps/&quot;&gt;Junping Sun&lt;/a&gt; using 
the textbook &lt;a href=&quot;http://www.amazon.com/gp/product/0131873253&quot;&gt;Database Systems: The Complete Book (2nd Edition)&lt;/a&gt;.  For the first class 
sessions, the professor lectured on a variety of topics in database theory.  This class is different than your typical “IT SQL” class.  This course is really more on the design and implementation of an actual database system.  I was familiar with the topics discussed, but I will have quite a bit of studying to do for this class.  Both professors seemed very knowledgeable of their respective areas, and very current on the latest research.&lt;/p&gt;

&lt;p&gt;The assignments for this course are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Research Proposal.&lt;/li&gt;
  &lt;li&gt;Research Report.&lt;/li&gt;
  &lt;li&gt;In-class mid-term at the next cluster meeting.&lt;/li&gt;
  &lt;li&gt;Final examination assignment completed over the last several weeks of the semester.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are supposed to select a research topic for this course.  KDD (Knowledge Discovery 
in Databases) is one of the topics.  This is the area I plan to research.  KDD is what 
computer science groups, such as the ACM, call Data Science.&lt;/p&gt;

&lt;p&gt;So far it looks like a good program.  I wanted to enter the world of academic publishing, 
but could not fit a traditional PhD program into my life.  This program will be quite a 
bit of work.  But, so far, the program looks like it will be a good fit for me.&lt;/p&gt;

&lt;h2 id=&quot;around-the-nsu-campus&quot;&gt;Around the NSU Campus&lt;/h2&gt;

&lt;p&gt;Here are some pictures that my wife and I took around the NSU campus.&lt;/p&gt;

&lt;p&gt;The campus is large, and it took some walking to learn my way around!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-1.jpg&quot; alt=&quot;Walking to class at NSU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the main entrance to the campus below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-2.jpg&quot; alt=&quot;Jeff in front of NSU&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the Carl DeSantis building at NSU.  The Graduate School of Information and Computer Science is located here.  I spent most of my time in the building below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-3.jpg&quot; alt=&quot;Carl DeSantis Building&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, this is Ft. Lauderdale, Florida, I had to stop at the beach!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-4.jpg&quot; alt=&quot;Jeff at Ft. Lauderdale Beach&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ft. Lauderdale is made up of many interesting canals.  My wife and I also took a canal tour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-09-02-first-visit-nsu-5.jpg&quot; alt=&quot;Ft. Lauderdale Canal Tour&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Washington University Mini Medical School</title>
   <link href="http://www.heatonresearch.com/learning/2014/05/20/washington-university-mini-medical-school.html"/>
   <updated>2014-05-20T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/learning/2014/05/20/washington-university-mini-medical-school</id>
   <content type="html">&lt;p&gt;My wife and I just finished the &lt;a href=&quot;https://minimed.wustl.edu/&quot;&gt;Mini-Med series&lt;/a&gt; at &lt;a href=&quot;http://www.wustl.edu/&quot;&gt;Washington University in St. Louis&lt;/a&gt;.&lt;br /&gt;
Mini-Med is a program offered by WashU that allows laypeople to learn about medicine.&lt;br /&gt;
Each night is taught by a world class expert in their field.  The biographies of these 
lecturers is absolutely amazing!  These are some of the doctors who are pushing the 
boundaries of human understanding in medicine!&lt;/p&gt;

&lt;p&gt;There are three different courses offered in Mini-Med.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 1&lt;/strong&gt;: Lectures and some hands-on labs.  I learned to suture and used a laparoscopy simulator. I also got to tour the Washington University Genome Institute.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 2&lt;/strong&gt;: Lectures and more hands-on labs.  I saw specimens of human organs from cadavers, became CPR certified, learned to examine patients, attended a posture lab, and took a walking tour of the medical school.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mini-Med 3&lt;/strong&gt;: Revolves more around patient stories and MD’s providing information on their cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Mini-Med courses are taught around the typical semester schedule.  There are no 
classes during the summer semester.  At the end of each class you are given a certificate, 
if you attended the required number of class sessions.  This is me at the Mini-Med&lt;br /&gt;
graduation ceremony!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-1.jpg&quot; alt=&quot;Jeff Graduates Mini-Med&quot; /&gt;
Jeff graduating Mini-Med 2&lt;/p&gt;

&lt;p&gt;I really enjoyed the hand-on labs.  I particularly enjoyed trying my hand at microsurgery.&lt;br /&gt;
My wife and I also earned our CPR certification.  Most of the labs were led by medical 
students, residents and postdocs. They were very knowledgeable, and it was interesting 
talking with some of them about their medical school journey. We really enjoyed learning 
from them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-2.jpg&quot; alt=&quot;Jeff trying his hand at microsurgery&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One night we saw human organ specimens from cadavers.  It was fascinating to be able 
to hold and examine vital organs.  We saw specimens of hearts, kidneys, bladders, 
the GI track, and even the brain!  Seeing two human brains was fascinating.&lt;br /&gt;
Being an AI/Machine Learning programmer, it was fascinating to see the “real thing”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2014-05-20-washington-university-mini-medical-school-3.jpg&quot; alt=&quot;Jeff holding a human brain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My wife, Tracy, and I are both life-long learners.  We are both involved in graduate 
programs. So this is right up our alley.  Tracy is earning a masters degree in Spanish.&lt;br /&gt;
So she found it fascinating to see the actual “voice box” that she has seen many times 
in her linguistics texts.  It was fascinating for us both to learn valuable knowledge 
from outside out fields of study.&lt;/p&gt;

&lt;p&gt;I work as a data scientist for a life insurance company and also working on a doctorate 
of Computer Science.  I use predictive modeling for insurance underwriting.  Life 
underwriting has much affinity with medicine.  The two Mini-Med classes have given me 
valuable information for my job.  As a data scientist it is very useful to learn about 
the knowledge domain that you are attempting to analyze. In addition to Mini-Med I also 
worked on the &lt;a href=&quot;http://www.jeffheaton.com/2014/05/review-of-the-first-three-johns-hopkins-coursera-data-science-courses/&quot;&gt;Johns Hopkins Coursera Data Science specialization&lt;/a&gt; this semester.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>I am Starting a Ph.D. in Computer Science</title>
   <link href="http://www.heatonresearch.com/phd/compsci/2014/05/02/starting-computer-science-phd.html"/>
   <updated>2014-05-02T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/phd/compsci/2014/05/02/starting-computer-science-phd</id>
   <content type="html">&lt;p&gt;I am starting a Ph.D. in &lt;a href=&quot;http://cec.nova.edu/doctoral/cisd/index.html&quot;&gt;Computer Science&lt;/a&gt; at &lt;a href=&quot;http://www.nova.edu/&quot;&gt;Nova Southeastern University (NSU)&lt;/a&gt;, in 
Ft. Lauderdale, Florida. A doctorate level degree is something I’ve considered on-and-off 
every since I earned my masters degree. I am very interested in Artificial Intelligence, 
and have read numerous peer reviewed articles as I constructed &lt;a href=&quot;http://www.heatonresearch.com/encog&quot;&gt;Encog&lt;/a&gt; and wrote several 
&lt;a href=&quot;/aifh/&quot;&gt;books on AI&lt;/a&gt;.  As I learn more about AI and Data Science I am beginning to see the 
frontiers of human understanding in these areas.  I would really to add a small bit to 
this understanding.&lt;/p&gt;

&lt;p&gt;My goal for doing this is really to become involved in research– both independently and 
through my employer.  I currently work in “industry”, and I do not plan on changing that.&lt;br /&gt;
I have worked as an adjunct professor, on a part-time basis.  Full-time academia is not 
something I am seeking in a Ph.D.&lt;/p&gt;

&lt;h2 id=&quot;my-criteria-for-a-program&quot;&gt;My Criteria for a Program&lt;/h2&gt;

&lt;p&gt;For my “day job” I work as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Data_science&quot;&gt;data scientist&lt;/a&gt; for a large insurance company.  I am very 
active in the open source community.  I am a techie!  I love to program.  I love to 
figure things out!  This is why I have such a passion for AI and data science. There are 
a number of degrees that offer a blend of management with technology.  I have such a 
degree for my masters.  These degrees are fine, and offer a good balance.  But this was 
not what I was looking for in a doctorate.  I did not want to research how technology 
is applied to business.  I want to research technology itself.  Much as I have been 
already through open source contribution.&lt;/p&gt;

&lt;p&gt;My criteria for a program were as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I want a doctorate level degree&lt;/li&gt;
  &lt;li&gt;I do not want to move away from St. Louis, MO USA to pursue this degree&lt;/li&gt;
  &lt;li&gt;I want exposure to peer reviewed publication&lt;/li&gt;
  &lt;li&gt;I do not want to quit my job, or reduce to half-time while I pursue my degree&lt;/li&gt;
  &lt;li&gt;I a degree that focused on technology, rather than management of technology&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I began my search late in 2013.  I evaluated a number of state schools, private 
universities and even the “for profit” institutions.  Most of the &lt;a href=&quot;http://en.wikipedia.org/wiki/For-profit_education&quot;&gt;“for profit”&lt;/a&gt; 
guys only had the management based degrees.  I really could not find computer science/engineering.&lt;br /&gt;
There salesmen did try quite hard to convince me, and still call me.  But I really want 
a degree that ends with the word “engineering” or “science”, not “management” or 
“administration”. There is nothing wrong with such degrees, and I have great respect for 
them. I have certainly managed people before and worked in administration. But I don’t 
want to publish/research on “management” or “administration”.&lt;/p&gt;

&lt;p&gt;So I applied to NSU, and after a few months, was accepted.&lt;/p&gt;

&lt;h2 id=&quot;why-nsu&quot;&gt;Why NSU?&lt;/h2&gt;

&lt;p&gt;NSU is a 2nd tier non-for-profit private university located in Ft. Lauderdale, Florida.&lt;br /&gt;
They have a traditional campus, with students living there. They also have a number of 
“distance learning” options.  I am pursuing a “distance degree”, and need to visit their 
campus four times a year.  There are certainly worse places to visit than Ft Lauderdale. 
This also works well, as my wife is pursuing a masters degree in Spanish.  Miami is very 
close to Ft. Lauderdale, and is a hub of Spanish speaking culture.&lt;/p&gt;

&lt;p&gt;There are essentially three phases to a Ph.D. from NSU.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Course work, for breadth of knowledge (32 credits)&lt;/li&gt;
  &lt;li&gt;Guided research (8 credits)&lt;/li&gt;
  &lt;li&gt;Dissertation (24 credits)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I like the approach of guided research before dissertation.  The courses cover topics that 
sound interesting to me.&lt;/p&gt;

&lt;p&gt;Nova seems to be active in research.  They are in the process of building a &lt;a href=&quot;http://cec.nova.edu/research/megalodon.html&quot;&gt;super computer&lt;/a&gt;, 
named Megalodon.  You can also see some of their &lt;a href=&quot;https://gscisweb.scis.nova.edu/dlist/webview.cfm&quot;&gt;dissertations here&lt;/a&gt;.  Some of these 
dissertations, particularly in the area of computer science, sound like topics similar to 
what I might like to pursue.&lt;/p&gt;

&lt;p&gt;I will start this fall.  I am beginning with two classes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CISD 760:  Artificial Intelligence&lt;/li&gt;
  &lt;li&gt;CISD 750  Database Management Systems&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am looking forward to starting the program, and seeing what the courses are like.&lt;br /&gt;
I will travel to Ft. Lauderdale in August, for the first time.  I will write more about 
this as I continue the process.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>So you want to be a data scientist?</title>
   <link href="http://www.heatonresearch.com/datascience/2014/02/26/be-a-data-scientist.html"/>
   <updated>2014-02-26T07:26:00-06:00</updated>
   <id>http://www.heatonresearch.com/datascience/2014/02/26/be-a-data-scientist</id>
   <content type="html">&lt;p&gt;Harvard business review calls it the &lt;a href=&quot;http://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/ar/1&quot;&gt;sexiest job of the 21st century&lt;/a&gt;. But, what skills 
are needed to become a data scientist, and how can you get these skills?  I began as an 
advanced computer programmer with business knowledge.  Open Source involvement in 
Artificial Intelligence gave me the foundation to  move into a data science role.&lt;/p&gt;

&lt;p&gt;There are really three critical skills that a data scientist must posses. A data scientist 
must be a &lt;strong&gt;statistician&lt;/strong&gt;, &lt;strong&gt;domain expert&lt;/strong&gt; and &lt;strong&gt;hacker&lt;/strong&gt; - not necessarily in that order. There 
are different types of data scientist. Each type will be stronger in one of these three 
skills. Lets take a look at each of these three skills and see how you might build up 
your knowledge.&lt;/p&gt;

&lt;h2 id=&quot;statistician&quot;&gt;Statistician&lt;/h2&gt;

&lt;p&gt;Data scientists examine data and see insights and patterns in data.  Seeing patterns in 
data is nothing new. &lt;a href=&quot;http://en.wikipedia.org/wiki/Ronald_Fisher&quot;&gt;Sir Ronald Fisher&lt;/a&gt; was doing this back in 1936. Fisher was interested 
in determining what sort of flower he was looking at. You might think it is easy to 
determine a flower type. For humans this is an easy task.  However, researchers are still 
determining exactly how humans performed this seemingly simple task.&lt;/p&gt;

&lt;p&gt;How does a human likely recognize a flower? Most likely we are considering features about 
the flower. What color is it? How big is it? What does it smell like? Fisher wanted to 
determine the iris species using a specified set of features about each iris. To do this 
he collected four numeric measurements for 150 iris flowers. This included 50 flowers 
each from three different iris species. Using these irises he was able to make a 
statistical model that would tell him the iris species for a new flower using just these 
four measurements.&lt;/p&gt;

&lt;p&gt;There are many different ways to acquire statistical skills. If you are not familiar with 
basic statistics terms such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_score&quot;&gt;z-score&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/P-value&quot;&gt;p-value&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;ANOVA&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution&quot;&gt;Normal Distribution&lt;/a&gt;, you should 
start with a &lt;a href=&quot;https://www.udacity.com/course/st095&quot;&gt;Statistics 101&lt;/a&gt; type class. &lt;a href=&quot;https://www.udacity.com/&quot;&gt;UDacity&lt;/a&gt; offers several good choices for this. 
As your statistical skills grow, you will find &lt;a href=&quot;https://www.khanacademy.org/&quot;&gt;Khan Academy&lt;/a&gt; to be indispensable. I’ve 
learned a great deal pouring over Khan Academy and Wikipedia pages for several statistical 
models. As you advance, &lt;a href=&quot;http://en.wikipedia.org/wiki/Artificial_intelligence&quot;&gt;Artificial Intelligence&lt;/a&gt; and specifically &lt;a href=&quot;http://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Machine Learning&lt;/a&gt; will 
also become important. I’ve written several &lt;a href=&quot;/book/&quot;&gt;books&lt;/a&gt; in this space that might be useful for you.&lt;/p&gt;

&lt;h2 id=&quot;domain-expert&quot;&gt;Domain Expert&lt;/h2&gt;

&lt;p&gt;A domain expert is someone who has real-world knowledge about the data that they are 
analyzing. While Fisher was a statistician, he was also an &lt;a href=&quot;http://en.wikipedia.org/wiki/Evolutionary_biology&quot;&gt;evolutionary biologist&lt;/a&gt; and a 
&lt;a href=&quot;http://en.wikipedia.org/wiki/Geneticist&quot;&gt;geneticist&lt;/a&gt;. Fisher was a domain expert. The collected iris data was not just a sheet of 
numbers to him. He knew something about the iris flowers he was analyzing.&lt;/p&gt;

&lt;p&gt;This domain knowledge allowed him to know what flower measurements to consider. Fisher 
measured the length and width of both the &lt;a href=&quot;http://en.wikipedia.org/wiki/Petal&quot;&gt;petal&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Sepal&quot;&gt;sepal&lt;/a&gt; of each iris flower. He did not 
measure the roots, stem thickness or chemical makeup of each flower. Because Fisher knew 
something about flowers, he had an idea of which measurements to consider. He also knew 
if his results made any sense.&lt;/p&gt;

&lt;p&gt;Being able to determine if your model makes any sense is critical. &lt;a href=&quot;http://en.wikipedia.org/wiki/Dogs_of_the_Dow&quot;&gt;Dogs of the Dow&lt;/a&gt; was a 
popular investing strategy from early 1990’s that would seemingly pick winning portfolios 
based on very simple data. Just plug in the dividend yields of the DJIA-30 stocks to gain 
a portfolio that beats the overall stock market average. Analysis of historic data created 
this model. The problem is that this model fit the historic data much better than it did 
the future data. “Dogs of the Dow” found a mostly coincidental pattern in the historic 
data. While there are some holdouts, the Dogs of the Dow is now a &lt;a href=&quot;http://online.barrons.com/article/SB50001424053111904399004579266300171650902.html&quot;&gt;largely discredited&lt;/a&gt; 
model.&lt;/p&gt;

&lt;p&gt;Becoming a domain expert is somewhat more elusive. The first question you should be 
asking is “what domain?” You might choose a domain such as finance, marketing, biology, 
or any other common business field. It will be helpful if you already have experience in 
a particular industry. If not, try to take some courses that will expose you to the data 
of a particular industry. Economics, marketing and finance classes are always good choices.&lt;/p&gt;

&lt;h2 id=&quot;hacker&quot;&gt;Hacker&lt;/h2&gt;

&lt;p&gt;Finally, a data scientist must be a &lt;a href=&quot;http://en.wikipedia.org/wiki/Hacker_(programmer_subculture)&quot;&gt;hacker&lt;/a&gt;. At first this idea might seem strange. By 
hacker, I do not mean someone who attempts to &lt;a href=&quot;http://en.wikipedia.org/wiki/Hacker_(computer_security)&quot;&gt;circumvent computer security&lt;/a&gt;. For this 
definition, a hacker is a programmer. However, not every programmer is a hacker. A hacker 
is a programmer who will hack at a problem until that problem is solved. The hacker is not 
intimidated by hitting a brick wall. The hacker will come up with a very creative way 
around the brick wall, even if earlier attempts have all failed.&lt;/p&gt;

&lt;p&gt;Fisher did not need to be a hacker. Fisher had 150 flowers to analyze. He measured each 
one by hand and made sure his data was clean and accurate. Consider if Fisher had 150 
million flowers. Further, a mechanical process with a 90% accuracy rate measured each of 
these flowers. Now we have a huge amount of somewhat inaccurate data. We now have a 
“&lt;a href=&quot;http://en.wikipedia.org/wiki/Big_data&quot;&gt;Big Data&lt;/a&gt;” problem. Big Data is any data set that is so large that it is difficult to 
work with. Typically “Big Data” starts at the point where a data set can no longer fit in 
the memory of a single computer. Not long ago everything over 640k (the original useable 
memory size of a PC) was “Big Data”.&lt;/p&gt;

&lt;p&gt;The hacker can wrangle “Big Data” and get it into a form that a statistical model can 
handle. This wrangling might mean merging data from many sources, or writing automated 
programs to harvest data from the Internet. The hacker might need to clean the data in 
some way. The data might need further wrangling to even get it into a statistical model.&lt;/p&gt;

&lt;p&gt;If you are a computer programmer already, then you already have some of the hacker skills. 
If you are a computer programmer who spends their free time learning new programming 
skills and perhaps contributing to open source, then you might be a hacker! If not, try 
something new. Two of the most predominant data science languages are Python and R. Java, 
C# and C/C++ are also choices. &lt;a href=&quot;https://www.udacity.com/&quot;&gt;UDacity&lt;/a&gt; and Coursera(https://www.coursera.org/) both have several courses to allow 
you to use hacker skills to sharpen data science. The best way to learn to be a hacker 
is to hack. Practice examples and then experiment with data that interests you.&lt;/p&gt;

&lt;h2 id=&quot;acquiring-data-science-skills&quot;&gt;Acquiring Data Science Skills&lt;/h2&gt;

&lt;p&gt;To summarize, a data scientist must have three primary skills.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mathematics &amp;amp; Statistics (statistician)&lt;/li&gt;
  &lt;li&gt;Business Domain Knowledge (real world knowledge)&lt;/li&gt;
  &lt;li&gt;Hacker (creative computer programmer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are also a number of programs available to teach data science.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/specialization/jhudatascience/1&quot;&gt;Johns Hopkins Data Science Specialization (Coursera)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.udacity.com/courses#!/Data%20Science&quot;&gt;UDacity Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://datascience.berkeley.edu/&quot;&gt;Berkeley Online Masters Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.ccsu.edu/datamining/&quot;&gt;University of Connecticut Data Mining Masters Degree&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are also many great data science blogs.  I personally read the following.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hilarymason.com/&quot;&gt;Hilary Mason&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://datascience101.wordpress.com/&quot;&gt;Data Science 101&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.r-bloggers.com/&quot;&gt;R Bloggers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kdnuggets.com/&quot;&gt;KD Nuggets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For me, the road to data science started at programming. I worked for many years as a 
computer programmer in the life insurance industry. As a result, I learned quite a bit 
about life insurance data. I also learned how to crunch data and provide reports to 
present data. Long before I had ever heard the term “data science,” I developed an 
interest in Artificial Intelligence. What does a hacker do when they are interested in 
something? I started experimenting and programming. I learned more about AI. This 
knowledge ultimately opened doors and I moved into more of a data science role.&lt;/p&gt;

&lt;p&gt;If you are interested in AI, you might find some of my projects interesting. I have a 
machine learning open source projects, write a blog and write books.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Using Kickstarter to Fund an Indie Technical Book</title>
   <link href="http://www.heatonresearch.com/aifh/kickstarter/2013/07/20/kickstarter-techie-book.html"/>
   <updated>2013-07-20T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/aifh/kickstarter/2013/07/20/kickstarter-techie-book</id>
   <content type="html">&lt;p&gt;During June and July of 2013 I &lt;a href=&quot;http://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-1-fund-algo&quot;&gt;funded a book project&lt;/a&gt; using &lt;a href=&quot;http://www.kickstarter.com/&quot;&gt;Kickstarter&lt;/a&gt; for total pledges 
of $18,889 USD from 818 project backers. Because my initial requested amount was $2,500 
this project was funded at a level of 755%.  To see my Kickstarter project, &lt;a href=&quot;http://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-1-fund-algo&quot;&gt;click here&lt;/a&gt;.&lt;br /&gt;
What does this all mean?  If you are familiar with Kickstarter these stats are likely 
very familiar.&lt;/p&gt;

&lt;p&gt;If you are not familiar with Kickstarter, let me explain how this works.  Kickstarter is 
all about crowdfunding.  Crowdfunding is where you obtain money from many individuals to 
achieve some sort of goal.  People who support a crowdfunding project are called backers.&lt;br /&gt;
Backers may or may not get something in return for their support.  These days just about 
&lt;strong&gt;EVERYTHING is being crowdfunded&lt;/strong&gt;.  You can even use crowdfunding to help with the costs of 
&lt;a href=&quot;https://www.adopttogether.org/&quot;&gt;adopting a child&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In my case, I want to write a book as an &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-publishing&quot;&gt;indie publisher&lt;/a&gt;.  This is nothing new to me. 
I’ve published a number of books both with big publishing houses, as well as on my own.&lt;br /&gt;
I am very much sold on the indie publishing route.  Other than the obvious creative 
freedom  indie publishing is simply more lucrative.  At least for my particular &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_intelligence&quot;&gt;niche&lt;/a&gt;.&lt;br /&gt;
Your results may vary!&lt;/p&gt;

&lt;h2 id=&quot;what-does-it-cost-to-indie-publish-a-book&quot;&gt;What Does it Cost to Indie Publish a Book?&lt;/h2&gt;

&lt;p&gt;I am creating a new book about Artificial Intelligence Programming.  It will be the first 
in a series of books.  I’ve written on &lt;a href=&quot;http://www.heatonresearch.com/book&quot;&gt;this topic&lt;/a&gt; before.  As an indie author I do have 
some costs.  These costs break down as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Editing&lt;/li&gt;
  &lt;li&gt;Book Cover&lt;/li&gt;
  &lt;li&gt;Book design and layout&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may have additional costs beyond these.  However, for me, these are the big three.&lt;br /&gt;
My book is essentially a technical/scientific/mathematical book.  Because of this I 
simply lay it out in &lt;a href=&quot;http://www.latex-project.org/&quot;&gt;LaTeX&lt;/a&gt; for typesetting, and bullet three above ends up costing me 
very little.  If you are not that familiar with typesetting then “step three” may 
cost more.&lt;/p&gt;

&lt;p&gt;I’ve found that previous book projects cost me around $2,500 USD to get off the ground.&lt;br /&gt;
Because of this I set my Kickstarter goal to be $2,500.  However, the question becomes, 
“Will I ever sell enough copies to make this money back?”  This is where Kickstarter 
comes in.  People will pledge money to get the finished project.  Essentially I am 
pre-selling the book.  For myself, as an author, this is an advance.  Though, $18k is a 
considerably larger advance than I ever got working with large publishing houses.&lt;/p&gt;

&lt;p&gt;I set my goal to be $2,500 over 30 days.  A project length of 30 days is VERY common in 
Kickstarter.  Most advice suggests to go with 30 days.  This seems about right for me, 
and I am not going to suggest to the contrary.  Under this model, I had 30 days to gain 
enough pledges to meet $2,500.  If I did not hit $2,500, then I get nothing. Kickstarter 
is all or nothing.&lt;/p&gt;

&lt;p&gt;Setting your funding goal is very important!&lt;/p&gt;

&lt;h2 id=&quot;setting-a-funding-goal&quot;&gt;Setting a Funding Goal&lt;/h2&gt;

&lt;p&gt;Kickstarter is all or nothing.  Either I hit (or exceed) the funding goal, or you get 
nothing!  My project would forever revert to the status of “Unsuccessful”.  That worried 
me a bit!  I did not want a failed Kickstarter page as a lasting monument to this book.&lt;/p&gt;

&lt;p&gt;There are two schools of thought on funding goal.  I simply set my funding goal to exactly 
what I needed to create the book.  If I hit $1000 and not $2500, then I don’t have enough 
money to produce my product and the project should be canceled.&lt;/p&gt;

&lt;p&gt;The other school of thought is to set this value higher.  There is a fear that if you hit 
your goal, in my case, $2,500, your project will lose all motivation and you won’t get 
much funding.  I think there is some truth in that.  I’ve seen projects burst towards an 
unmet goal in the final days of the project.&lt;/p&gt;

&lt;p&gt;Ultimately, this is your own decision. When I do the second volume in this series, I will 
very likely set it to $2,500 again.  I felt like this was high enough that it seemed 
“non-trivial”, yet was actually a genuine estimate of my costs.  I do not think my 
project lost any motivation once it hit $2,500.  Actually quite the opposite.  For a 
variety of reasons, which I will cover later in this post, my project really took off 
after it met its funding goal.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-my-kickstarter-book-project&quot;&gt;Overview of My Kickstarter Book Project&lt;/h2&gt;

&lt;p&gt;My Kickstarter project started on June 10, 2013, you can see it at &lt;a href=&quot;http://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-1-fund-algo&quot;&gt;this link&lt;/a&gt;.  You can see 
its complete progress here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-08-20-kickstarter-techie-book-1.png&quot; alt=&quot;AI For Humans Volume 1 Funding Progress&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here are some important dates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;June 9:&lt;/strong&gt; Project starts.  I post to my author Facebook page and Twitter that I started a Kickstarter project.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 11:&lt;/strong&gt; Project is going slowly!  I post a more compelling “call to action” to all followers.  This actually gets me to 50%.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 18:&lt;/strong&gt; Now at 50% I post this as news to my blog, Facebook and Twitter.  This gives some “buzz” and I quickly go to 80%.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 24:&lt;/strong&gt; When my project got to within $300 of goal, a very generous backer pledged the remaining amount and put me at exactly $2500.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 25:&lt;/strong&gt; The news of the project hitting $2500 caused quite a few additional backers.  I reached 110% the next day.  I also adjusted the embedded widget on my website to explain what the widget was and more of a call for action.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 31:&lt;/strong&gt; Pretty much a social media chain-reaction.  One of the backers posted the project to Hacker News.  This created a ton of traffic to the project.  This traffic translated into backers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;July 1:&lt;/strong&gt; Another backer posts the project to Reddit.  This causes a similar traffic burst.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;July 2:&lt;/strong&gt; The combined traffic of Reddit and Hacker News causes Kickstarter to push my project to the front page of Kickstarter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;July 3-10:&lt;/strong&gt; Between remnant Reddit and Hacker News traffic, and ongoing placement on Kickstarter popular projects the project stays in a very nice trend until the project ends.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I feel like the project got its initial lift, to $2,500 from my existing Facebook and 
Twitter followers.  However, the major bang came from &lt;a href=&quot;https://news.ycombinator.com/item?id=5976590&quot;&gt;Hacker News&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/programming/comments/1hi3gs/artificial_intelligence_for_humans_fundamental/&quot;&gt;Reddit&lt;/a&gt; and finally 
Kickstarter itself. So where did all of these backers come from?  Here is a fragment from 
my backer source report.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-08-20-kickstarter-techie-book-2.png&quot; alt=&quot;AI For Humans Volume 1 Funding Sources&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is just the top portion of the chart, there were quite a few other sources too, just in lower dollar amounts.  As you can see, Direct Traffic accounted $3,460 worth of backers, this was mostly Hacker News. Kickstarter Popular really delivered.  They broke this into the home page, as well as Popular (Discover).  The Embedded Widget also delivered $1,349.  This is a widget that I embedded in several of my sites.  You can see it here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-08-20-kickstarter-techie-book-3.png&quot; alt=&quot;AI For Humans Volume 1 Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Just my followers alone would have gotten me past the goal, and then some.  However, the real power came from social media, particularly Kickstarter itself.&lt;/p&gt;

&lt;h2 id=&quot;was-kickstarter-worth-it&quot;&gt;Was Kickstarter Worth It?&lt;/h2&gt;

&lt;p&gt;I was not sure if I should use Kickstarter for this project.  I figured I already had my 
established base of followers.  I could even do some sort of pre-sale event on my own 
website.  Kickstarter also takes 5%.  In the end, I decided that maybe running it on 
Kickstarter would get me more followers in return for that 5%.  I did get more followers, 
but I also got my total revenue more than doubled.  The following chart shows it the best.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-08-20-kickstarter-techie-book-4.png&quot; alt=&quot;AI For Humans Volume 1 Widget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Just over half of my revenue came from Kickstarter!  &lt;script type=&quot;math/tex&quot;&gt;So YES, Kickstarter was worth their very small 5%.&lt;/script&gt;&lt;br /&gt;
Kickstarter pledges were $9,762, whereas others were $9,127.&lt;/p&gt;

&lt;h2 id=&quot;project-video-and-story&quot;&gt;Project Video and Story&lt;/h2&gt;

&lt;p&gt;The project video is very important.  It is very important to show what your book is, and why is it important.  My video probably could have been better.  I feel like my story (the text that goes with your project) was better than the video.  I tried to focus on really two aspects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Artificial Intelligence (AI) is just an inherently cool topic.&lt;/strong&gt;  There are many things people can use AI for, I can’t cover them all. It really is just a very cool technology.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Science is becoming the practical business application of AI.&lt;/strong&gt; You can get a job “doing AI”.  For most business this is called Data Science.  Data Scientist is becoming a very hot career field.  Harvard Business Review even called “Data Scientist” the sexiest job of the 21st century!  &lt;a href=&quot;http://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/ar/2&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Every project will take a different video.  I suggest you find books like yours and watch their videos!  Make it relavant!&lt;/p&gt;

&lt;h2 id=&quot;deciding-on-rewards&quot;&gt;Deciding on Rewards&lt;/h2&gt;

&lt;p&gt;Rewards are what cause backers to pledge.  I tried a variety of rewords.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $3&lt;/strong&gt; (3 backers): Supporter, bronze level. Show your support! As thanks for your pledge, you’ll be updated on the project’s progress.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $7&lt;/strong&gt; (100 backers): Let’s get rolling! The first 100 backers get the e-book for less. Don’t wait, sponsor me today. Receive a DRM-Free electronic copy of the e-book (PDF, MOBI, ePub). Plus free updates for life!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $9&lt;/strong&gt; (294 backers): Buy the ebook. Receive a DRM-Free electronic copy of the e-book (PDF, MOBI, ePub).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $29&lt;/strong&gt; (297 backers): EBook extravaganza. Buy the ebook. Receive a DRM-Free electronic copy of the e-book (PDF, MOBI, ePub). Plus all of my current AI books. Every book listed at this URL. http://www.heatonresearch.com/book/cat/1&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $34&lt;/strong&gt; (46 backers): Buy the paperback &amp;amp; ebook. You will receive a signed copy of the paperback edition of the book. Add $10 for shipping outside the USA. You will also receive a DRM-Free electronic copy of the e-book (PDF, MOBI, ePub).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $49&lt;/strong&gt; (47 backers): Paperback + EBook Extravaganza - Buy the ebook &amp;amp; paperback of AIFH V1. Plus all of my current AI books. Every book listed at this URL. http://www.heatonresearch.com/book/cat/1&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $49&lt;/strong&gt; (2 backers): Beta tester! You will receive three draft updates as the book is created. You will be seeing material at the same time it goes to the editor. At the end, you will receive a DRM-Free electronic copy of the e-book (PDF, MOBI, ePub).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $59&lt;/strong&gt; (5 backers): Paperback extravaganza. You will get Java &amp;amp; C# versions of “Programming Neural Networks with Encog3”, “Introduction to Neural Networks”, plus a paperback edition of the new AIFH Vol1 (this Kickstarter). You will also get the ebooks. There will be two shipments. The published books in July, and AIFHV1 in Dec 2013. Does not include a paperback of “Math of Neural Networks”, as that book was only released as ebook. (This reword added at the last minute for a request).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $100&lt;/strong&gt; (19 backers): Supporter, silver level. You will receive the paperback, ebook, beta test updates, just as above. However, you will also be mentioned in the books acknowledgements as a supporter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $500&lt;/strong&gt; (0 backers): Assign me a language! Get the ebook, signed paperback, beta updates, and mention in the books acknowledgements. Plus, assign me an additional language to add examples for. Your choice of Groovy, Scala, F#, Clojure, Python or PHP.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pledge $1,000&lt;/strong&gt; (0 backers): Supporter, gold level. Get the ebook, signed paperback, beta updates and mention in the book’s acknowledgements. Plus, you will be listed on the homepage (http://www.heatonresearch.com/) as a supporter for one year. I will include a link to a website of your choice. Target site cannot be for online gambling, pornography, politics, social cause or anything illegal that might get me in trouble! (if you are unsure, check with me first).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My “high ticket” pledges did nothing.  I was not sure about these, but I did see other 
projects have some success with these.  I would not spend much time thinking about 
high-ticket pledges.  For the next volume I will probably have one $1k high-ticket pledge 
and the rest will top out at $250.&lt;/p&gt;

&lt;p&gt;I also did an Early Bird pledge level. I’ve seen other sites do this.  The idea is that 
it helps secure some initial buzz for your project.  In reality, I am not sure the 
difference between $9 and $7 meant much.  Next time I will probably just do the $9 
reward level.  $9 is for an ebook, which is what I will ultimately price this book at 
on Amazon.&lt;/p&gt;

&lt;p&gt;The “ebook extravaganza” was every popular.  I basically threw in my previous AI books at 
a discount.  This will be a very important component for future Kickstarter projects.&lt;br /&gt;
The idea here is to give the backers something to read while they wait for the book to be 
done.  If you have other material you can package up in a digital format, this might be an 
option for you!&lt;/p&gt;

&lt;p&gt;The $100 backers allowed people to express additional support, and also be mentioned in 
the books acknowledgments. This was a popular level and contributed to the bottom line.&lt;/p&gt;

&lt;p&gt;Look at other projects and pick rewards that make sense!  This is what I did.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-word-out&quot;&gt;Getting the Word Out&lt;/h2&gt;

&lt;p&gt;Publicity is critical.  While I did get quite a few backers from Kickstarter, I would have 
not done this without external sources.  Kickstarter is an amplifier.  An amplifier takes 
a signal and makes it stronger.  However, it has to be a good signal to begin with!&lt;br /&gt;
Kickstarter will not “kick in” until your project reaches some degree of popularity.  To 
do this, you need to rely on sources such as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your own blog:&lt;/strong&gt; I have two primary sites.  My blog and Heaton Research.  Together these 
sites see about 100k hits a month.
Other blogs: Try to reach out to similar blogs.  Ideally BEFORE you start a Kickstarter 
campaign.  Don’t view similar blogs as competitors.  Comment on their stories and get to 
know them.  They might help you promote an occasional project. My blog has a decent amount 
of traffic, and I am often very willing to do this.  If the content is in line with my 
topic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Social News Sites:&lt;/strong&gt;  Decide what social news sites make sense.  Don’t post a children’s 
book to Hacker News.  Write supporting articles and post those. But think about posting 
to the Reddits, Slashdots, DZones, Diggs and Hacker News sites.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Facebook and Twitter:&lt;/strong&gt; If you have a blog, you should have a page for followers.  This 
is mine.  Twitter is important too.  I earned several thousand dollars on this Kickstarter 
from these two.
Pay Attention&lt;/p&gt;

&lt;p&gt;You backers (and potential backers) will post comments and ask questions.  A quick reply 
will often win a pledge.  Also look for FAQ’s.  You can easily add FAQ’s to your project.&lt;br /&gt;
I ended up being out of town for the last 4 days of my Kickstarter!  It actually ended on 
a travel day!  Here I am in Denver International Airport responding to queries on 
Kickstarter.  I was on my way back from San Francisco to St. Louis.  Yeah for Denver for 
having free wifi!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-08-20-kickstarter-techie-book-5.jpg&quot; alt=&quot;Jeff Heaton at Denver International Airport&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Questions from users will often give you great ideas for additional pledge levels to add.&lt;/p&gt;

&lt;h1 id=&quot;after-the-project-ends&quot;&gt;After the Project Ends&lt;/h1&gt;

&lt;p&gt;Other than the obvious task of writing your book, here are a few suggestions that I have.&lt;/p&gt;

&lt;p&gt;You can only survey each gift type once:  I did not know this going in. Ultimately you will send your backers a survey when your project has been funded and you are ready to send them their reward.  This survey allows you to collect necessary information to get them their reward.  Here is the part I did not know.  You can only send the survey once!  Why is this a problem?  I have rewards that are staggered.  If they got my ebook extravaganza I had planned (and offered in the text of the gift) to send that in July.  But my book won’t be done until December. So for a “paperback + ebook extravaganza” backer I need their email address NOW and their shipping address in December.  If I use a survey I must ask for BOTH now.  What if they move over those 6 months?  To solve this I am simply messaging all backers that I need info from NOW. &lt;br /&gt;
Make a final edit to your story: Once your project funding period ends in either success or failure, your project’s story will “lock”.  You will not be able to change it.  I left a link on my project page with instructions for users who might have “missed the Kickstarter”, you can see mine &lt;a href=&quot;http://www.kickstarter.com/projects/jeffheaton/artificial-intelligence-for-humans-vol-1-fund-algo&quot;&gt;here&lt;/a&gt;.  This provides a link to a page on my own website that will evolve as the project does.  For now, it links to a EJunkie Buy Button that allows users to still order the ebook extravaganza or preorder.  Eventually it will likely take them to Amazon to buy the finished book.  (once I finish the book).
Provide translation: It is easy to provide a translation.  Just use Google translate.  You can find a great example of a project doing it &lt;a href=&quot;http://www.kickstarter.com/projects/lightup/lightup-learn-by-making&quot;&gt;here&lt;/a&gt;.  I did not do this.  I am selling an English book.  If you can’t read my promo page, you probably do not need my book.
Plans for Next Time&lt;/p&gt;

&lt;p&gt;I will do a Kickstarter project again.  This is a multi-volume set.  The main points for improvement are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Better video.&lt;/strong&gt;  There will be less of me talking into a camera.  More of me doing voice-overs with animations showing what the book is about.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hit social media sooner.&lt;/strong&gt;  I will time a series of articles about examples from the next book to go out every few days once the Kickstarter campaign begins.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Wire release.&lt;/strong&gt;  I will try a wire release from one of the sites that do this sort of thing.  If I use a number of media-friendly (and accurate) buzz words like “Big Data”, “Data Science”, “Business Intelligence”, this will probably get some useful links from some of the media sites.  Probably not going for CNN level coverage, but coverage from some of the business trade journals would be nice.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>2013 Visit to the Computer History Museum in Palo Alto</title>
   <link href="http://www.heatonresearch.com/compsci/2013/07/17/visit-to-computer-history-museum.html"/>
   <updated>2013-07-17T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/compsci/2013/07/17/visit-to-computer-history-museum</id>
   <content type="html">&lt;p&gt;During July 2013 I happened to be in Silicon Valley.  While there my wife and I toured 
the Computer History Museum.  If you find your self in Mountain View, CA, it is worth a 
visit.  They have computer systems from the very beginning (abacus) to today (tablets 
and phones).  There are quite a few computers from the 1940s-1960s.  It was also 
interesting seeing the Google Street View Car and an IBM Watson set.&lt;/p&gt;

&lt;p&gt;It is a very cool looking building.  Here I am near the entrance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-1.jpg&quot; alt=&quot;Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The museum did have some information on Artificial Intelligence.  There was a very 
interesting video that explained why some problems, such as AI, are in fact very hard.&lt;br /&gt;
They had an exhibit featuring IBM Watson.  I had to take a picture at this location!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-2.jpg&quot; alt=&quot;IBM Watson at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;They also had quite a few robots!  I would have loved to see a robot out and moving about, 
but alas no.  However, they DID have a working Babbage machine.  Seeing a real Babbage 
machine calculate a log table was amazing!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-3.jpg&quot; alt=&quot;Robots at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was fascinating seeing the Xerox Alto.  This is the machine that started it all!  The 
mouse and GUI!  Xerox was too much of a lumbering corporate dinosaur to realize what they 
had invented.  Instead of capitalizing on their new GUI, Xerox gives Steve Jobs was given 
a tour.  Steve Jobs then promptly stole this technology and created the Mac.  Then Bill 
Gates promptly stole Steve Jobbs’ “stolen property” and created Windows!  Which then 
caused Apple to sue Microsoft for stealing what Apple had “rightly stolen” from Xerox!&lt;br /&gt;
Gets complicated! :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-4.jpg&quot; alt=&quot;Xeros Alto at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I wish they had more in the 8-bit era!  As a generation x-er 8-bits are where I got my 
start!  Ah the days of programming 6510 machine language!  Here you see my by a 
Commodore 64 and Vic 20.  Would have loved to have seen just a little more on Commodore!&lt;br /&gt;
Rather than just two machines on a shelf that I have to stand on my tip-toes to see!&lt;br /&gt;
Oh well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-5.jpg&quot; alt=&quot;Commodore at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sitting in a Google Street View car was also fun!  I had also used Google Maps to help me 
find my way to the Computer History Museum!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-6.jpg&quot; alt=&quot;Google Car at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There were several video interviews with Donald Knuth!  I am a major Knuth fan!  I’ve 
used several of his algorithms in The Art of Computer Programming for Encog!  Specifically 
in the areas of random number generation and efficient string comparison/sorting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-07-17-visit-to-computer-history-museum-7.jpg&quot; alt=&quot;Knuth Quote at the Computer History Museum&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was a fun visit.  Really like the Computer History Museum.  It was only 10 minutes 
from where I was staying in Palo Alto.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GPU Programming in Java Using OpenCL (JUG)</title>
   <link href="http://www.heatonresearch.com/opencl/gpu/2013/06/13/jug-java-gpu.html"/>
   <updated>2013-06-13T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/opencl/gpu/2013/06/13/jug-java-gpu</id>
   <content type="html">&lt;p&gt;Tonight (June 13, 2013), I am giving a presentation at the &lt;a href=&quot;http://java.ociweb.com/javasig/&quot;&gt;St. Louis Java User’s Group&lt;/a&gt; on 
the topic of &lt;a href=&quot;http://java.ociweb.com/javasig/knowledgebase/2013-06/index.html&quot;&gt;GPU Programming in Java Using OpenCL&lt;/a&gt;. This features the &lt;a href=&quot;http://www.lwjgl.org/&quot;&gt;LWJGL framework&lt;/a&gt;.&lt;br /&gt;
Included are links to the slides, as well as over 45 minutes of video. The description 
of this talk is as follows.&lt;/p&gt;

&lt;h2 id=&quot;about-the-presentation&quot;&gt;About the Presentation&lt;/h2&gt;

&lt;p&gt;General Purpose Computing on Graphics Processing Units (GPGPU) allows you to make use of 
your video card (GPU) to perform general purpose computing. The typical GPU contains 
hundreds of cores capable of performing mathematically intense operations. However, 
harnessing the power of the GPU is much different than the traditional CPU programming 
that most programmers are used to. This presentation will show how to make use of the GPU 
from Java using OpenCL. OpenCL abstracts the differences between competing GPU 
architectures.&lt;/p&gt;

&lt;p&gt;GPU’s are not suited to every task, so the criteria for a good GPU task will be reviewed. 
To demonstrate the power of the GPU in Java I will show how to use the GPU to speed a 
Big Data process over a large volume of financial data. This can greatly speed up certain 
data mining and predictive Modeling applications. I will also show how GPU’s can be used 
in an Amazon EC2 cluster.&lt;/p&gt;

&lt;h2 id=&quot;about-the-presenter&quot;&gt;About the Presenter&lt;/h2&gt;

&lt;p&gt;Jeff Heaton is a Lead Software Analyst at the Reinsurance Group of America (RGA). Jeff is 
also the lead developer of the Encog Machine Learning Framework and a Senior Member of 
the IEEE. Jeff has worked with technologies such as Java, C#, C/C++, Groovy, Scala, R 
and Octave.&lt;/p&gt;

&lt;p&gt;Materials&lt;/p&gt;

&lt;p&gt;Here are the links and materials I discussed during the presentation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OpenCL Hello World: &lt;a href=&quot;https://github.com/jeffheaton/opencl-hello-world&quot;&gt;https://github.com/jeffheaton/opencl-hello-world&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LWJGL: &lt;a href=&quot;http://www.lwjgl.org/&quot;&gt;http://www.lwjgl.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Encog CUDA Kernel: &lt;a href=&quot;https://github.com/encog/encog-c/blob/master/encog-core/cuda_eval.cu&quot;&gt;https://github.com/encog/encog-c/blob/master/encog-core/cuda_eval.cu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Encog Project: &lt;a href=&quot;http://www.encog.org&quot;&gt;http://www.encog.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;My Slides: &lt;a href=&quot;http://java.ociweb.com/javasig/knowledgebase/2013-06/JUG_GPU.pdf&quot;&gt;Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Youtube Slides: &lt;a href=&quot;http://www.youtube.com/watch?v=4q9fPOI-x80&quot;&gt;Part 1&lt;/a&gt; &lt;a href=&quot;http://www.youtube.com/watch?v=gZDNvL28PVA&quot;&gt;Part 2&lt;/a&gt; &lt;a href=&quot;http://www.youtube.com/watch?v=mTOcsUFyETM&quot;&gt;Part 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Basic Classification in R: Neural Networks and Support Vector Machines</title>
   <link href="http://www.heatonresearch.com/r/ai/2013/06/12/r-classification.html"/>
   <updated>2013-06-12T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/r/ai/2013/06/12/r-classification</id>
   <content type="html">&lt;p&gt;In this article I will introduce you to classification in R. We will use the Iris data 
set to perform this classification.  The Iris data set is a classic data set that is 
often used to demonstrate machine learning.  This data set provides four measurements 
for three different iris species.  Data such as this typically comes in a CSV File.  The 
iris CSV file looks something like this.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;quot;sepal_l&amp;quot;,&amp;quot;sepal_w&amp;quot;,&amp;quot;petal_l&amp;quot;,&amp;quot;petal_w&amp;quot;,&amp;quot;species&amp;quot;
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can download the above file here.&lt;/p&gt;

&lt;h2 id=&quot;reading-a-csv-file-in-r&quot;&gt;Reading a CSV File in R&lt;/h2&gt;

&lt;p&gt;By default R expects to find files in your home directory.  You can also specify a full path.  We will now load the iris dataset.  Of course, R has the iris dataset build into the variables iris and iris3.  However, we will assume that you might want to use your own dataset.  Therefore I will demonstrate how to load the iris.csv file.  The following command is used to load the Iris data set.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;irisdata &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;iris.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;head&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;sep&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also load the data right over the web.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;irisdata &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://www.heatonresearch.com/dload/data/iris.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;head&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;sep&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now that the iris data set is loaded, you can display the entire data set just by entering the variable name.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt; irisdata
sepal_l sepal_w petal_l petal_w species
1 5.1 3.5 1.4 0.2 Iris-setosa
2 4.9 3.0 1.4 0.2 Iris-setosa
3 4.7 3.2 1.3 0.2 Iris-setosa
4 4.6 3.1 1.5 0.2 Iris-setosa
5 5.0 3.6 1.4 0.2 Iris-setosa
6 5.4 3.9 1.7 0.4 Iris-setosa
7 4.6 3.4 1.4 0.3 Iris-setosa
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also use the summary function to provide a very useful summary of the iris data.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt; summary(irisdata)
 sepal_l sepal_w petal_l petal_w 
 Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 
 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 
 Median :5.800 Median :3.000 Median :4.350 Median :1.300 
 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 
 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 
 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 
 species 
 Iris-setosa :50 
 Iris-versicolor:50 
 Iris-virginica :50&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;training-and-validation-data&quot;&gt;Training and Validation Data&lt;/h2&gt;

&lt;p&gt;It is often useful to break the data into training and validation sets.  This allows you to validate the SVM or ANN on data that it was never trained with.  The Iris dataset has 150 elements in it.  For our training set we will sample 100 elements from this 150 element set.  This is done with the following commands.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;irisTrainData = sample(1:150,100)
irisValData = setdiff(1:150,irisTrainData)
It is very important to note that the above vectors are only indexes, and not the actual data.  To obtain the actual data you must use one of the following commands.

irisdata[irisTrainData,]
irisdata[irisValData,]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;using-a-support-vector-machine-svm&quot;&gt;Using a Support Vector Machine (SVM)&lt;/h2&gt;

&lt;p&gt;I will now show you how to train a support vector for the Iris data set.  First, we must tell R that we are using SVM’s.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;kernlab&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we create a radial basis function (RBF) that will be used during training.  This will be used as the kernel function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;rbf &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; rbfdot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sigma&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we train the SVM.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;irisSVM &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; ksvm&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;species&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;irisdata&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;irisTrainData&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;type&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;C-bsvc&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;kernel&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rbf&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;C&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;prob.model&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we get the fitted values for this iris SVM.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;fitted&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;irisSVM&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Test on the validation set with probabilities as output.  The -5 means to remove the 5th column, which is species.  We are trying to predict species.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;predict&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;irisSVM&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; irisdata&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;irisValData&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; type&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;probabilities&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This produces output similar to the following.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Iris-setosa Iris-versicolor Iris-virginica
 [1,] 0.964182671 0.022183652 0.013633677
 [2,] 0.952685528 0.032202528 0.015111944
 [3,] 0.966094194 0.021206723 0.012699083
 [4,] 0.965805632 0.020603214 0.013591154
 [5,] 0.962410318 0.024487673 0.013102009
 [6,] 0.964783325 0.022303353 0.012913322
 [7,] 0.975483475 0.012628443 0.011888082
 [8,] 0.918612644 0.060459572 0.020927784
 [9,] 0.953575715 0.030428791 0.015995494
[10,] 0.948050721 0.035563597 0.016385682
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above shows the predictions for the first 10 elements of the validation set.  The numbers you see are probabilities.  As you can see each line has one column with the maximum probability.  These samples are all Iris-setosa.  I only show ten rows, so there is not much variety.  If you run the above command in R, you will see the other species as well.&lt;/p&gt;

&lt;h2 id=&quot;using-a-neural-network-ann&quot;&gt;Using a Neural Network (ANN)&lt;/h2&gt;

&lt;p&gt;I will now show you how to do exactly the same thing using an Artificial Neural Network.  First, we must tell R that we are using ANN’s.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;nnet&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The neural network requires that the species be normalized using one-of-n normalization. We will normalize between 0 and 1.  This can be done with the following command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ideal &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; class.ind&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;irisdata&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;species&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now train a neural network for the training data.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;irisANN &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; nnet&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;irisdata&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;irisTrainData&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; ideal&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;irisTrainData&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt; size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; softmax&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can test the output from the neural network.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;predict&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;irisANN&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; irisdata&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;irisValData&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; type&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;class&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The new series of books will cover R, as well as the usual Java and C#. You can pledge ($7) at Kickstarter and pre-order and support this project.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to Chart a Function in R</title>
   <link href="http://www.heatonresearch.com/r/2013/04/01/chart-function-r.html"/>
   <updated>2013-04-01T08:26:00-05:00</updated>
   <id>http://www.heatonresearch.com/r/2013/04/01/chart-function-r</id>
   <content type="html">&lt;p&gt;R works really well as a quick scientific calculator.  I often leave R open simply for 
quick calculations.  R can also be used to quickly graph a function.  In this article 
I will show you how to graph a function with R.  The function we will examine is the 
Sigmoid (or Logistic) function.  This function is often used as an activation function 
for a neural network.&lt;/p&gt;

&lt;p&gt;We use the following code to define the sigmoid function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;sigmoid &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once the sigmoid function has been created it can easily be queried as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;sigmoid&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code would produce 0.5.&lt;/p&gt;

&lt;h2 id=&quot;plot-a-function&quot;&gt;Plot a Function&lt;/h2&gt;

&lt;p&gt;The above function could easily be plotted with the following command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sigmoid&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The -5 and 5 specify the range to plot over. This produces the following plot.&lt;/p&gt;

&lt;p&gt;!{Sigmoid function graphed in R}(/images/blog/)&lt;/p&gt;

&lt;p&gt;If you would like to save your plot to a file, use the following. The plot will be saved to your documents directory.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;png&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;test.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sigmoid&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; xlim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
dev.off&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code was used to produce the plot that you see here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-04-01-chart-function-r-1.png&quot; alt=&quot;Sigmoid Function&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you would like to plot two equations on the same graph, the following code can be used.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
par&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;new&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
plot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>How to Represent Equations in Computer Programs</title>
   <link href="http://www.heatonresearch.com/geneticprogramming/2013/03/01/represent-equations-programs.html"/>
   <updated>2013-03-01T07:26:00-06:00</updated>
   <id>http://www.heatonresearch.com/geneticprogramming/2013/03/01/represent-equations-programs</id>
   <content type="html">&lt;p&gt;I am currently in the process of adding Genetic Programming to Encog. At their most simple level, a Genetic Programs can be used to create a formula from training data. A neural network or support vector machine is a black box. You really cannot see into how either of these two algorithms is actually providing the values that they are computing. However, with a Genetic Program, you are left with a formula. The formula will likely give you more understanding into the nature of the data than a neural network or SVM.&lt;/p&gt;

&lt;p&gt;To implement Genetic Programming I must have a way to represent formulas in a program. There are many different ways of doing this. Consider the following formula.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In Encog, I refer to the above formula as the “Common Form” of the equation. Encog can parse the above formula and store it internally. Internally Encog stores these expressions in an opcode format based on Reverse Polish Notation (RPN).&lt;/p&gt;

&lt;p&gt;RPN does not require parenthesis. This is a very nice feature of RPN. There are no rules of precedence. Everything is non-ambiguous. Consider a very simple expression, such as 2+4. In RPN this would be stored as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above basically says to push 2 and 4 onto a stack. Then once we hit the + operator, we pop 2 values off the stack and add them. We know to pop two values off the stack because the plus operator requires two operands. The result of the addition, which is 6, is placed back on the sack. Once the entire expression is evaluated, the answer is the last value on the stack. Now consider a slightly more complex expression.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Written in RPN, this expression would be as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There we start off just as before. 2 and 4 are added and the resulting 6 is on the stack. But now we push another 2 onto the stack. The division now pops 6 and 2 from the stack and divides, resulting in 3.&lt;/p&gt;

&lt;p&gt;Recall the slightly more complex equation from earlier in this article?&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This can be expressed in RPN as well. The above expression in RPN is as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;RPN also lends itself nicely to trees. The above expression could be rendered as the following tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/2013-04-10-represent-equations-programs-1.png&quot; alt=&quot;Equation Tree&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Encog Workbench rendered the above tree. You can see that the operands for each operator and function are represented by its leaf nodes. Encog is also capable of rendering the expression as a mathematical formula. To render as a formula, LaTex is used. Here you can see the above expression rendered as a formula.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\frac{1}{((x+2)^2)}}&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>How to Compute a Derivative in R</title>
   <link href="http://www.heatonresearch.com/r/2013/02/28/compute-derivative-r.html"/>
   <updated>2013-02-28T19:00:00-06:00</updated>
   <id>http://www.heatonresearch.com/r/2013/02/28/compute-derivative-r</id>
   <content type="html">&lt;p&gt;There are many times that you will want to take the derivative of an expression.  Taking 
a derivative is a very common thing to do in Machine Learning.  One example is the 
activation function of a neural network.  If you are going to train the neural network 
using any of the backpropagation techniques, you will need the derivative of the 
activation function.  In this article I will show you how to take the derivative of a 
function using R.&lt;/p&gt;

&lt;p&gt;For this article we will obtain the derivative of the Sigmoid Activation Function. You 
might be tempted to create a function for the sigmoid.  This can be done as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;sigmoid &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once the sigmoid function has been created it can easily be queried as follows.&lt;/p&gt;

&lt;p&gt;sigmoid(0)
The above code would produce 0.5.  Functions are useful when you need to graph the function.  However, to take the derivative you must produce an expression.  This is done as follows.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;kn&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Ryacas&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
x &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; Sym&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
s &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;expression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;e&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
deriv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This produces the following.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;e^-x * log(e)/(1 + e^-x)^2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you would like to simplify the above, use the following.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;Simplify&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;deriv&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</content>
 </entry>
 

</feed>
