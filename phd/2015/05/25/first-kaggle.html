<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>My First Kaggle Competition</title>
  <meta name="description" content="I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it, Kaggle is an ongoing forum for competitive data science. Individual...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://www.heatonresearch.com/phd/2015/05/25/first-kaggle.html">
  <link rel="alternate" type="application/rss+xml" title="Heaton Research" href="http://www.heatonresearch.com/feed.xml" />

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Heaton Research</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/download/">Downloads</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/jeff/">Blog</a>
          
        
          
          <a class="page-link" href="/book/">Books</a>
          
        
          
          <a class="page-link" href="/encog/">Encog</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/jeff_index">Articles</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">My First Kaggle Competition</h1>
    <p class="post-meta">May 25, 2015</p>
  </header>

  <article class="post-content">
    <p>I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it, 
<a href="https://www.kaggle.com/">Kaggle</a> is an ongoing forum for competitive data science. Individuals and teams compete to 
create the best model for data sets provided by industry and sometimes academia.<br />
Individuals who enter are ranked as either Novice, Kaggler and <a href="https://www.kaggle.com/wiki/UserRankingAndTierSystem">Kaggle Master</a>.  To become 
a Kaggle master, one must place in the top 10% of two competitions; and in one of the top 
10 slots of a third competition.</p>

<p>I’ve talked about Kaggle in many of my presentations.  I’ve also used Kaggle data in 
my books. Until now, I had yet to actually enter a Kaggle competition.  I decided it was 
finally time to try this for myself. I competed in the <a href="http://otto%20group%20product%20classification%20challenge/">Otto Group Product Classification</a> 
Challenge that ended on May 18th, 2015.  My score was sufficient to land in the top 10%, 
so I’ve completed one of the requirements for Kaggle master.  My Kaggle profile can be 
seen here.</p>

<p>My goals for entering were:</p>

<ul>
  <li>See how hard Kaggle actually is, and move towards a Kaggle master designation.</li>
  <li>Learn from the other Kagglers and forums.</li>
  <li>Build a basic toolkit that I will use for future Kaggle competitions.</li>
  <li>Gain an example (from my entry) for the <a href="http://www.heatonresearch.com/aifh">Artificial Intelligence for Humans series</a>.</li>
  <li>Maybe get an idea or two for my future dissertation (I am a phd student at <a href="http://cec.nova.edu/">Nova Southeastern University)</a>.</li>
</ul>

<h2 id="the-otto-classification-challenge">The Otto Classification Challenge</h2>

<p>First, I will give a brief introduction to the exact nature of the Otto Classification 
Challenge.  For a complete description, refer to the Kaggle description(<a href="https://www.kaggle.com/c/otto-group-product-classification-challenge">found here</a>).<br />
This challenge was introduced by the <a href="http://en.wikipedia.org/wiki/Otto_GmbH">Otto Group</a>, who is the world’s largest mail order 
company and currently one of the biggest <a href="http://en.wikipedia.org/wiki/E-commerce">e-commerce</a> companies, mainly based in Germany 
and France but operating in more than 20 countries.  They have many products sold over 
numerous countries.  They would like to be able to classify these products into 9 
categories, using 93 features (columns).  These 93 columns represent counts, and are 
often zero.</p>

<p>The data are completely redacted.  You do not know what the 9 categories are, nor do you 
know the meaning behind the 93 features.  You only know that the features are integer 
counts. Most Kaggle competitions provide you with a test and training dataset.  For the 
training dataset you are given the outcomes, or correct answers.  For the test set, you 
are only given the 93 features, and you must provide the outcome.  The test and training 
sets are divided as follows:</p>

<ul>
  <li>Test Data: 144K rows</li>
  <li>Training Data: 61K rows</li>
</ul>

<p>You do not actually submit your model to Kaggle.  Rather, you submit your predictions 
based on the test data.  This allows you to use any platform to make these predictions.<br />
The actual format of a submission for this competition is the probability of each of 
the 9 categories being the outcome.  This is not like a university multiple choice test 
where you must submit your answer as A, B, C, or D.  Rather, you would submit your 
answer as:</p>

<ul>
  <li>A: 80% probability</li>
  <li>B: 16% probability</li>
  <li>C: 2% probability</li>
  <li>D: 2% probability</li>
</ul>

<p>I wish college exams were graded like this!  Often I am very confident about two of the 
answers, and can eliminate the other two.  Simply assign a probability to each, and you 
get a partial score.  If A were the correct answer for the above, I would get 80% of the 
points.</p>

<p>The actual Kaggle score is slightly more complex than that.  Rather, you are graded on a 
logarithm based scale and are very heavily penalized for having a lower probability on 
the correct answer. The following are a few lines from my submission:</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text">1,0.0003,0.2132,0.2340,0.5468,6.2998e-05,0.0001,0.0050,0.0001,4.3826e-05
2,0.0011,0.0029,0.0010,0.0003,0.0001,0.5207,0.0013,0.4711,0.0011
3,3.2977e-06,4.1419e-06,7.4524e-06,2.6550e-06,5.0014e-07,0.9998,5.2621e-06,0.0001,6.6447e-06
4,0.0001,0.6786,0.3162,0.0039,3.3378e-05,4.1196e-05,0.0001,0.0001,0.0006
5,0.1403,0.0002,0.0002,6.734e-05,0.0001,0.0027,0.0009,0.0297,0.8255</code></pre></figure>

<p>Each line starts with a number that specifies the data item that is being answered.<br />
The sample above shows the answers for items 1-5.  The next 9 values are the probabilities 
for each of the product classes.  These probabilities must add up to 1.0 (100%).</p>

<h2 id="what-i-learned-from-kaggle">What I Learned from Kaggle</h2>

<p>If you want to do well in Kaggle, the following are very important topics, along with 
the tools I used.</p>

<ul>
  <li>Deep Learning - Using <a href="http://h2o.ai/">H2O</a> and <a href="https://github.com/Lasagne/Lasagne">Lasagne</a></li>
  <li>Gradient Boosting Machines (GBM) - <a href="https://github.com/dmlc/xgboost">Using XGBOOST</a></li>
  <li>Ensemble Learning - <a href="http://www.numpy.org/">Using NumPy</a></li>
  <li>Feature Engineering - Using NumPy and <a href="http://scikit-learn.org/stable/">Scikit-Learn</a></li>
</ul>

<p>The two areas that I learned the most about, during this challenge, were GBM parameter 
tuning and ensemble learning.  I got pretty good at tuning a GBM.  The individual scores 
for my GBM’s were in line with those used by the top teams.</p>

<p>Before Kaggle I typically used only one model, if I were using neural networks, I just 
used neural networks.  If I were using an SVM, Random Forest or Gradient Boosting, I stuck 
to just that model.  With Kaggle, it is critical to use multiple models, ensembled to 
produce better results than each of the models could produce independently.</p>

<p>Some of my main takeaways from the competition:</p>

<ul>
  <li>GPU is really important for deep learning.  It is best to use a deep learning package that supports it, such as H2O, Theano or Lasagne.</li>
  <li>The <a href="http://lvdmaaten.github.io/tsne/">t-sne</a> visualization is awesome for high-dimension visualization and creating features.</li>
  <li>I need to learn to ensemble better!</li>
</ul>

<p>This competition was the first time I used <a href="http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">T-SNE</a>.  It works like PCA in that it is capable 
of reducing dimensions, however, the data points separate in such a way that the 
visualization is often clearer than <a href="http://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>. This is done using a stochastic nearest 
neighbor process. I plan to learn more about how t-sne actually performs the reduction, 
compared to PCA.</p>

<p><img src="/images/blog/2015-05-25-first-kaggle-1.jpeg" alt="t-SNE Plot of the Otto Group Challenge" /></p>

<h2 id="my-approach-to-the-otto-challenge">My Approach to the Otto Challenge</h2>

<p>So far I’ve only worked with single model systems.  I’ve used models that contain ensembles 
that are “built in”, such as random forests and gradient boosting machines.  However, it 
is possible to create higher-level ensembles of these models.  I used a total of 20 models, 
this included 10 deep neural networks and 10 gradient boosting machines.  My deep neural 
network system provided one prediction and my gradient boosting machines provided the other.<br />
These two predictions were blended together, using a simple ratio.  The resulting prediction 
vector was then normalized so that the sum equaled 1.0(100%).</p>

<p><img src="/images/blog/2015-05-25-first-kaggle-2.png" alt="Jeff Heaton's Kaggle Model for the Otto Group" /></p>

<p>I did not remove or engineer any fields.  For both model types I converted all 93 
attributes into Z-Scores.  For the neural network I normalized all values to be in a 
specific range.</p>

<p>My 10 deep learning neural networks used a simple bagging method.  I averaged the 
predictions from 20 different neural networks.  Each of these neural networks was created 
by choosing a different 80/20 split between training and validation.  The neural network 
was trained on the training data until the validation score did not improve for 25 epochs.<br />
Once training stopped I used the weights from the epoch that produced the highest training 
score. This process is a simple form of bagging called <a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">bootstrap aggregation</a>.</p>

<p>My 10 gradient boosting machines (GBM) were each components of a 10-fold cross-validation.<br />
I essentially broke the Kaggle training data into 10 folds and used each of these folds 
as a validation set, and the others as training.  This produced 10 gradient boosting machines.<br />
I then used an NxM coefficient matrix to blend each of these together.  Where N is the 
number of models, M is the number of features.  In this case it was a 10x9 grid.  This 
matrix weighted each of the 10 model’s predictive power in each of the 9 categories.<br />
These coefficients were a straight probability calculation from the <a href="http://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> of 
each of the 10 models.  This allowed each model to potentially specialize in each of the 
9 categories.</p>

<p>I spent considerable time tuning my GBM.  I used Nelder-Mead searches to optimize my 
hyper-parameter vector.  I ultimately settled on the following parameters:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'max_depth'</span><span class="p">:</span> <span class="mi">13</span><span class="p">,</span><span class="s">'min_child_weight'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span><span class="s">'subsample'</span><span class="p">:</span> <span class="o">.</span><span class="mi">78</span><span class="p">,</span><span class="s">'gamma'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span><span class="s">'colsample_bytree'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s">'eta'</span><span class="p">:</span><span class="o">.</span><span class="mo">005</span><span class="p">,</span> <span class="s">'threads'</span><span class="p">:</span><span class="mi">24</span><span class="p">}</span>
<span class="n">Each</span> <span class="n">of</span> <span class="n">these</span> <span class="n">two</span> <span class="n">approaches</span> <span class="p">(</span><span class="n">GBM</span> <span class="ow">and</span> <span class="n">neural</span> <span class="n">network</span><span class="p">)</span> <span class="n">produced</span> <span class="n">a</span> <span class="n">separate</span> <span class="n">submission</span> <span class="nb">file</span><span class="o">.</span> <span class="n">I</span> <span class="n">then</span> <span class="n">blended</span> <span class="n">these</span> <span class="n">together</span><span class="p">,</span> <span class="n">weighting</span> <span class="n">each</span><span class="o">.</span>  <span class="n">I</span> <span class="n">found</span> <span class="n">that</span> <span class="mf">0.65</span> <span class="n">gave</span> <span class="n">me</span> <span class="n">the</span> <span class="n">best</span> <span class="n">blend</span> <span class="k">with</span> <span class="n">my</span> <span class="n">deep</span> <span class="n">neural</span> <span class="n">network</span><span class="o">.</span></code></pre></figure>

<h2 id="what-worked-well-for-top-teams">What Worked Well for Top Teams</h2>

<p>The top Kaggle teams made use of more sophisticated ensemble techniques than I did.<br />
This will be my primary learning area for the next competition.  You can read about 
some of the top models here:</p>

<ul>
  <li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov">The Top Scoring Model</a></li>
  <li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score">Relatively Simple Model for a Top 25 Score</a></li>
  <li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models">Share Your Models</a></li>
  <li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent">One of the Top Ten</a></li>
  <li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging">TSNE &amp; Meta-Bagging</a></li>
</ul>

<p>The above write-ups are very useful, I’ve already started examining their approaches.</p>

<p>Some of the top technologies discussed were:</p>

<ul>
  <li>Feature Engineering</li>
  <li>Input Transformation - good write up <a href="http://fmwww.bc.edu/repec/bocode/t/transint.html">here</a>
    <ul>
      <li>log transforms</li>
      <li>sqrt(x + 3/8) - Not sure what this one is called, but I saw it used a few times</li>
      <li>z-score transforms</li>
      <li>ranged transformation</li>
    </ul>
  </li>
  <li>Hyperparameter Optimization
    <ul>
      <li>Nelder-Mead</li>
      <li><a href="https://github.com/JasperSnoek/spearmint">Spearmint</a></li>
    </ul>
  </li>
</ul>

<p>I will probably not enter another Kaggle until the fall of this year.  This blog post 
will be updated to contain my notes as I investigate other techniques for this 
competition.</p>

  </article>
  
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'heatonresearch';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">&copy; 2016 by Heaton Research - <a href="/legal/">Legal and Copyright Info</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Heaton Research</li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jeffheaton">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">jeffheaton</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/jeffheaton">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">jeffheaton</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Jeff Heaton is a data scientist, phd student and indie publisher.  Heaton Research is the homepage for his projects.
</p>
      </div>
    </div>

  </div>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-5393865-1', 'auto');
  ga('send', 'pageview');
</script>
  </body>

</html>
